{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8811f403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.4'\n",
    "os.environ['OMP_NUM_THREADS']='1'\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    ")\n",
    "#os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "#os.environ['XLA_FLAGS']='--xla_gpu_deterministic_reductions --xla_gpu_autotune_level=1'\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models import scalar\n",
    "import pickle\n",
    "import time\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "#import sympy\n",
    "#import optuna\n",
    "from util import *\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as special\n",
    "\n",
    "matplotlib.style.use('default') # 'classic'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['axes.prop_cycle'] = plt.cycler(color='krbg')\n",
    "matplotlib.rcParams['legend.numpoints'] = 1\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "@jax.jit\n",
    "def arcsinh(x: any) -> any:\n",
    "    return jnp.arcsinh(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def sinh(x: any) -> any:\n",
    "    return jnp.sinh(x)\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# jax.config.update(\"jax_platform_name\",\"cpu\")\n",
    "num_devices = jax.local_device_count()\n",
    "jax.devices()\n",
    "\n",
    "# jax.config.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd1f757",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    volume: int\n",
    "    features: Sequence[int]\n",
    "    kernel_init: Callable = nn.initializers.variance_scaling(\n",
    "        2, \"fan_in\", \"truncated_normal\")  # for ReLU / CELU\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features:\n",
    "            x = nn.Dense(feat, use_bias=False,\n",
    "                         kernel_init=self.kernel_init,\n",
    "                         bias_init=self.bias_init)(x)\n",
    "            x = arcsinh(x)\n",
    "        x = nn.Dense(1, use_bias=False,\n",
    "                     kernel_init=self.bias_init)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CV_MLP(nn.Module):\n",
    "    volume: int\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = MLP(self.volume, self.features)(x)\n",
    "        y = self.param('bias', nn.initializers.zeros, (1,))\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8f053b",
   "metadata": {},
   "source": [
    "# 16x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d62080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.003559981), np.complex128(0.0001198993089513105+0j))\n",
      "bin size 1: (np.float32(0.003559981), np.complex128(0.00011990134551381811+0j))\n",
      "jack bin size 2: (np.float32(0.003559981), np.complex128(0.00016616860248677823+0j))\n",
      "bin size 2: (np.float32(0.003559981), np.complex128(0.00016616854843756865+0j))\n",
      "jack bin size 4: (np.float32(0.003559981), np.complex128(0.00022852752223321844+0j))\n",
      "bin size 4: (np.float32(0.003559981), np.complex128(0.0002285271755981584+0j))\n",
      "jack bin size 5: (np.float32(0.003559981), np.complex128(0.0002523341360845214+0j))\n",
      "bin size 5: (np.float32(0.003559981), np.complex128(0.0002523335635520957+0j))\n",
      "jack bin size 10: (np.float32(0.003559981), np.complex128(0.0003383678146177811+0j))\n",
      "bin size 10: (np.float32(0.003559981), np.complex128(0.0003383683100943418+0j))\n",
      "jack bin size 20: (np.float32(0.003559981), np.complex128(0.0004391616290682918+0j))\n",
      "bin size 20: (np.float32(0.003559981), np.complex128(0.00043916189680241264+0j))\n",
      "jack bin size 50: (np.float32(0.003559981), np.complex128(0.0005626122479360114+0j))\n",
      "bin size 50: (np.float32(0.003559981), np.complex128(0.0005626122502753421+0j))\n",
      "jack bin size 100: (np.float32(0.003559981), np.complex128(0.0006289310575087525+0j))\n",
      "bin size 100: (np.float32(0.003559981), np.complex128(0.0006289308288614985+0j))\n",
      "jack bin size 200: (np.float32(0.003559981), np.complex128(0.0006601814626574177+0j))\n",
      "bin size 200: (np.float32(0.003559981), np.complex128(0.0006601812834318607+0j))\n",
      "jack bin size 500: (np.float32(0.003559981), np.complex128(0.0006577933112743479+0j))\n",
      "bin size 500: (np.float32(0.003559981), np.complex128(0.0006577933574365648+0j))\n",
      "jack bin size 1000: (np.float32(0.003559981), np.complex128(0.0007126670606370409+0j))\n",
      "bin size 1000: (np.float32(0.003559981), np.complex128(0.0007126670942750586+0j))\n",
      "jack bin size 2000: (np.float32(0.003559981), np.complex128(0.0006881249501020648+0j))\n",
      "bin size 2000: (np.float32(0.003559981), np.complex128(0.0006881245145840304+0j))\n",
      "jack bin size 5000: (np.float32(0.003559981), np.complex128(0.0006710194170254357+0j))\n",
      "bin size 5000: (np.float32(0.003559981), np.complex128(0.0006710195839473511+0j))\n",
      "jack bin size 10000: (np.float32(0.003559981), np.complex128(0.0008403770334552974+0j))\n",
      "bin size 10000: (np.float32(0.003559981), np.complex128(0.0008403771401693423+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_16x16_0.1_0.5_sweep1.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(16,16), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 8))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38ec1ab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.0035733639), np.complex128(0.0018269160736232577+0j)),\n",
       " (np.float32(0.003664993), np.complex128(0.0018159802222430918+0j)),\n",
       " (np.float32(0.0037466763), np.complex128(0.0013367552145961727+0j)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jackknife(jax.vmap(lambda x: model.observe(x, 8))(conf[:10000])), jackknife(jax.vmap(lambda x: model.observe(x, 8))(conf[:10000:10])), jackknife(jax.vmap(lambda x: model.observe(x, 8))(conf[:100000:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d3380b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.0032723916), np.complex128(0.002714385779448242+0j)),\n",
       " (np.float32(0.002867671), np.complex128(0.002605791378300637+0j)),\n",
       " (np.float32(0.0061721196), np.complex128(0.0018729680450633168+0j)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jackknife(jax.vmap(lambda x: model.observe(x, 8))(conf[:5000])), jackknife(jax.vmap(lambda x: model.observe(x, 8))(conf[:5000:10])), jackknife(jax.vmap(lambda x: model.observe(x, 8))(conf[:50000:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1066fa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j) for i, j in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee11657",
   "metadata": {},
   "source": [
    "## Correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48a640cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 1.6973195670288987e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041135293), np.complex128(0.00012021877104932775+0j)) <f>: (np.float32(-0.0008033856), np.complex128(0.0013654006044040887+0j))\n",
      "Epoch 200: <Test loss>: 8.363530469068792e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042029056), np.complex128(9.973289211984072e-05+0j)) <f>: (np.float32(-0.00089275936), np.complex128(0.0013601811764412403+0j))\n",
      "Epoch 300: <Test loss>: 5.385579697758658e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004106143), np.complex128(7.308258434463936e-05+0j)) <f>: (np.float32(-0.0007959964), np.complex128(0.0014125446881846026+0j))\n",
      "Epoch 400: <Test loss>: 3.949854090024019e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041228402), np.complex128(6.281580300539063e-05+0j)) <f>: (np.float32(-0.00081269146), np.complex128(0.0013921231672515766+0j))\n",
      "Epoch 500: <Test loss>: 3.2356444990000455e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041155503), np.complex128(6.609645524473569e-05+0j)) <f>: (np.float32(-0.0008054082), np.complex128(0.0013965461640171191+0j))\n",
      "Epoch 600: <Test loss>: 3.5141504213243024e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041248817), np.complex128(6.59485515776189e-05+0j)) <f>: (np.float32(-0.0008147336), np.complex128(0.0013753278313048197+0j))\n",
      "Epoch 700: <Test loss>: 2.766703119050362e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004094186), np.complex128(5.480772014167314e-05+0j)) <f>: (np.float32(-0.00078404055), np.complex128(0.0014020250220211415+0j))\n",
      "Epoch 800: <Test loss>: 2.459865072523826e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004079446), np.complex128(5.2121560721924375e-05+0j)) <f>: (np.float32(-0.00076929986), np.complex128(0.0014040242190963797+0j))\n",
      "Epoch 900: <Test loss>: 2.4234036573034246e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040843305), np.complex128(4.889092329614062e-05+0j)) <f>: (np.float32(-0.0007741822), np.complex128(0.001406041430384746+0j))\n",
      "Epoch 1000: <Test loss>: 2.2064266431698343e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040548113), np.complex128(4.556060114564229e-05+0j)) <f>: (np.float32(-0.0007446631), np.complex128(0.00139891224210902+0j))\n",
      "Epoch 1100: <Test loss>: 2.2730416731064906e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040466194), np.complex128(4.781506961118067e-05+0j)) <f>: (np.float32(-0.0007364716), np.complex128(0.0014074218011808414+0j))\n",
      "Epoch 1200: <Test loss>: 2.386453161307145e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040517366), np.complex128(5.4002591060651705e-05+0j)) <f>: (np.float32(-0.0007415891), np.complex128(0.0013795346575128263+0j))\n",
      "Epoch 1300: <Test loss>: 2.3331122065428644e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004051049), np.complex128(4.71473583711925e-05+0j)) <f>: (np.float32(-0.000740903), np.complex128(0.0014012391202582694+0j))\n",
      "Epoch 1400: <Test loss>: 2.5446709059906425e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004063107), np.complex128(5.498686720968376e-05+0j)) <f>: (np.float32(-0.00075295777), np.complex128(0.0013837724377208556+0j))\n",
      "Epoch 1500: <Test loss>: 2.6307254756829934e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004069951), np.complex128(5.9028453449075195e-05+0j)) <f>: (np.float32(-0.00075980264), np.complex128(0.0013803768854068873+0j))\n",
      "Epoch 1600: <Test loss>: 2.2268156953941798e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004066524), np.complex128(4.866978931544549e-05+0j)) <f>: (np.float32(-0.00075637724), np.complex128(0.00140158811392246+0j))\n",
      "Epoch 1700: <Test loss>: 2.3467607661586953e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004068428), np.complex128(4.977541957496623e-05+0j)) <f>: (np.float32(-0.00075828086), np.complex128(0.0013927010175388862+0j))\n",
      "Epoch 1800: <Test loss>: 2.252256081192172e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040514898), np.complex128(5.4803589241567645e-05+0j)) <f>: (np.float32(-0.00074134115), np.complex128(0.0013929270832275767+0j))\n",
      "Epoch 1900: <Test loss>: 2.2713995804224396e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004059958), np.complex128(4.645166642780264e-05+0j)) <f>: (np.float32(-0.0007498145), np.complex128(0.001390534491258607+0j))\n",
      "Epoch 2000: <Test loss>: 2.419541488052346e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004074037), np.complex128(4.188803273208531e-05+0j)) <f>: (np.float32(-0.00076389214), np.complex128(0.0014021383085867985+0j))\n",
      "Epoch 2100: <Test loss>: 2.2265560346568236e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040925886), np.complex128(4.402927409789917e-05+0j)) <f>: (np.float32(-0.0007824393), np.complex128(0.0013970647703781555+0j))\n",
      "Epoch 2200: <Test loss>: 2.8251427011127817e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040516173), np.complex128(5.9145640979898936e-05+0j)) <f>: (np.float32(-0.0007414723), np.complex128(0.0013828020805644054+0j))\n",
      "Epoch 2300: <Test loss>: 2.766341822280083e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004100458), np.complex128(6.142994548593054e-05+0j)) <f>: (np.float32(-0.0007903114), np.complex128(0.0013827678281873312+0j))\n",
      "Epoch 2400: <Test loss>: 2.2299179818219272e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040725833), np.complex128(5.3032126865523224e-05+0j)) <f>: (np.float32(-0.0007624386), np.complex128(0.001398766859797438+0j))\n",
      "Epoch 2500: <Test loss>: 2.53156531471177e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004080693), np.complex128(5.880553945480293e-05+0j)) <f>: (np.float32(-0.00077054737), np.complex128(0.0013875979207974348+0j))\n",
      "Epoch 2600: <Test loss>: 2.372400331296376e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004100025), np.complex128(5.250335579443829e-05+0j)) <f>: (np.float32(-0.00078987906), np.complex128(0.0013869373573625234+0j))\n",
      "Epoch 2700: <Test loss>: 2.4003575163078494e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040764767), np.complex128(4.793308570066281e-05+0j)) <f>: (np.float32(-0.0007663327), np.complex128(0.0014047895694330073+0j))\n",
      "Epoch 2800: <Test loss>: 2.416019924567081e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004048937), np.complex128(6.003755860709065e-05+0j)) <f>: (np.float32(-0.0007387901), np.complex128(0.001397098895894574+0j))\n",
      "Epoch 2900: <Test loss>: 2.4809298793115886e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040998133), np.complex128(5.164561125641101e-05+0j)) <f>: (np.float32(-0.000789666), np.complex128(0.0013928040283914208+0j))\n",
      "Epoch 3000: <Test loss>: 2.877143742807675e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004074395), np.complex128(6.394753484484857e-05+0j)) <f>: (np.float32(-0.00076425046), np.complex128(0.0013815615102110356+0j))\n",
      "Epoch 3100: <Test loss>: 2.6344168873038143e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004049177), np.complex128(5.877558448244487e-05+0j)) <f>: (np.float32(-0.0007390282), np.complex128(0.0013935376635640906+0j))\n",
      "Epoch 3200: <Test loss>: 3.1542667784378864e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040392457), np.complex128(7.51785443851126e-05+0j)) <f>: (np.float32(-0.0007290993), np.complex128(0.0013730609582457782+0j))\n",
      "Epoch 3300: <Test loss>: 2.5716396976349643e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004096033), np.complex128(5.946318509463002e-05+0j)) <f>: (np.float32(-0.0007858851), np.complex128(0.0013943960027614421+0j))\n",
      "Epoch 3400: <Test loss>: 3.100069761785562e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040901313), np.complex128(7.177024257523324e-05+0j)) <f>: (np.float32(-0.000779984), np.complex128(0.001379031655012457+0j))\n",
      "Epoch 3500: <Test loss>: 3.493761369099957e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040572155), np.complex128(7.442754516017598e-05+0j)) <f>: (np.float32(-0.00074707), np.complex128(0.0013830530109416388+0j))\n",
      "Epoch 3600: <Test loss>: 2.804064706651843e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040836674), np.complex128(5.755166062575736e-05+0j)) <f>: (np.float32(-0.00077352265), np.complex128(0.0013891969993641824+0j))\n",
      "Epoch 3700: <Test loss>: 3.019655423486256e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041011316), np.complex128(6.589135327942118e-05+0j)) <f>: (np.float32(-0.0007909855), np.complex128(0.0013878596343304137+0j))\n",
      "Epoch 3800: <Test loss>: 2.80885706160916e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040791994), np.complex128(6.057759252577606e-05+0j)) <f>: (np.float32(-0.0007690509), np.complex128(0.0013833615360566195+0j))\n",
      "Epoch 3900: <Test loss>: 3.0184080515027745e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004106368), np.complex128(6.223522124958528e-05+0j)) <f>: (np.float32(-0.00079622044), np.complex128(0.001382151539121305+0j))\n",
      "Epoch 4000: <Test loss>: 3.108970986431814e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040795114), np.complex128(6.812781942501805e-05+0j)) <f>: (np.float32(-0.00076936715), np.complex128(0.001380015713119737+0j))\n",
      "Epoch 4100: <Test loss>: 3.310550482638064e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040919213), np.complex128(6.310224247307018e-05+0j)) <f>: (np.float32(-0.0007817743), np.complex128(0.0013849348619102337+0j))\n",
      "Epoch 4200: <Test loss>: 3.383302782822284e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040868446), np.complex128(6.42837948707102e-05+0j)) <f>: (np.float32(-0.00077669934), np.complex128(0.0013793640299307338+0j))\n",
      "Epoch 4300: <Test loss>: 3.0163494102453114e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004076991), np.complex128(6.384898790164097e-05+0j)) <f>: (np.float32(-0.00076684466), np.complex128(0.0013866746289442978+0j))\n",
      "Epoch 4400: <Test loss>: 3.100522690147045e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004117768), np.complex128(6.0110495555402396e-05+0j)) <f>: (np.float32(-0.00080762064), np.complex128(0.0013903655128650403+0j))\n",
      "Epoch 4500: <Test loss>: 3.3058138342312304e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040908675), np.complex128(5.870907778362555e-05+0j)) <f>: (np.float32(-0.00078072207), np.complex128(0.0013813402652272666+0j))\n",
      "Epoch 4600: <Test loss>: 3.7506927128561074e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004100042), np.complex128(7.682411738222237e-05+0j)) <f>: (np.float32(-0.00078989856), np.complex128(0.0013703893996946366+0j))\n",
      "Epoch 4700: <Test loss>: 3.1740917165734572e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004118279), np.complex128(6.279157262012694e-05+0j)) <f>: (np.float32(-0.0008081335), np.complex128(0.001378202240044635+0j))\n",
      "Epoch 4800: <Test loss>: 3.266412477387348e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041041686), np.complex128(6.25915886893961e-05+0j)) <f>: (np.float32(-0.0007940212), np.complex128(0.0013851841430989411+0j))\n",
      "Epoch 4900: <Test loss>: 3.2643622489558766e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004110167), np.complex128(6.159853933313423e-05+0j)) <f>: (np.float32(-0.0008000202), np.complex128(0.0013869291114198944+0j))\n",
      "Epoch 5000: <Test loss>: 3.5106882023683283e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041160053), np.complex128(6.342168156884775e-05+0j)) <f>: (np.float32(-0.00080585777), np.complex128(0.0013773512587653216+0j))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 80\u001b[39m\n\u001b[32m     76\u001b[39m     save()\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_train//nt):  \u001b[38;5;66;03m# one epoch\u001b[39;00m\n\u001b[32m     79\u001b[39m     g_params, opt_state = train_batch_shard(\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m         \u001b[43mconf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnt\u001b[49m\u001b[43m*\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnt\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, g_params, opt_state)\n\u001b[32m     82\u001b[39m epochs+=\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/array.py:382\u001b[39m, in \u001b[36mArrayImpl.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    377\u001b[39m       out = lax.squeeze(out, dimensions=dims)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayImpl(\n\u001b[32m    380\u001b[39m         out.aval, sharding, [out], committed=\u001b[38;5;28;01mFalse\u001b[39;00m, _skip_checks=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrewriting_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/numpy/indexing.py:618\u001b[39m, in \u001b[36mrewriting_take\u001b[39m\u001b[34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value, out_sharding)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrewriting_take\u001b[39m(arr, idx, indices_are_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m, unique_indices=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    610\u001b[39m                    mode=\u001b[38;5;28;01mNone\u001b[39;00m, fill_value=\u001b[38;5;28;01mNone\u001b[39;00m, out_sharding=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    611\u001b[39m   \u001b[38;5;66;03m# Computes arr[idx].\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    615\u001b[39m   \u001b[38;5;66;03m# For simplicity of generated primitives, we call lax.dynamic_slice in the\u001b[39;00m\n\u001b[32m    616\u001b[39m   \u001b[38;5;66;03m# simplest cases: i.e. non-dynamic arrays indexed with integers and slices.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (result := \u001b[43m_attempt_rewriting_take_via_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    621\u001b[39m   \u001b[38;5;66;03m# TODO(mattjj,dougalm): expand dynamic shape indexing support\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/numpy/indexing.py:594\u001b[39m, in \u001b[36m_attempt_rewriting_take_via_slice\u001b[39m\u001b[34m(arr, idx, mode)\u001b[39m\n\u001b[32m    592\u001b[39m   int_start_indices = [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m start_indices]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    593\u001b[39m   int_limit_indices = [i + s \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(int_start_indices, slice_sizes)]\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m   arr = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m      \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mint_start_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mint_limit_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# We must be careful with dtypes because dynamic_slice requires all\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;66;03m# start indices to have matching types.\u001b[39;00m\n\u001b[32m    599\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(start_indices) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/lax/slicing.py:108\u001b[39m, in \u001b[36mslice\u001b[39m\u001b[34m(operand, start_indices, limit_indices, strides)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mslice\u001b[39m(operand: ArrayLike, start_indices: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[32m     58\u001b[39m           limit_indices: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[32m     59\u001b[39m           strides: Sequence[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> Array:\n\u001b[32m     60\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Wraps XLA's `Slice\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m  <https://www.tensorflow.org/xla/operation_semantics#slice>`_\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m  operator.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m \u001b[33;03m    - :func:`jax.lax.dynamic_slice`\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mslice_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/core.py:502\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    501\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/core.py:520\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    518\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    522\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/core.py:525\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/core.py:1029\u001b[39m, in \u001b[36mEvalTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m   1027\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m primitive.bind_with_trace(arg._trace, args, params)\n\u001b[32m   1028\u001b[39m check_eval_args(args)\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/lax/slicing.py:1414\u001b[39m, in \u001b[36m_slice_impl\u001b[39m\u001b[34m(x, start_indices, limit_indices, strides)\u001b[39m\n\u001b[32m   1410\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch.apply_primitive(\n\u001b[32m   1411\u001b[39m     slice_p, x, start_indices=start_indices,\n\u001b[32m   1412\u001b[39m     limit_indices=limit_indices, strides=strides)\n\u001b[32m   1413\u001b[39m slice_sizes = \u001b[38;5;28mtuple\u001b[39m(np.array(limit_indices) - np.array(start_indices))\n\u001b[32m-> \u001b[39m\u001b[32m1414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_slice_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1415\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mslice_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslice_sizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/dispatch.py:86\u001b[39m, in \u001b[36mapply_primitive\u001b[39m\u001b[34m(prim, *args, **params)\u001b[39m\n\u001b[32m     83\u001b[39m fun = xla_primitive_callable(prim, **params)\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# TODO(yashkatariya): Investigate adding is_primitive to jit and never\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# triggering the disable jit path instead of messing around with it here.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m prev = \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjax_jit\u001b[49m\u001b[43m.\u001b[49m\u001b[43mswap_thread_local_state_disable_jit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     88\u001b[39m   outs = fun(*args)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_1h_cor.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 100 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e13941d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 3.8803340430604294e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040660813), np.complex128(0.00018642592014506685+0j)) <f>: (np.float32(-0.0007559332), np.complex128(0.0013675406168072992+0j))\n",
      "Epoch 200: <Test loss>: 2.640999446157366e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040614195), np.complex128(0.00016411317641249674+0j)) <f>: (np.float32(-0.00075127423), np.complex128(0.001320381056026774+0j))\n",
      "Epoch 300: <Test loss>: 2.0644747564801946e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041151587), np.complex128(0.00013166960235537817+0j)) <f>: (np.float32(-0.00080501253), np.complex128(0.0013918129929480702+0j))\n",
      "Epoch 400: <Test loss>: 1.9398430595174432e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004083332), np.complex128(0.00014444008577612694+0j)) <f>: (np.float32(-0.00077318825), np.complex128(0.001364304655198366+0j))\n",
      "Epoch 500: <Test loss>: 1.504843748989515e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041010836), np.complex128(0.0001172677860461366+0j)) <f>: (np.float32(-0.0007909414), np.complex128(0.0013851689198202414+0j))\n",
      "Epoch 600: <Test loss>: 1.3196249710745178e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004135991), np.complex128(0.00012258668069194738+0j)) <f>: (np.float32(-0.000825844), np.complex128(0.0013776661269130938+0j))\n",
      "Epoch 700: <Test loss>: 1.2161588529124856e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00410754), np.complex128(0.00010378074606850928+0j)) <f>: (np.float32(-0.0007973933), np.complex128(0.0013933406489655853+0j))\n",
      "Epoch 800: <Test loss>: 1.2025630894640926e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040482352), np.complex128(0.00010340891748626908+0j)) <f>: (np.float32(-0.0007380873), np.complex128(0.0013783030942660205+0j))\n",
      "Epoch 900: <Test loss>: 1.1539756997080985e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004054983), np.complex128(0.00011008554622990044+0j)) <f>: (np.float32(-0.00074483483), np.complex128(0.0013806264203169065+0j))\n",
      "Epoch 1000: <Test loss>: 1.0978226782754064e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004050735), np.complex128(0.00010721866984152515+0j)) <f>: (np.float32(-0.0007405869), np.complex128(0.001370036727071427+0j))\n",
      "Epoch 1100: <Test loss>: 1.0336614650441334e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004004539), np.complex128(0.00010115262936319862+0j)) <f>: (np.float32(-0.0006943932), np.complex128(0.0013895727606267534+0j))\n",
      "Epoch 1200: <Test loss>: 1.0277451110596303e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040352154), np.complex128(9.881251013299695e-05+0j)) <f>: (np.float32(-0.0007250663), np.complex128(0.001386196744853783+0j))\n",
      "Epoch 1300: <Test loss>: 1.0138427569472697e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040093414), np.complex128(0.00010803275845636463+0j)) <f>: (np.float32(-0.0006991961), np.complex128(0.0013700525846534056+0j))\n",
      "Epoch 1400: <Test loss>: 9.415597560291644e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040386626), np.complex128(9.923371129672832e-05+0j)) <f>: (np.float32(-0.0007285158), np.complex128(0.0013807043127595866+0j))\n",
      "Epoch 1500: <Test loss>: 9.101088835450355e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004036492), np.complex128(9.257427217196205e-05+0j)) <f>: (np.float32(-0.00072634756), np.complex128(0.0013923931267271848+0j))\n",
      "Epoch 1600: <Test loss>: 9.126408258453012e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004033199), np.complex128(9.271115481960354e-05+0j)) <f>: (np.float32(-0.0007230552), np.complex128(0.0013789195101927025+0j))\n",
      "Epoch 1700: <Test loss>: 1.0292658771504648e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004092659), np.complex128(9.323304370010965e-05+0j)) <f>: (np.float32(-0.0007825114), np.complex128(0.0014172617479503609+0j))\n",
      "Epoch 1800: <Test loss>: 8.969553164206445e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040388084), np.complex128(9.77686055113289e-05+0j)) <f>: (np.float32(-0.00072866026), np.complex128(0.0014001487529214033+0j))\n",
      "Epoch 1900: <Test loss>: 9.543460691929795e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039935214), np.complex128(0.00010153914206635126+0j)) <f>: (np.float32(-0.0006833752), np.complex128(0.0013587636354729865+0j))\n",
      "Epoch 2000: <Test loss>: 8.683617124916054e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004026739), np.complex128(8.844986574629055e-05+0j)) <f>: (np.float32(-0.000716591), np.complex128(0.0013761482391660778+0j))\n",
      "Epoch 2100: <Test loss>: 9.013278940983582e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040192246), np.complex128(9.044340166476236e-05+0j)) <f>: (np.float32(-0.00070907746), np.complex128(0.0013903396332912508+0j))\n",
      "Epoch 2200: <Test loss>: 8.491639164276421e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040166387), np.complex128(8.13179341088823e-05+0j)) <f>: (np.float32(-0.000706495), np.complex128(0.0013808874995466065+0j))\n",
      "Epoch 2300: <Test loss>: 8.69678842718713e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040050764), np.complex128(7.926875393524669e-05+0j)) <f>: (np.float32(-0.0006949292), np.complex128(0.0013799861545869283+0j))\n",
      "Epoch 2400: <Test loss>: 7.987380740814842e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004018488), np.complex128(8.667049440123417e-05+0j)) <f>: (np.float32(-0.0007083406), np.complex128(0.0013803527818822795+0j))\n",
      "Epoch 2500: <Test loss>: 8.218315088015515e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040165386), np.complex128(7.924015478614783e-05+0j)) <f>: (np.float32(-0.000706394), np.complex128(0.0013823925743673836+0j))\n",
      "Epoch 2600: <Test loss>: 7.915652531664819e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040136357), np.complex128(8.502654680627721e-05+0j)) <f>: (np.float32(-0.0007034897), np.complex128(0.0013804406963167702+0j))\n",
      "Epoch 2700: <Test loss>: 8.067291673796717e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040369835), np.complex128(8.409182163653358e-05+0j)) <f>: (np.float32(-0.0007268387), np.complex128(0.0013969030230419712+0j))\n",
      "Epoch 2800: <Test loss>: 8.320021152030677e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040186956), np.complex128(8.434097596458536e-05+0j)) <f>: (np.float32(-0.00070854963), np.complex128(0.0013714045386625951+0j))\n",
      "Epoch 2900: <Test loss>: 7.756946615700144e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004019119), np.complex128(8.381887300672254e-05+0j)) <f>: (np.float32(-0.0007089755), np.complex128(0.0013840893356391211+0j))\n",
      "Epoch 3000: <Test loss>: 7.583249043818796e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040473533), np.complex128(7.569297227329765e-05+0j)) <f>: (np.float32(-0.00073720847), np.complex128(0.0013884522004537997+0j))\n",
      "Epoch 3100: <Test loss>: 7.757295861665625e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040112734), np.complex128(8.364064964286219e-05+0j)) <f>: (np.float32(-0.0007011272), np.complex128(0.0013849977847955257+0j))\n",
      "Epoch 3200: <Test loss>: 7.70297810959164e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004029574), np.complex128(8.472632313546254e-05+0j)) <f>: (np.float32(-0.00071942667), np.complex128(0.0013820992725311026+0j))\n",
      "Epoch 3300: <Test loss>: 7.649778126506135e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040267534), np.complex128(7.993692900950774e-05+0j)) <f>: (np.float32(-0.0007166078), np.complex128(0.0013882848712487587+0j))\n",
      "Epoch 3400: <Test loss>: 7.845979780540802e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040246416), np.complex128(8.4078770846565e-05+0j)) <f>: (np.float32(-0.0007144941), np.complex128(0.0013717901950563208+0j))\n",
      "Epoch 3500: <Test loss>: 7.3354344749532174e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004021248), np.complex128(8.050670778758999e-05+0j)) <f>: (np.float32(-0.0007111007), np.complex128(0.0013734603155903338+0j))\n",
      "Epoch 3600: <Test loss>: 7.306892712222179e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004028185), np.complex128(7.766777245866145e-05+0j)) <f>: (np.float32(-0.00071803713), np.complex128(0.0013863087628128816+0j))\n",
      "Epoch 3700: <Test loss>: 7.484533398383064e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040125465), np.complex128(7.822783053899066e-05+0j)) <f>: (np.float32(-0.0007023977), np.complex128(0.0013801943329231467+0j))\n",
      "Epoch 3800: <Test loss>: 7.42672227715957e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004013707), np.complex128(7.922425756021401e-05+0j)) <f>: (np.float32(-0.0007035624), np.complex128(0.0013842680823031867+0j))\n",
      "Epoch 3900: <Test loss>: 7.361836196650984e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040089614), np.complex128(8.275884122418198e-05+0j)) <f>: (np.float32(-0.0006988144), np.complex128(0.0013689515610214498+0j))\n",
      "Epoch 4000: <Test loss>: 7.297716820175992e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040174252), np.complex128(7.549124797294463e-05+0j)) <f>: (np.float32(-0.00070727867), np.complex128(0.0013779747788887303+0j))\n",
      "Epoch 4100: <Test loss>: 7.495014870073646e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004025935), np.complex128(8.24538999227286e-05+0j)) <f>: (np.float32(-0.0007157878), np.complex128(0.001370363520120847+0j))\n",
      "Epoch 4200: <Test loss>: 8.06889329396654e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004010397), np.complex128(8.051041846177305e-05+0j)) <f>: (np.float32(-0.0007002514), np.complex128(0.001387149975821696+0j))\n",
      "Epoch 4300: <Test loss>: 7.430501682392787e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004028163), np.complex128(7.909980725684397e-05+0j)) <f>: (np.float32(-0.0007180193), np.complex128(0.0013873604376497194+0j))\n",
      "Epoch 4400: <Test loss>: 7.210101102828048e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040277955), np.complex128(8.246325589609613e-05+0j)) <f>: (np.float32(-0.0007176507), np.complex128(0.0013706678588341853+0j))\n",
      "Epoch 4500: <Test loss>: 8.261382390628569e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040036496), np.complex128(8.857798707988871e-05+0j)) <f>: (np.float32(-0.00069350394), np.complex128(0.0013575839582344155+0j))\n",
      "Epoch 4600: <Test loss>: 7.295642262761248e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004011928), np.complex128(8.229121698920752e-05+0j)) <f>: (np.float32(-0.00070178165), np.complex128(0.0013730146541064+0j))\n",
      "Epoch 4700: <Test loss>: 7.463300789822824e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003997734), np.complex128(7.579542811046303e-05+0j)) <f>: (np.float32(-0.0006875875), np.complex128(0.0013796755997014544+0j))\n",
      "Epoch 4800: <Test loss>: 7.967846613610163e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040122014), np.complex128(9.170679107859975e-05+0j)) <f>: (np.float32(-0.0007020549), np.complex128(0.0013545302953879112+0j))\n",
      "Epoch 4900: <Test loss>: 7.426417596434476e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004020525), np.complex128(8.726111004203664e-05+0j)) <f>: (np.float32(-0.00071038207), np.complex128(0.0013633246566320727+0j))\n",
      "Epoch 5000: <Test loss>: 7.524613920395495e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039926865), np.complex128(9.158664610873696e-05+0j)) <f>: (np.float32(-0.00068254163), np.complex128(0.001360213652769133+0j))\n",
      "Epoch 5100: <Test loss>: 7.322582860069815e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040141833), np.complex128(8.505143528119303e-05+0j)) <f>: (np.float32(-0.00070403825), np.complex128(0.001369724649858083+0j))\n",
      "Epoch 5200: <Test loss>: 7.410035777866142e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039936), np.complex128(8.554884798391478e-05+0j)) <f>: (np.float32(-0.0006834522), np.complex128(0.0013697252841613622+0j))\n",
      "Epoch 5300: <Test loss>: 7.329784693865804e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040068277), np.complex128(7.963702249033263e-05+0j)) <f>: (np.float32(-0.0006966818), np.complex128(0.0013749433166569964+0j))\n",
      "Epoch 5400: <Test loss>: 7.3166934271284845e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004004295), np.complex128(8.814130098735499e-05+0j)) <f>: (np.float32(-0.0006941461), np.complex128(0.0013679061023567478+0j))\n",
      "Epoch 5500: <Test loss>: 7.98457858763868e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039814315), np.complex128(9.091781294482377e-05+0j)) <f>: (np.float32(-0.00067128683), np.complex128(0.0013551257793063811+0j))\n",
      "Epoch 5600: <Test loss>: 7.389709480776219e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004006044), np.complex128(8.510509733860948e-05+0j)) <f>: (np.float32(-0.0006958963), np.complex128(0.0013675819733811+0j))\n",
      "Epoch 5700: <Test loss>: 7.210956482595066e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004017226), np.complex128(8.785144817515448e-05+0j)) <f>: (np.float32(-0.0007070803), np.complex128(0.0013716760204660731+0j))\n",
      "Epoch 5800: <Test loss>: 7.598680895171128e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040119537), np.complex128(8.59633334616781e-05+0j)) <f>: (np.float32(-0.0007018072), np.complex128(0.0013640629856490082+0j))\n",
      "Epoch 5900: <Test loss>: 7.244381777127273e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00401437), np.complex128(8.587189864398802e-05+0j)) <f>: (np.float32(-0.0007042253), np.complex128(0.0013712222399001662+0j))\n",
      "Epoch 6000: <Test loss>: 7.528278729296289e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003994874), np.complex128(8.614210391211673e-05+0j)) <f>: (np.float32(-0.0006847272), np.complex128(0.001366679867257487+0j))\n",
      "Epoch 6100: <Test loss>: 7.397665285679977e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040051546), np.complex128(8.346000799775005e-05+0j)) <f>: (np.float32(-0.0006950106), np.complex128(0.0013655777018796286+0j))\n",
      "Epoch 6200: <Test loss>: 7.559803634649143e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040213163), np.complex128(8.866420475310776e-05+0j)) <f>: (np.float32(-0.0007111679), np.complex128(0.00136002386922801+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 5000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train5e3_1h_cor.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 300:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 100 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ba60e4",
   "metadata": {},
   "source": [
    "## Less correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc7d5988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1000: <Test loss>: 7.070616993587464e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038086204), np.complex128(0.0003171916006836953+0j)) <f>: (np.float32(-0.0004984736), np.complex128(0.00135238787263224+0j))\n",
      "Epoch 2000: <Test loss>: 6.451654189731926e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003807981), np.complex128(0.0002848847586271894+0j)) <f>: (np.float32(-0.0004978356), np.complex128(0.0013816749236373485+0j))\n",
      "Epoch 3000: <Test loss>: 5.650717866956256e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00387733), np.complex128(0.00025288300059037467+0j)) <f>: (np.float32(-0.00056718255), np.complex128(0.001363743043075003+0j))\n",
      "Epoch 4000: <Test loss>: 4.966200140188448e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039230757), np.complex128(0.0002328514017406269+0j)) <f>: (np.float32(-0.00061293127), np.complex128(0.0013658682127814811+0j))\n",
      "Epoch 5000: <Test loss>: 4.2883028072537854e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039781583), np.complex128(0.00019511884043730966+0j)) <f>: (np.float32(-0.00066801184), np.complex128(0.001368160331111033+0j))\n",
      "Epoch 6000: <Test loss>: 3.9709539123578e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003983578), np.complex128(0.0001997644300810892+0j)) <f>: (np.float32(-0.0006734332), np.complex128(0.0013517947990662308+0j))\n",
      "Epoch 7000: <Test loss>: 3.6136167182121426e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040027075), np.complex128(0.00019011720050535888+0j)) <f>: (np.float32(-0.00069256156), np.complex128(0.0013699206495713418+0j))\n",
      "Epoch 8000: <Test loss>: 3.505661152303219e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004015798), np.complex128(0.0001894952344249842+0j)) <f>: (np.float32(-0.0007056519), np.complex128(0.0013680060685535426+0j))\n",
      "Epoch 9000: <Test loss>: 3.3166292269015685e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004002543), np.complex128(0.00018260375130312135+0j)) <f>: (np.float32(-0.0006924), np.complex128(0.0013611373252042373+0j))\n",
      "Epoch 10000: <Test loss>: 3.12881420541089e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004022128), np.complex128(0.00017422914025394008+0j)) <f>: (np.float32(-0.0007119817), np.complex128(0.0013763207696580078+0j))\n",
      "Epoch 11000: <Test loss>: 3.061641109525226e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040254644), np.complex128(0.000177283040964174+0j)) <f>: (np.float32(-0.00071531825), np.complex128(0.001367758309692705+0j))\n",
      "Epoch 12000: <Test loss>: 3.08699527522549e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004036574), np.complex128(0.00018210899474538104+0j)) <f>: (np.float32(-0.00072642916), np.complex128(0.0013693033456200687+0j))\n",
      "Epoch 13000: <Test loss>: 3.031942469533533e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004054454), np.complex128(0.00017722668311782115+0j)) <f>: (np.float32(-0.000744309), np.complex128(0.0013822001267524882+0j))\n",
      "Epoch 14000: <Test loss>: 3.0109675208223052e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00406249), np.complex128(0.00018296099090994098+0j)) <f>: (np.float32(-0.00075234484), np.complex128(0.001384245374245793+0j))\n",
      "Epoch 15000: <Test loss>: 3.077701694564894e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00406499), np.complex128(0.00019103793928779717+0j)) <f>: (np.float32(-0.00075484486), np.complex128(0.0013708019505473984+0j))\n",
      "Epoch 16000: <Test loss>: 3.0872874049237e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040738815), np.complex128(0.00019106499232265307+0j)) <f>: (np.float32(-0.00076373393), np.complex128(0.0013836949258601429+0j))\n",
      "Epoch 17000: <Test loss>: 3.077025030506775e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004104233), np.complex128(0.00019570162243261457+0j)) <f>: (np.float32(-0.0007940879), np.complex128(0.0013881332727650408+0j))\n",
      "Epoch 18000: <Test loss>: 3.1123370717978105e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004130854), np.complex128(0.0002029619846264696+0j)) <f>: (np.float32(-0.0008207099), np.complex128(0.0013932852108589871+0j))\n",
      "Epoch 19000: <Test loss>: 3.0827031878288835e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004118761), np.complex128(0.00019990817906172755+0j)) <f>: (np.float32(-0.00080861204), np.complex128(0.0013864489438375747+0j))\n",
      "Epoch 20000: <Test loss>: 3.100805406575091e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041566878), np.complex128(0.00020119728947353463+0j)) <f>: (np.float32(-0.0008465403), np.complex128(0.0013943745633106068+0j))\n",
      "Epoch 21000: <Test loss>: 3.1521518394583836e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041760267), np.complex128(0.00020711451447378058+0j)) <f>: (np.float32(-0.0008658809), np.complex128(0.0014056628781877467+0j))\n",
      "Epoch 22000: <Test loss>: 3.125484363408759e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00414773), np.complex128(0.0002085420457186811+0j)) <f>: (np.float32(-0.00083758106), np.complex128(0.001402437953455871+0j))\n",
      "Epoch 23000: <Test loss>: 3.197116529918276e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004174208), np.complex128(0.0002100640881521756+0j)) <f>: (np.float32(-0.0008640602), np.complex128(0.0013948027180240357+0j))\n",
      "Epoch 24000: <Test loss>: 3.214067328372039e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042094737), np.complex128(0.00021454518709808847+0j)) <f>: (np.float32(-0.0008993289), np.complex128(0.0013971797061323383+0j))\n",
      "Epoch 25000: <Test loss>: 3.32816707668826e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004194816), np.complex128(0.00022121942134684103+0j)) <f>: (np.float32(-0.00088467036), np.complex128(0.0014037664382437315+0j))\n",
      "Epoch 26000: <Test loss>: 3.254825423937291e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004212938), np.complex128(0.00021996920958362808+0j)) <f>: (np.float32(-0.0009027912), np.complex128(0.001397177042058566+0j))\n",
      "Epoch 27000: <Test loss>: 3.2540527172386646e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042223977), np.complex128(0.0002202880738420589+0j)) <f>: (np.float32(-0.0009122515), np.complex128(0.0013955920450246153+0j))\n",
      "Epoch 28000: <Test loss>: 3.274168193456717e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004241412), np.complex128(0.00021964515989589015+0j)) <f>: (np.float32(-0.0009312616), np.complex128(0.0014034792257189305+0j))\n",
      "Epoch 29000: <Test loss>: 3.3128089853562415e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004242427), np.complex128(0.0002232244540122475+0j)) <f>: (np.float32(-0.0009322802), np.complex128(0.0014019060267259722+0j))\n",
      "Epoch 30000: <Test loss>: 3.2975993235595524e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004250006), np.complex128(0.00021942340746949784+0j)) <f>: (np.float32(-0.00093985716), np.complex128(0.001398042485452644+0j))\n",
      "Epoch 31000: <Test loss>: 3.5221495636506006e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004251997), np.complex128(0.00023318165174291853+0j)) <f>: (np.float32(-0.00094184745), np.complex128(0.0013945854057205976+0j))\n",
      "Epoch 32000: <Test loss>: 3.3912005164893344e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042445366), np.complex128(0.00022408607572906863+0j)) <f>: (np.float32(-0.0009343916), np.complex128(0.0014061710819750052+0j))\n",
      "Epoch 33000: <Test loss>: 3.3929118217201903e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042535393), np.complex128(0.00022576807359198385+0j)) <f>: (np.float32(-0.0009433918), np.complex128(0.0014049381232609853+0j))\n",
      "Epoch 34000: <Test loss>: 3.385458330740221e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042467965), np.complex128(0.0002330610072592234+0j)) <f>: (np.float32(-0.0009366495), np.complex128(0.0014090453638541645+0j))\n",
      "Epoch 35000: <Test loss>: 3.4984113881364465e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004275841), np.complex128(0.00023130796742904292+0j)) <f>: (np.float32(-0.0009656978), np.complex128(0.0014039796910061831+0j))\n",
      "Epoch 36000: <Test loss>: 3.4896525903604925e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042797634), np.complex128(0.0002313204473460603+0j)) <f>: (np.float32(-0.0009696164), np.complex128(0.0014056600872533185+0j))\n",
      "Epoch 37000: <Test loss>: 3.529392051859759e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042852643), np.complex128(0.00023157036283804705+0j)) <f>: (np.float32(-0.0009751193), np.complex128(0.0014018255970701754+0j))\n",
      "Epoch 38000: <Test loss>: 3.5467688576318324e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004288023), np.complex128(0.00023388024165941503+0j)) <f>: (np.float32(-0.000977876), np.complex128(0.0014007881306267909+0j))\n",
      "Epoch 39000: <Test loss>: 3.54722433257848e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042917947), np.complex128(0.00023774229232003736+0j)) <f>: (np.float32(-0.0009816454), np.complex128(0.0014036620319239827+0j))\n",
      "Epoch 40000: <Test loss>: 3.656122134998441e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042921635), np.complex128(0.0002402109055370137+0j)) <f>: (np.float32(-0.0009820195), np.complex128(0.0014057061376713852+0j))\n",
      "Epoch 41000: <Test loss>: 3.709196607815102e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004294365), np.complex128(0.00024077272380894257+0j)) <f>: (np.float32(-0.0009842206), np.complex128(0.0014033644168254037+0j))\n",
      "Epoch 42000: <Test loss>: 3.6543809983413666e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004280355), np.complex128(0.00023371938234782152+0j)) <f>: (np.float32(-0.00097020966), np.complex128(0.0014068828971148721+0j))\n",
      "Epoch 43000: <Test loss>: 3.806594759225845e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043291473), np.complex128(0.00024716225103084676+0j)) <f>: (np.float32(-0.0010189996), np.complex128(0.0014113633617575058+0j))\n",
      "Epoch 44000: <Test loss>: 3.781487976084463e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004317129), np.complex128(0.00024452267722255635+0j)) <f>: (np.float32(-0.0010069838), np.complex128(0.0014082496938207934+0j))\n",
      "Epoch 45000: <Test loss>: 3.8349211536115035e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004306469), np.complex128(0.00024332208383335524+0j)) <f>: (np.float32(-0.00099632), np.complex128(0.0014007623779136572+0j))\n",
      "Epoch 46000: <Test loss>: 3.7918925954727456e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043187886), np.complex128(0.0002429624338740748+0j)) <f>: (np.float32(-0.0010086407), np.complex128(0.001411792785077493+0j))\n",
      "Epoch 47000: <Test loss>: 3.864440805045888e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043097204), np.complex128(0.00024391487196288882+0j)) <f>: (np.float32(-0.0009995745), np.complex128(0.0014054506403105417+0j))\n",
      "Epoch 48000: <Test loss>: 3.921292591257952e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043413956), np.complex128(0.0002456418102131321+0j)) <f>: (np.float32(-0.0010312458), np.complex128(0.0014069847662215042+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 1000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*10:10]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_1h_lesscor.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 480:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 1000 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "10ee7891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1000: <Test loss>: 0.00012618991604540497 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003915264), np.complex128(0.0004287480090012535+0j)) <f>: (np.float32(-0.0006051193), np.complex128(0.001156404524398189+0j))\n",
      "Epoch 2000: <Test loss>: 0.0001022633514367044 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003929221), np.complex128(0.00042119954625800663+0j)) <f>: (np.float32(-0.00061907305), np.complex128(0.0012019055089643818+0j))\n",
      "Epoch 3000: <Test loss>: 9.52078917180188e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039239856), np.complex128(0.0003928995980159179+0j)) <f>: (np.float32(-0.0006138394), np.complex128(0.0012357334102842995+0j))\n",
      "Epoch 4000: <Test loss>: 8.921051630750299e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038537877), np.complex128(0.00037293515602585057+0j)) <f>: (np.float32(-0.00054364075), np.complex128(0.001235828175194205+0j))\n",
      "Epoch 5000: <Test loss>: 8.414618787355721e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0037948922), np.complex128(0.00034125243668086845+0j)) <f>: (np.float32(-0.0004847465), np.complex128(0.0012596262193428247+0j))\n",
      "Epoch 6000: <Test loss>: 7.882094359956682e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003772693), np.complex128(0.0003395605912595444+0j)) <f>: (np.float32(-0.00046254325), np.complex128(0.0012433274160029897+0j))\n",
      "Epoch 7000: <Test loss>: 7.450021803379059e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038026755), np.complex128(0.00032802029940476026+0j)) <f>: (np.float32(-0.0004925283), np.complex128(0.0012453510971848032+0j))\n",
      "Epoch 8000: <Test loss>: 6.969316018512473e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0037939148), np.complex128(0.00031893333405792485+0j)) <f>: (np.float32(-0.00048377042), np.complex128(0.001252929625903482+0j))\n",
      "Epoch 9000: <Test loss>: 6.584785296581686e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038097466), np.complex128(0.0003067346988297066+0j)) <f>: (np.float32(-0.0004996019), np.complex128(0.0012528896647968954+0j))\n",
      "Epoch 10000: <Test loss>: 6.237455818336457e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038317603), np.complex128(0.00028953356736011063+0j)) <f>: (np.float32(-0.00052161166), np.complex128(0.0012565078575618469+0j))\n",
      "Epoch 11000: <Test loss>: 5.8793058997252956e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038396365), np.complex128(0.00028038193479396544+0j)) <f>: (np.float32(-0.0005294905), np.complex128(0.0012525447306736912+0j))\n",
      "Epoch 12000: <Test loss>: 5.712973143090494e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038916375), np.complex128(0.0002702817333890091+0j)) <f>: (np.float32(-0.0005814917), np.complex128(0.0012542058441011403+0j))\n",
      "Epoch 13000: <Test loss>: 5.458898522192612e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003922968), np.complex128(0.00026436917051786494+0j)) <f>: (np.float32(-0.0006128202), np.complex128(0.0012558438688892281+0j))\n",
      "Epoch 14000: <Test loss>: 5.7406585256103426e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039617317), np.complex128(0.00025955212869773+0j)) <f>: (np.float32(-0.00065158674), np.complex128(0.001248660130531495+0j))\n",
      "Epoch 15000: <Test loss>: 5.598023199127056e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039664204), np.complex128(0.00026294688813259943+0j)) <f>: (np.float32(-0.00065627747), np.complex128(0.001234080289078168+0j))\n",
      "Epoch 16000: <Test loss>: 5.195791527512483e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003982073), np.complex128(0.0002476628431787553+0j)) <f>: (np.float32(-0.0006719262), np.complex128(0.001251176919082523+0j))\n",
      "Epoch 17000: <Test loss>: 5.1558599807322025e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004014734), np.complex128(0.00023800686021777258+0j)) <f>: (np.float32(-0.00070458784), np.complex128(0.0012546833476096875+0j))\n",
      "Epoch 18000: <Test loss>: 5.159604916116223e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040215836), np.complex128(0.00024086033694937576+0j)) <f>: (np.float32(-0.00071143784), np.complex128(0.0012505096320328528+0j))\n",
      "Epoch 19000: <Test loss>: 5.387831697589718e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040477267), np.complex128(0.0002428967359119364+0j)) <f>: (np.float32(-0.00073757995), np.complex128(0.0012562697401108523+0j))\n",
      "Epoch 20000: <Test loss>: 5.040314863435924e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040374976), np.complex128(0.00023793324932222672+0j)) <f>: (np.float32(-0.0007273548), np.complex128(0.0012563670422338746+0j))\n",
      "Epoch 21000: <Test loss>: 5.1165865443181247e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004045008), np.complex128(0.00023416439367089414+0j)) <f>: (np.float32(-0.00073486153), np.complex128(0.001253638396387609+0j))\n",
      "Epoch 22000: <Test loss>: 5.2487208449747413e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004049943), np.complex128(0.00023621693565190928+0j)) <f>: (np.float32(-0.0007397983), np.complex128(0.0012503216245409116+0j))\n",
      "Epoch 23000: <Test loss>: 5.121553840581328e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004061934), np.complex128(0.00023419962921805116+0j)) <f>: (np.float32(-0.00075178494), np.complex128(0.0012537565036581874+0j))\n",
      "Epoch 24000: <Test loss>: 5.430157034425065e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040960824), np.complex128(0.00024605044424314524+0j)) <f>: (np.float32(-0.0007859338), np.complex128(0.0012426488383549504+0j))\n",
      "Epoch 25000: <Test loss>: 5.008893640479073e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004060385), np.complex128(0.00023227575565721247+0j)) <f>: (np.float32(-0.00075024145), np.complex128(0.001261231006639085+0j))\n",
      "Epoch 26000: <Test loss>: 5.230345777818002e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004076489), np.complex128(0.00023548880306018617+0j)) <f>: (np.float32(-0.00076634216), np.complex128(0.0012539857408632737+0j))\n",
      "Epoch 27000: <Test loss>: 5.144073293195106e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040715686), np.complex128(0.00023539118378552434+0j)) <f>: (np.float32(-0.00076142146), np.complex128(0.0012550508629296296+0j))\n",
      "Epoch 28000: <Test loss>: 5.21618130733259e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040693055), np.complex128(0.000235976423706036+0j)) <f>: (np.float32(-0.0007591607), np.complex128(0.0012564968206847894+0j))\n",
      "Epoch 29000: <Test loss>: 5.157592022442259e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040900544), np.complex128(0.00023409750639010734+0j)) <f>: (np.float32(-0.0007799083), np.complex128(0.001258337568800895+0j))\n",
      "Epoch 30000: <Test loss>: 5.286064333631657e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00406959), np.complex128(0.00023736034660049466+0j)) <f>: (np.float32(-0.0007594454), np.complex128(0.001257493184275685+0j))\n",
      "Epoch 31000: <Test loss>: 5.2096325816819444e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040879208), np.complex128(0.00023893421161189606+0j)) <f>: (np.float32(-0.00077777484), np.complex128(0.001260264328441654+0j))\n",
      "Epoch 32000: <Test loss>: 5.241499820840545e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040728888), np.complex128(0.00023733227868039206+0j)) <f>: (np.float32(-0.0007627435), np.complex128(0.00125817036645651+0j))\n",
      "Epoch 33000: <Test loss>: 5.3515435865847394e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004087742), np.complex128(0.0002358885885594551+0j)) <f>: (np.float32(-0.0007775969), np.complex128(0.0012613278013194839+0j))\n",
      "Epoch 34000: <Test loss>: 5.250771573628299e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040707616), np.complex128(0.00023996451042822625+0j)) <f>: (np.float32(-0.0007606198), np.complex128(0.0012648287748384477+0j))\n",
      "Epoch 35000: <Test loss>: 5.377924389904365e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040877415), np.complex128(0.0002396996570940154+0j)) <f>: (np.float32(-0.00077759475), np.complex128(0.0012642048741330716+0j))\n",
      "Epoch 36000: <Test loss>: 5.410673111327924e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004069983), np.complex128(0.0002423575939822373+0j)) <f>: (np.float32(-0.0007598382), np.complex128(0.0012619977524429265+0j))\n",
      "Epoch 37000: <Test loss>: 5.448138836072758e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004075565), np.complex128(0.00023876237885357316+0j)) <f>: (np.float32(-0.0007654198), np.complex128(0.0012640586037968985+0j))\n",
      "Epoch 38000: <Test loss>: 5.7533827202860266e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00411748), np.complex128(0.00024620611812543164+0j)) <f>: (np.float32(-0.00080733217), np.complex128(0.0012694747926369407+0j))\n",
      "Epoch 39000: <Test loss>: 5.664511991199106e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040702894), np.complex128(0.00024535692875288196+0j)) <f>: (np.float32(-0.0007601396), np.complex128(0.0012642254255593162+0j))\n",
      "Epoch 40000: <Test loss>: 5.512286224984564e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040651797), np.complex128(0.0002472957718711087+0j)) <f>: (np.float32(-0.000755033), np.complex128(0.0012614450205654716+0j))\n",
      "Epoch 41000: <Test loss>: 5.476735896081664e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040597515), np.complex128(0.0002460021420484376+0j)) <f>: (np.float32(-0.0007496064), np.complex128(0.001265629519298052+0j))\n",
      "Epoch 42000: <Test loss>: 5.6203018175438046e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040578614), np.complex128(0.00024968752338823353+0j)) <f>: (np.float32(-0.0007477135), np.complex128(0.0012613087722211092+0j))\n",
      "Epoch 43000: <Test loss>: 5.6401426263619214e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040687034), np.complex128(0.00024858088617225714+0j)) <f>: (np.float32(-0.0007585589), np.complex128(0.0012634569037062928+0j))\n",
      "Epoch 44000: <Test loss>: 5.619922012556344e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004057951), np.complex128(0.00025305570551570637+0j)) <f>: (np.float32(-0.0007478064), np.complex128(0.0012577521068742356+0j))\n",
      "Epoch 45000: <Test loss>: 5.6089513236656785e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004080417), np.complex128(0.00024845526240782064+0j)) <f>: (np.float32(-0.0007702718), np.complex128(0.0012631533261568896+0j))\n",
      "Epoch 46000: <Test loss>: 5.613181201624684e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004061175), np.complex128(0.00025119231277253505+0j)) <f>: (np.float32(-0.0007510292), np.complex128(0.0012631708329273942+0j))\n",
      "Epoch 47000: <Test loss>: 5.622769822366536e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004072786), np.complex128(0.0002498912298863339+0j)) <f>: (np.float32(-0.0007626396), np.complex128(0.0012646123505596003+0j))\n",
      "Epoch 48000: <Test loss>: 5.6208500609500334e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040684203), np.complex128(0.000249433691073498+0j)) <f>: (np.float32(-0.0007582749), np.complex128(0.0012654900994372939+0j))\n",
      "Epoch 49000: <Test loss>: 5.696727384929545e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004095373), np.complex128(0.0002473639119008718+0j)) <f>: (np.float32(-0.00078522746), np.complex128(0.0012757244559857912+0j))\n",
      "Epoch 50000: <Test loss>: 5.738477921113372e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004059148), np.complex128(0.00025626407493199634+0j)) <f>: (np.float32(-0.00074899924), np.complex128(0.001260577674261556+0j))\n",
      "Epoch 51000: <Test loss>: 5.6958349887281656e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004073142), np.complex128(0.00025129814627466193+0j)) <f>: (np.float32(-0.00076299673), np.complex128(0.0012671536232172036+0j))\n",
      "Epoch 52000: <Test loss>: 5.637188587570563e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004067238), np.complex128(0.0002512036826588139+0j)) <f>: (np.float32(-0.0007570921), np.complex128(0.0012641510852149994+0j))\n",
      "Epoch 53000: <Test loss>: 5.772897930000909e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040799584), np.complex128(0.00025353598410109997+0j)) <f>: (np.float32(-0.0007698135), np.complex128(0.0012656822933308776+0j))\n",
      "Epoch 54000: <Test loss>: 5.812798553961329e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040882146), np.complex128(0.000254997656720001+0j)) <f>: (np.float32(-0.0007780685), np.complex128(0.0012625462979187392+0j))\n",
      "Epoch 55000: <Test loss>: 5.822089588036761e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004098487), np.complex128(0.0002546625860127878+0j)) <f>: (np.float32(-0.0007883393), np.complex128(0.001267317654045193+0j))\n",
      "Epoch 56000: <Test loss>: 5.721437628380954e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004068635), np.complex128(0.00025247682448556826+0j)) <f>: (np.float32(-0.0007584892), np.complex128(0.0012704752157688226+0j))\n",
      "Epoch 57000: <Test loss>: 5.9036159655079246e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040917248), np.complex128(0.00025996231677077706+0j)) <f>: (np.float32(-0.0007815754), np.complex128(0.0012621763722463365+0j))\n",
      "Epoch 58000: <Test loss>: 5.90236704738345e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041102762), np.complex128(0.0002569366584140475+0j)) <f>: (np.float32(-0.00080013013), np.complex128(0.0012594212125230022+0j))\n",
      "Epoch 59000: <Test loss>: 5.896646325709298e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004097776), np.complex128(0.0002615992315281264+0j)) <f>: (np.float32(-0.00078763074), np.complex128(0.001262660345648331+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 500\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*10:10]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train5e3_1h_lesscor.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 300:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 1000 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfea0e9",
   "metadata": {},
   "source": [
    "## not correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5523dec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1000: <Test loss>: 7.078117778291926e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0036656081), np.complex128(0.00033176402078865635+0j)) <f>: (np.float32(-0.0003554611), np.complex128(0.0013215891500522512+0j))\n",
      "Epoch 2000: <Test loss>: 6.659529753960669e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0037704762), np.complex128(0.0002968838737589497+0j)) <f>: (np.float32(-0.0004603293), np.complex128(0.0013846416869346086+0j))\n",
      "Epoch 3000: <Test loss>: 6.086563735152595e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038585153), np.complex128(0.0002487962955658579+0j)) <f>: (np.float32(-0.0005483698), np.complex128(0.00139009136698779+0j))\n",
      "Epoch 4000: <Test loss>: 5.5228818382602185e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003911488), np.complex128(0.00021877055081942837+0j)) <f>: (np.float32(-0.0006013405), np.complex128(0.0013843440718360293+0j))\n",
      "Epoch 5000: <Test loss>: 4.856935265706852e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039220694), np.complex128(0.0001937250858420241+0j)) <f>: (np.float32(-0.0006119237), np.complex128(0.001388034955756772+0j))\n",
      "Epoch 6000: <Test loss>: 4.533864921540953e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003934082), np.complex128(0.00017798367650874577+0j)) <f>: (np.float32(-0.00062393694), np.complex128(0.0013947371310649712+0j))\n",
      "Epoch 7000: <Test loss>: 3.9832841139286757e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039415285), np.complex128(0.0001693648269819262+0j)) <f>: (np.float32(-0.00063138513), np.complex128(0.0014017425033406063+0j))\n",
      "Epoch 8000: <Test loss>: 3.830050627584569e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003972864), np.complex128(0.0001777559616315294+0j)) <f>: (np.float32(-0.0006627204), np.complex128(0.0013626783015906147+0j))\n",
      "Epoch 9000: <Test loss>: 3.5098197258776054e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004010571), np.complex128(0.0001704310273638565+0j)) <f>: (np.float32(-0.00070042507), np.complex128(0.0013904226001601643+0j))\n",
      "Epoch 10000: <Test loss>: 3.364730946486816e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00402868), np.complex128(0.0001685586750868671+0j)) <f>: (np.float32(-0.0007185355), np.complex128(0.0013998761293720228+0j))\n",
      "Epoch 11000: <Test loss>: 3.302430923213251e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040147747), np.complex128(0.00017348484878618054+0j)) <f>: (np.float32(-0.00070463074), np.complex128(0.0013981207584772915+0j))\n",
      "Epoch 12000: <Test loss>: 3.256914351368323e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00403747), np.complex128(0.0001801265908494584+0j)) <f>: (np.float32(-0.0007273236), np.complex128(0.001401505020192891+0j))\n",
      "Epoch 13000: <Test loss>: 3.17794838338159e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004035571), np.complex128(0.00018215866069213883+0j)) <f>: (np.float32(-0.0007254264), np.complex128(0.001407152095426545+0j))\n",
      "Epoch 14000: <Test loss>: 3.1420659070136026e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004028203), np.complex128(0.00018873288117135072+0j)) <f>: (np.float32(-0.0007180571), np.complex128(0.0013950837143767011+0j))\n",
      "Epoch 15000: <Test loss>: 3.2033989555202425e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004034738), np.complex128(0.00019231758272316286+0j)) <f>: (np.float32(-0.0007245914), np.complex128(0.0014061109500241413+0j))\n",
      "Epoch 16000: <Test loss>: 3.141651541227475e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040256297), np.complex128(0.00019054123224747347+0j)) <f>: (np.float32(-0.00071548385), np.complex128(0.0013880651485928598+0j))\n",
      "Epoch 17000: <Test loss>: 3.091041435254738e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040326063), np.complex128(0.00019241065087179676+0j)) <f>: (np.float32(-0.0007224612), np.complex128(0.001395679198295171+0j))\n",
      "Epoch 18000: <Test loss>: 3.1061739719007164e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004040885), np.complex128(0.00019237390885435177+0j)) <f>: (np.float32(-0.0007307384), np.complex128(0.0013937274471052135+0j))\n",
      "Epoch 19000: <Test loss>: 3.1115952879190445e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004046871), np.complex128(0.00019214256259086223+0j)) <f>: (np.float32(-0.00073672575), np.complex128(0.0013900600324057997+0j))\n",
      "Epoch 20000: <Test loss>: 3.1296400266001e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040284796), np.complex128(0.00019673293613419143+0j)) <f>: (np.float32(-0.0007183351), np.complex128(0.0013887718893064933+0j))\n",
      "Epoch 21000: <Test loss>: 3.202631341991946e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040210444), np.complex128(0.0001983331405892597+0j)) <f>: (np.float32(-0.0007108994), np.complex128(0.0013846659173198722+0j))\n",
      "Epoch 22000: <Test loss>: 3.1533905712421983e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040278267), np.complex128(0.00020019856310292433+0j)) <f>: (np.float32(-0.000717677), np.complex128(0.0013761890882972552+0j))\n",
      "Epoch 23000: <Test loss>: 3.223880776204169e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004072109), np.complex128(0.0001983305875185611+0j)) <f>: (np.float32(-0.0007619629), np.complex128(0.0013800507266607462+0j))\n",
      "Epoch 24000: <Test loss>: 3.2631258363835514e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004026601), np.complex128(0.00020050775423634807+0j)) <f>: (np.float32(-0.0007164564), np.complex128(0.0013843879656229468+0j))\n",
      "Epoch 25000: <Test loss>: 3.222818122594617e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040428042), np.complex128(0.0002022798073073212+0j)) <f>: (np.float32(-0.0007326579), np.complex128(0.0013811201619894002+0j))\n",
      "Epoch 26000: <Test loss>: 3.2771193218650296e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004053276), np.complex128(0.0002038664218146336+0j)) <f>: (np.float32(-0.0007431284), np.complex128(0.0013733413202951645+0j))\n",
      "Epoch 27000: <Test loss>: 3.3077627449529245e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00402918), np.complex128(0.0002038311862674766+0j)) <f>: (np.float32(-0.0007190351), np.complex128(0.001370944034481929+0j))\n",
      "Epoch 28000: <Test loss>: 3.38271347573027e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040481286), np.complex128(0.00020477563213497325+0j)) <f>: (np.float32(-0.0007379819), np.complex128(0.001376355910059673+0j))\n",
      "Epoch 29000: <Test loss>: 3.341323463246226e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040593767), np.complex128(0.00020706681486718818+0j)) <f>: (np.float32(-0.00074923254), np.complex128(0.0013642197854196153+0j))\n",
      "Epoch 30000: <Test loss>: 3.3766798878787085e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004050028), np.complex128(0.00020391567546425993+0j)) <f>: (np.float32(-0.00073988416), np.complex128(0.001371639992039817+0j))\n",
      "Epoch 31000: <Test loss>: 3.4124383091693744e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004057869), np.complex128(0.0002050567553482944+0j)) <f>: (np.float32(-0.00074772426), np.complex128(0.001370824151162169+0j))\n",
      "Epoch 32000: <Test loss>: 3.455174737609923e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004056136), np.complex128(0.00020519343184737015+0j)) <f>: (np.float32(-0.00074599206), np.complex128(0.001370473000866829+0j))\n",
      "Epoch 33000: <Test loss>: 3.4733133361442015e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004046812), np.complex128(0.00020392151105442813+0j)) <f>: (np.float32(-0.0007366669), np.complex128(0.0013678934162911648+0j))\n",
      "Epoch 34000: <Test loss>: 3.5095807106699795e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040589883), np.complex128(0.00020414451622979677+0j)) <f>: (np.float32(-0.0007488428), np.complex128(0.0013793362474471067+0j))\n",
      "Epoch 35000: <Test loss>: 3.55121010215953e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040821005), np.complex128(0.00021096229331062194+0j)) <f>: (np.float32(-0.00077195646), np.complex128(0.0013597111577113871+0j))\n",
      "Epoch 36000: <Test loss>: 3.6619716411223635e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040470236), np.complex128(0.00020888894618205055+0j)) <f>: (np.float32(-0.000736877), np.complex128(0.0013623150995329709+0j))\n",
      "Epoch 37000: <Test loss>: 3.592812572605908e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040419046), np.complex128(0.0002091169623533245+0j)) <f>: (np.float32(-0.0007317567), np.complex128(0.00135981087018687+0j))\n",
      "Epoch 38000: <Test loss>: 3.644407479441725e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004061306), np.complex128(0.0002090967915090474+0j)) <f>: (np.float32(-0.0007511601), np.complex128(0.0013575254754720773+0j))\n",
      "Epoch 39000: <Test loss>: 3.706560528371483e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004061256), np.complex128(0.00020907268798443953+0j)) <f>: (np.float32(-0.00075110927), np.complex128(0.0013595072926374668+0j))\n",
      "Epoch 40000: <Test loss>: 3.756879596039653e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004093391), np.complex128(0.00021378700438851543+0j)) <f>: (np.float32(-0.000783243), np.complex128(0.0013606277259497648+0j))\n",
      "Epoch 41000: <Test loss>: 3.8802689232397825e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040712436), np.complex128(0.00022154519951101464+0j)) <f>: (np.float32(-0.00076109596), np.complex128(0.0013263961540229934+0j))\n",
      "Epoch 42000: <Test loss>: 3.7821118894498795e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040441686), np.complex128(0.0002175447914476268+0j)) <f>: (np.float32(-0.0007340223), np.complex128(0.001355442423503335+0j))\n",
      "Epoch 43000: <Test loss>: 3.843250669888221e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040566702), np.complex128(0.000212029429289889+0j)) <f>: (np.float32(-0.0007465252), np.complex128(0.001359417221571827+0j))\n",
      "Epoch 44000: <Test loss>: 3.8199166738195345e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004080572), np.complex128(0.00021102031620308257+0j)) <f>: (np.float32(-0.0007704249), np.complex128(0.0013708814921786045+0j))\n",
      "Epoch 45000: <Test loss>: 3.8992999179754406e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040648407), np.complex128(0.00021812768444600558+0j)) <f>: (np.float32(-0.0007546941), np.complex128(0.0013545472947157926+0j))\n",
      "Epoch 46000: <Test loss>: 3.9369908336084336e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040789815), np.complex128(0.00021759081015052943+0j)) <f>: (np.float32(-0.00076883583), np.complex128(0.001351051395623062+0j))\n",
      "Epoch 47000: <Test loss>: 3.959531750297174e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004047677), np.complex128(0.00021823781535284872+0j)) <f>: (np.float32(-0.00073753187), np.complex128(0.0013573174239965148+0j))\n",
      "Epoch 48000: <Test loss>: 3.985185321653262e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040548434), np.complex128(0.00022134867649755067+0j)) <f>: (np.float32(-0.0007447003), np.complex128(0.001358035709029829+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 1000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*100:100]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_1h_notcor.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 480:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 1000 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26743e28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1000: <Test loss>: 0.00013046312960796058 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003588918), np.complex128(0.00033545750535284344+0j)) <f>: (np.float32(-0.00027877043), np.complex128(0.0013596951732687524+0j))\n",
      "Epoch 2000: <Test loss>: 0.00010999697406077757 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0035916779), np.complex128(0.0003455206634462972+0j)) <f>: (np.float32(-0.00028153358), np.complex128(0.0013073457430581874+0j))\n",
      "Epoch 3000: <Test loss>: 0.00011215556151000783 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003627803), np.complex128(0.00034207982187819724+0j)) <f>: (np.float32(-0.00031765693), np.complex128(0.0013419049960589698+0j))\n",
      "Epoch 4000: <Test loss>: 0.000104175167507492 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0035819851), np.complex128(0.00034927326507610146+0j)) <f>: (np.float32(-0.00027183723), np.complex128(0.0012895190299795256+0j))\n",
      "Epoch 5000: <Test loss>: 0.00010291319631505758 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0035463863), np.complex128(0.00033014740373623995+0j)) <f>: (np.float32(-0.00023623982), np.complex128(0.0013282342380653265+0j))\n",
      "Epoch 6000: <Test loss>: 9.918052091961727e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0035443248), np.complex128(0.00032229323852760476+0j)) <f>: (np.float32(-0.00023417658), np.complex128(0.0013063649833279592+0j))\n",
      "Epoch 7000: <Test loss>: 9.771910117706284e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003531606), np.complex128(0.0003090933872884055+0j)) <f>: (np.float32(-0.00022145899), np.complex128(0.0013074353066812039+0j))\n",
      "Epoch 8000: <Test loss>: 9.263465472031385e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0035970192), np.complex128(0.0002877012237624539+0j)) <f>: (np.float32(-0.00028687128), np.complex128(0.0013113797850529522+0j))\n",
      "Epoch 9000: <Test loss>: 8.865015843184665e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0035994295), np.complex128(0.0002742137794164861+0j)) <f>: (np.float32(-0.00028928227), np.complex128(0.001300597009889298+0j))\n",
      "Epoch 10000: <Test loss>: 8.423670078627765e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0036128124), np.complex128(0.0002585111577287281+0j)) <f>: (np.float32(-0.00030266395), np.complex128(0.0012890204676021103+0j))\n",
      "Epoch 11000: <Test loss>: 8.396265184273943e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003652312), np.complex128(0.0002442686387592551+0j)) <f>: (np.float32(-0.00034216506), np.complex128(0.0013049215627859156+0j))\n",
      "Epoch 12000: <Test loss>: 7.663850556127727e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0037222842), np.complex128(0.0002502322471867892+0j)) <f>: (np.float32(-0.0004121372), np.complex128(0.0012707822185559334+0j))\n",
      "Epoch 13000: <Test loss>: 7.321676821447909e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0037317658), np.complex128(0.00024135104982371143+0j)) <f>: (np.float32(-0.0004216201), np.complex128(0.0012744479840668213+0j))\n",
      "Epoch 14000: <Test loss>: 7.072181324474514e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003782639), np.complex128(0.00023835927911967065+0j)) <f>: (np.float32(-0.00047249513), np.complex128(0.0012748377000015338+0j))\n",
      "Epoch 15000: <Test loss>: 6.907873466843739e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003788204), np.complex128(0.00023035016947752006+0j)) <f>: (np.float32(-0.0004780598), np.complex128(0.0012871348108138407+0j))\n",
      "Epoch 16000: <Test loss>: 6.618821498705074e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038135436), np.complex128(0.00024497609306407776+0j)) <f>: (np.float32(-0.00050340185), np.complex128(0.0012683834104148279+0j))\n",
      "Epoch 17000: <Test loss>: 6.330189353320748e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038276582), np.complex128(0.00023994381628374384+0j)) <f>: (np.float32(-0.000517513), np.complex128(0.0012743069150175374+0j))\n",
      "Epoch 18000: <Test loss>: 6.345089786918834e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038180242), np.complex128(0.0002507500765263087+0j)) <f>: (np.float32(-0.000507878), np.complex128(0.0012701677055390887+0j))\n",
      "Epoch 19000: <Test loss>: 6.0767419199692085e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038466651), np.complex128(0.00024045994886199163+0j)) <f>: (np.float32(-0.0005365183), np.complex128(0.00127536544032979+0j))\n",
      "Epoch 20000: <Test loss>: 5.963636431260966e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038983384), np.complex128(0.00023890068868359274+0j)) <f>: (np.float32(-0.0005881922), np.complex128(0.0012853305985666144+0j))\n",
      "Epoch 21000: <Test loss>: 5.8106699725613e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038814566), np.complex128(0.000245788857570822+0j)) <f>: (np.float32(-0.0005713084), np.complex128(0.0012773725027656897+0j))\n",
      "Epoch 22000: <Test loss>: 5.7240755268139765e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038971829), np.complex128(0.00024245581584501433+0j)) <f>: (np.float32(-0.0005870409), np.complex128(0.0012883870523475469+0j))\n",
      "Epoch 23000: <Test loss>: 5.651979518006556e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003941354), np.complex128(0.00023614598883013588+0j)) <f>: (np.float32(-0.0006312104), np.complex128(0.0012996043252574217+0j))\n",
      "Epoch 24000: <Test loss>: 5.440544555312954e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003917414), np.complex128(0.00023241463636018329+0j)) <f>: (np.float32(-0.00060726784), np.complex128(0.0012904616046523487+0j))\n",
      "Epoch 25000: <Test loss>: 5.545955355046317e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039102784), np.complex128(0.00023912475631695398+0j)) <f>: (np.float32(-0.0006001339), np.complex128(0.0012988785554454135+0j))\n",
      "Epoch 26000: <Test loss>: 5.523567961063236e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003903239), np.complex128(0.0002417873236191137+0j)) <f>: (np.float32(-0.00059309416), np.complex128(0.001299192535568595+0j))\n",
      "Epoch 27000: <Test loss>: 5.593345485976897e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003904042), np.complex128(0.00024307616445202714+0j)) <f>: (np.float32(-0.0005938984), np.complex128(0.0013031377751042783+0j))\n",
      "Epoch 28000: <Test loss>: 5.558167686103843e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038971098), np.complex128(0.00024287935600208758+0j)) <f>: (np.float32(-0.00058696466), np.complex128(0.0012968430762226077+0j))\n",
      "Epoch 29000: <Test loss>: 5.497591700986959e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039127264), np.complex128(0.0002347738481190875+0j)) <f>: (np.float32(-0.0006025817), np.complex128(0.0013090808162479857+0j))\n",
      "Epoch 30000: <Test loss>: 5.4850410379003733e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003912125), np.complex128(0.00023691822135734218+0j)) <f>: (np.float32(-0.0006019779), np.complex128(0.0013091915656005262+0j))\n",
      "Epoch 31000: <Test loss>: 5.442356996354647e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003906149), np.complex128(0.00024427266658507774+0j)) <f>: (np.float32(-0.00059600256), np.complex128(0.0013070161590743388+0j))\n",
      "Epoch 32000: <Test loss>: 5.916026566410437e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038725366), np.complex128(0.00024048211776159806+0j)) <f>: (np.float32(-0.00056239), np.complex128(0.0013212325447487106+0j))\n",
      "Epoch 33000: <Test loss>: 5.422939284471795e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038998122), np.complex128(0.0002359251878586623+0j)) <f>: (np.float32(-0.00058966473), np.complex128(0.0013162294142040538+0j))\n",
      "Epoch 34000: <Test loss>: 5.7851295423461124e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038667303), np.complex128(0.00025821487466703513+0j)) <f>: (np.float32(-0.00055658456), np.complex128(0.0013056853907946733+0j))\n",
      "Epoch 35000: <Test loss>: 5.4730833653593436e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003912087), np.complex128(0.00024034074741825656+0j)) <f>: (np.float32(-0.0006019402), np.complex128(0.0012973392282475621+0j))\n",
      "Epoch 36000: <Test loss>: 5.512505231308751e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039286353), np.complex128(0.00023567720699167695+0j)) <f>: (np.float32(-0.00061849254), np.complex128(0.0013204849549038995+0j))\n",
      "Epoch 37000: <Test loss>: 5.540929123526439e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003928468), np.complex128(0.00021798491863544994+0j)) <f>: (np.float32(-0.0006183232), np.complex128(0.0013405652206727402+0j))\n",
      "Epoch 38000: <Test loss>: 5.4257507144939154e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039236476), np.complex128(0.000230183712439488+0j)) <f>: (np.float32(-0.0006134993), np.complex128(0.0013214496033308373+0j))\n",
      "Epoch 39000: <Test loss>: 5.595458424068056e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003907383), np.complex128(0.00023997335895897045+0j)) <f>: (np.float32(-0.00059723295), np.complex128(0.0013052359234910645+0j))\n",
      "Epoch 40000: <Test loss>: 5.965263335383497e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0038515558), np.complex128(0.0002558958777360293+0j)) <f>: (np.float32(-0.0005414083), np.complex128(0.0013091944833956103+0j))\n",
      "Epoch 41000: <Test loss>: 5.640209565171972e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039388314), np.complex128(0.00023539716209393036+0j)) <f>: (np.float32(-0.00062868546), np.complex128(0.0013247521667840816+0j))\n",
      "Epoch 42000: <Test loss>: 5.584955943049863e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039413143), np.complex128(0.00023659302992370178+0j)) <f>: (np.float32(-0.000631168), np.complex128(0.0013206909766089686+0j))\n",
      "Epoch 43000: <Test loss>: 5.629398947348818e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003949006), np.complex128(0.0002393222149277547+0j)) <f>: (np.float32(-0.000638859), np.complex128(0.0013185546431647773+0j))\n",
      "Epoch 44000: <Test loss>: 5.635379420709796e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039535314), np.complex128(0.00023879816941609945+0j)) <f>: (np.float32(-0.0006433805), np.complex128(0.0013271691159989706+0j))\n",
      "Epoch 45000: <Test loss>: 5.6524277169955894e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039474294), np.complex128(0.00023678810989720566+0j)) <f>: (np.float32(-0.0006372836), np.complex128(0.001318661332976331+0j))\n",
      "Epoch 46000: <Test loss>: 5.63026696909219e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039534057), np.complex128(0.00023285825221604177+0j)) <f>: (np.float32(-0.0006432597), np.complex128(0.0013353969175541917+0j))\n",
      "Epoch 47000: <Test loss>: 5.7237062719650567e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003953227), np.complex128(0.0002391228692646985+0j)) <f>: (np.float32(-0.00064308237), np.complex128(0.0013250131191531256+0j))\n",
      "Epoch 48000: <Test loss>: 5.5411011999240145e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003960623), np.complex128(0.00022711580948436675+0j)) <f>: (np.float32(-0.00065047893), np.complex128(0.001316804473556935+0j))\n",
      "Epoch 49000: <Test loss>: 5.9077567129861563e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039653373), np.complex128(0.00024281564023769653+0j)) <f>: (np.float32(-0.00065519207), np.complex128(0.0013177084825903855+0j))\n",
      "Epoch 50000: <Test loss>: 5.8986835938412696e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039661024), np.complex128(0.0002402433818649064+0j)) <f>: (np.float32(-0.00065595575), np.complex128(0.0013179561145905674+0j))\n",
      "Epoch 51000: <Test loss>: 5.886559665668756e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039826976), np.complex128(0.0002374831635729209+0j)) <f>: (np.float32(-0.00067255367), np.complex128(0.0013220887273149128+0j))\n",
      "Epoch 52000: <Test loss>: 5.902341945329681e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039907615), np.complex128(0.00024000957767621016+0j)) <f>: (np.float32(-0.0006806156), np.complex128(0.0013156506758921535+0j))\n",
      "Epoch 53000: <Test loss>: 5.815057738800533e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039998386), np.complex128(0.00024014952083717354+0j)) <f>: (np.float32(-0.0006896936), np.complex128(0.0013287582994345637+0j))\n",
      "Epoch 54000: <Test loss>: 5.9152258472749963e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039850865), np.complex128(0.00023910480747882457+0j)) <f>: (np.float32(-0.00067493896), np.complex128(0.0013216766839047743+0j))\n",
      "Epoch 55000: <Test loss>: 5.89855553698726e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003988525), np.complex128(0.00023935513526794278+0j)) <f>: (np.float32(-0.0006783783), np.complex128(0.0013206113081171069+0j))\n",
      "Epoch 56000: <Test loss>: 5.939120819675736e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.003996208), np.complex128(0.00023985528340355589+0j)) <f>: (np.float32(-0.00068606454), np.complex128(0.0013188999578699488+0j))\n",
      "Epoch 57000: <Test loss>: 5.95772908127401e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004003868), np.complex128(0.00024278781018132365+0j)) <f>: (np.float32(-0.00069372554), np.complex128(0.001318959074935566+0j))\n",
      "Epoch 58000: <Test loss>: 5.985847019474022e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039973417), np.complex128(0.00023882964671632746+0j)) <f>: (np.float32(-0.0006871941), np.complex128(0.0013189527319027745+0j))\n",
      "Epoch 59000: <Test loss>: 5.96447462157812e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004015131), np.complex128(0.00024029626690080588+0j)) <f>: (np.float32(-0.00070498517), np.complex128(0.0013204489264776434+0j))\n",
      "Epoch 60000: <Test loss>: 6.03050175413955e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039998502), np.complex128(0.00024027395528296162+0j)) <f>: (np.float32(-0.0006897024), np.complex128(0.001317633254221478+0j))\n",
      "Epoch 61000: <Test loss>: 6.180123455123976e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040110443), np.complex128(0.0002439192962282609+0j)) <f>: (np.float32(-0.0007008967), np.complex128(0.0013186017084680906+0j))\n",
      "Epoch 62000: <Test loss>: 6.089235466788523e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040099532), np.complex128(0.0002441613305020042+0j)) <f>: (np.float32(-0.0006998051), np.complex128(0.0013159076955808668+0j))\n",
      "Epoch 63000: <Test loss>: 6.0648882936220616e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040105195), np.complex128(0.00023446327737603162+0j)) <f>: (np.float32(-0.0007003739), np.complex128(0.0013314530734857225+0j))\n",
      "Epoch 64000: <Test loss>: 6.151469278847799e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040208157), np.complex128(0.00024328846575956008+0j)) <f>: (np.float32(-0.00071067223), np.complex128(0.0013165995935977683+0j))\n",
      "Epoch 65000: <Test loss>: 6.191020656842738e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004032163), np.complex128(0.00024244793462677082+0j)) <f>: (np.float32(-0.00072201487), np.complex128(0.0013154554373428298+0j))\n",
      "Epoch 66000: <Test loss>: 6.248972931643948e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040328535), np.complex128(0.00024463360100849846+0j)) <f>: (np.float32(-0.0007227059), np.complex128(0.0013152277224656134+0j))\n",
      "Epoch 67000: <Test loss>: 6.552609556820244e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040090024), np.complex128(0.00025494126715848415+0j)) <f>: (np.float32(-0.00069885753), np.complex128(0.0013062018405245606+0j))\n",
      "Epoch 68000: <Test loss>: 6.243457755772397e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00403003), np.complex128(0.00024598877410682946+0j)) <f>: (np.float32(-0.0007198859), np.complex128(0.001315918098154645+0j))\n",
      "Epoch 69000: <Test loss>: 6.183459481690079e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040199785), np.complex128(0.000246050586961383+0j)) <f>: (np.float32(-0.0007098338), np.complex128(0.0013131167611525882+0j))\n",
      "Epoch 70000: <Test loss>: 6.326790753519163e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040306523), np.complex128(0.0002471612202880181+0j)) <f>: (np.float32(-0.0007205068), np.complex128(0.0013093739912236107+0j))\n",
      "Epoch 71000: <Test loss>: 6.403449515346438e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00402138), np.complex128(0.0002478849603295331+0j)) <f>: (np.float32(-0.0007112362), np.complex128(0.0013102930966751053+0j))\n",
      "Epoch 72000: <Test loss>: 6.348752503981814e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004028313), np.complex128(0.0002501422871242231+0j)) <f>: (np.float32(-0.00071816583), np.complex128(0.0013093533129367105+0j))\n",
      "Epoch 73000: <Test loss>: 6.362983549479395e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004040155), np.complex128(0.00025566372273585884+0j)) <f>: (np.float32(-0.00073000626), np.complex128(0.0013007087741270852+0j))\n",
      "Epoch 74000: <Test loss>: 6.417647091438994e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040297154), np.complex128(0.00024457369106378236+0j)) <f>: (np.float32(-0.0007195721), np.complex128(0.0013111331679380171+0j))\n",
      "Epoch 75000: <Test loss>: 6.418544944608584e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004035188), np.complex128(0.0002490922773334932+0j)) <f>: (np.float32(-0.000725043), np.complex128(0.0013078284478536236+0j))\n",
      "Epoch 76000: <Test loss>: 6.476385169662535e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040334025), np.complex128(0.00025235207288814063+0j)) <f>: (np.float32(-0.0007232566), np.complex128(0.0013054052824665987+0j))\n",
      "Epoch 77000: <Test loss>: 6.492876855190843e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040331003), np.complex128(0.0002512776899939092+0j)) <f>: (np.float32(-0.0007229509), np.complex128(0.001306049354016252+0j))\n",
      "Epoch 78000: <Test loss>: 6.463009776780382e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040330035), np.complex128(0.00024916672868088396+0j)) <f>: (np.float32(-0.000722859), np.complex128(0.0013072286506728553+0j))\n",
      "Epoch 79000: <Test loss>: 6.604976806556806e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004028335), np.complex128(0.00025004217820919064+0j)) <f>: (np.float32(-0.0007181909), np.complex128(0.001310589443167126+0j))\n",
      "Epoch 80000: <Test loss>: 6.628664414165542e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040325625), np.complex128(0.0002505749771060982+0j)) <f>: (np.float32(-0.0007224149), np.complex128(0.0013069782277382455+0j))\n",
      "Epoch 81000: <Test loss>: 6.634377496084198e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004025705), np.complex128(0.00025411502370705787+0j)) <f>: (np.float32(-0.0007155596), np.complex128(0.0013067442966888934+0j))\n",
      "Epoch 82000: <Test loss>: 6.733869668096304e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004032891), np.complex128(0.00025288774200738634+0j)) <f>: (np.float32(-0.0007227436), np.complex128(0.0013089685445675755+0j))\n",
      "Epoch 83000: <Test loss>: 6.660470535280183e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004053562), np.complex128(0.0002495662763164232+0j)) <f>: (np.float32(-0.0007434164), np.complex128(0.0013115680462662052+0j))\n",
      "Epoch 84000: <Test loss>: 6.739656964782625e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040340875), np.complex128(0.000250976348363565+0j)) <f>: (np.float32(-0.0007239401), np.complex128(0.0013063921315083068+0j))\n",
      "Epoch 85000: <Test loss>: 6.719099474139512e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00406397), np.complex128(0.00024952200194753823+0j)) <f>: (np.float32(-0.0007538221), np.complex128(0.0013134780603003945+0j))\n",
      "Epoch 86000: <Test loss>: 6.806840974604711e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004024964), np.complex128(0.00025183234063878363+0j)) <f>: (np.float32(-0.0007148196), np.complex128(0.001306158834762234+0j))\n",
      "Epoch 87000: <Test loss>: 6.818765541538596e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00403638), np.complex128(0.00025347623273220366+0j)) <f>: (np.float32(-0.0007262345), np.complex128(0.0013047212498103588+0j))\n",
      "Epoch 88000: <Test loss>: 6.889731594128534e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040305182), np.complex128(0.0002612214880678081+0j)) <f>: (np.float32(-0.00072037155), np.complex128(0.0012948445134506486+0j))\n",
      "Epoch 89000: <Test loss>: 6.866818876005709e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004039889), np.complex128(0.000258472956813741+0j)) <f>: (np.float32(-0.0007297428), np.complex128(0.0013035633926045908+0j))\n",
      "Epoch 90000: <Test loss>: 6.649250281043351e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040309075), np.complex128(0.00024628973515520615+0j)) <f>: (np.float32(-0.00072076276), np.complex128(0.0013168866792619134+0j))\n",
      "Epoch 91000: <Test loss>: 7.040194032015279e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040323124), np.complex128(0.00025574562714677964+0j)) <f>: (np.float32(-0.0007221662), np.complex128(0.0013034459196372913+0j))\n",
      "Epoch 92000: <Test loss>: 7.024884689599276e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040315045), np.complex128(0.00025452686096863074+0j)) <f>: (np.float32(-0.0007213561), np.complex128(0.001305750089729147+0j))\n",
      "Epoch 93000: <Test loss>: 6.965258944546804e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040352526), np.complex128(0.0002564485144679924+0j)) <f>: (np.float32(-0.0007251071), np.complex128(0.0013022738540380701+0j))\n",
      "Epoch 94000: <Test loss>: 7.043611549306661e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040332917), np.complex128(0.0002553488545880867+0j)) <f>: (np.float32(-0.0007231478), np.complex128(0.0013023598655627236+0j))\n",
      "Epoch 95000: <Test loss>: 7.102280505932868e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040056626), np.complex128(0.0002532011671151984+0j)) <f>: (np.float32(-0.00069551566), np.complex128(0.0013058321685734695+0j))\n",
      "Epoch 96000: <Test loss>: 7.115799962775782e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040221545), np.complex128(0.00025865270250547133+0j)) <f>: (np.float32(-0.00071200944), np.complex128(0.001302999243268111+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 500\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*100:100]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train5e3_1h_notcor.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 480:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 1000 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0efd1a",
   "metadata": {},
   "source": [
    "## sweep1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90bb08ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.003559981), np.complex128(0.0001198993089513105+0j))\n",
      "bin size 1: (np.float32(0.003559981), np.complex128(0.00011990134551381811+0j))\n",
      "jack bin size 2: (np.float32(0.003559981), np.complex128(0.00016616860248677823+0j))\n",
      "bin size 2: (np.float32(0.003559981), np.complex128(0.00016616854843756865+0j))\n",
      "jack bin size 4: (np.float32(0.003559981), np.complex128(0.00022852752223321844+0j))\n",
      "bin size 4: (np.float32(0.003559981), np.complex128(0.0002285271755981584+0j))\n",
      "jack bin size 5: (np.float32(0.003559981), np.complex128(0.0002523341360845214+0j))\n",
      "bin size 5: (np.float32(0.003559981), np.complex128(0.0002523335635520957+0j))\n",
      "jack bin size 10: (np.float32(0.003559981), np.complex128(0.0003383678146177811+0j))\n",
      "bin size 10: (np.float32(0.003559981), np.complex128(0.0003383683100943418+0j))\n",
      "jack bin size 20: (np.float32(0.003559981), np.complex128(0.0004391616290682918+0j))\n",
      "bin size 20: (np.float32(0.003559981), np.complex128(0.00043916189680241264+0j))\n",
      "jack bin size 50: (np.float32(0.003559981), np.complex128(0.0005626122479360114+0j))\n",
      "bin size 50: (np.float32(0.003559981), np.complex128(0.0005626122502753421+0j))\n",
      "jack bin size 100: (np.float32(0.003559981), np.complex128(0.0006289310575087525+0j))\n",
      "bin size 100: (np.float32(0.003559981), np.complex128(0.0006289308288614985+0j))\n",
      "jack bin size 200: (np.float32(0.003559981), np.complex128(0.0006601814626574177+0j))\n",
      "bin size 200: (np.float32(0.003559981), np.complex128(0.0006601812834318607+0j))\n",
      "jack bin size 500: (np.float32(0.003559981), np.complex128(0.0006577933112743479+0j))\n",
      "bin size 500: (np.float32(0.003559981), np.complex128(0.0006577933574365648+0j))\n",
      "jack bin size 1000: (np.float32(0.003559981), np.complex128(0.0007126670606370409+0j))\n",
      "bin size 1000: (np.float32(0.003559981), np.complex128(0.0007126670942750586+0j))\n",
      "jack bin size 2000: (np.float32(0.003559981), np.complex128(0.0006881249501020648+0j))\n",
      "bin size 2000: (np.float32(0.003559981), np.complex128(0.0006881245145840304+0j))\n",
      "jack bin size 5000: (np.float32(0.003559981), np.complex128(0.0006710194170254357+0j))\n",
      "bin size 5000: (np.float32(0.003559981), np.complex128(0.0006710195839473511+0j))\n",
      "jack bin size 10000: (np.float32(0.003559981), np.complex128(0.0008403770334552974+0j))\n",
      "bin size 10000: (np.float32(0.003559981), np.complex128(0.0008403771401693423+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_16x16_0.1_0.5_sweep1.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(16,16), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ecb58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j) for i, j in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d3093d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 1.6973221136140637e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041135307), np.complex128(0.00012021882655086468+0j)) <f>: (np.float32(-0.00080338574), np.complex128(0.0013654004775434329+0j))\n",
      "Epoch 200: <Test loss>: 8.363538654521108e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042029032), np.complex128(9.973308241082446e-05+0j)) <f>: (np.float32(-0.0008927599), np.complex128(0.001360180795859273+0j))\n",
      "Epoch 300: <Test loss>: 5.3856133490626235e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041061393), np.complex128(7.308318693275455e-05+0j)) <f>: (np.float32(-0.00079599564), np.complex128(0.0014125441807419793+0j))\n",
      "Epoch 400: <Test loss>: 3.949764050048543e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004122831), np.complex128(6.281355122874963e-05+0j)) <f>: (np.float32(-0.00081268646), np.complex128(0.001392125323882726+0j))\n",
      "Epoch 500: <Test loss>: 3.2355690109397983e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041155806), np.complex128(6.609901624422528e-05+0j)) <f>: (np.float32(-0.000805434), np.complex128(0.0013965414701728534+0j))\n",
      "Epoch 600: <Test loss>: 3.513607907734695e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004119512), np.complex128(6.440882397582248e-05+0j)) <f>: (np.float32(-0.000809363), np.complex128(0.0013776122111343657+0j))\n",
      "Epoch 700: <Test loss>: 2.9406382964225486e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004066192), np.complex128(4.8047732053974435e-05+0j)) <f>: (np.float32(-0.0007560492), np.complex128(0.0014103488570928265+0j))\n",
      "Epoch 800: <Test loss>: 2.539757815611665e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004082084), np.complex128(5.4559675844359884e-05+0j)) <f>: (np.float32(-0.00077193446), np.complex128(0.0014038476290634632+0j))\n",
      "Epoch 900: <Test loss>: 2.390052941336762e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040769908), np.complex128(5.4310192471482044e-05+0j)) <f>: (np.float32(-0.0007668409), np.complex128(0.001403551536292754+0j))\n",
      "Epoch 1000: <Test loss>: 2.307479462615447e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004080138), np.complex128(5.0041089574648985e-05+0j)) <f>: (np.float32(-0.000769992), np.complex128(0.0013932383992769856+0j))\n",
      "Epoch 1100: <Test loss>: 2.407303099971614e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040576877), np.complex128(4.9734051107978894e-05+0j)) <f>: (np.float32(-0.00074754376), np.complex128(0.0014042702019080359+0j))\n",
      "Epoch 1200: <Test loss>: 2.302838993273326e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040526465), np.complex128(5.3421489969036544e-05+0j)) <f>: (np.float32(-0.000742497), np.complex128(0.0013824519451543123+0j))\n",
      "Epoch 1300: <Test loss>: 2.321083456990891e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040543643), np.complex128(5.1701053327404584e-05+0j)) <f>: (np.float32(-0.0007442206), np.complex128(0.0013952326487866465+0j))\n",
      "Epoch 1400: <Test loss>: 2.423998466838384e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004064041), np.complex128(5.39707173208742e-05+0j)) <f>: (np.float32(-0.00075389544), np.complex128(0.0013874505087153593+0j))\n",
      "Epoch 1500: <Test loss>: 2.5503250071778893e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004048842), np.complex128(6.036292447413281e-05+0j)) <f>: (np.float32(-0.00073869526), np.complex128(0.0013781455333314785+0j))\n",
      "Epoch 1600: <Test loss>: 2.1077521523693576e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004069532), np.complex128(5.0184965415943144e-05+0j)) <f>: (np.float32(-0.00075938564), np.complex128(0.0014018696177177487+0j))\n",
      "Epoch 1700: <Test loss>: 2.293351599291782e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004057322), np.complex128(4.841029584394349e-05+0j)) <f>: (np.float32(-0.0007471755), np.complex128(0.0013961838499840663+0j))\n",
      "Epoch 1800: <Test loss>: 2.413157972114277e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004054304), np.complex128(5.526033517530462e-05+0j)) <f>: (np.float32(-0.0007441554), np.complex128(0.001388763009060585+0j))\n",
      "Epoch 1900: <Test loss>: 2.289293888679822e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040617366), np.complex128(4.708222335321435e-05+0j)) <f>: (np.float32(-0.0007515882), np.complex128(0.0013898326981105508+0j))\n",
      "Epoch 2000: <Test loss>: 2.309410774614662e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040621203), np.complex128(4.121488630587388e-05+0j)) <f>: (np.float32(-0.00075197674), np.complex128(0.001399491995306167+0j))\n",
      "Epoch 2100: <Test loss>: 2.285438313265331e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004094207), np.complex128(4.294988813655343e-05+0j)) <f>: (np.float32(-0.00078406057), np.complex128(0.0013965617678777863+0j))\n",
      "Epoch 2200: <Test loss>: 2.6955776775139384e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004062168), np.complex128(5.6291565624576074e-05+0j)) <f>: (np.float32(-0.00075202214), np.complex128(0.0013877382286827836+0j))\n",
      "Epoch 2300: <Test loss>: 2.5574397568561835e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041018734), np.complex128(5.890787239570798e-05+0j)) <f>: (np.float32(-0.0007917263), np.complex128(0.0013836165259748395+0j))\n",
      "Epoch 2400: <Test loss>: 2.217672317783581e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040762736), np.complex128(5.258735737057578e-05+0j)) <f>: (np.float32(-0.00076612766), np.complex128(0.0013995749621750805+0j))\n",
      "Epoch 2500: <Test loss>: 2.502389406799921e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040911213), np.complex128(5.84447517140155e-05+0j)) <f>: (np.float32(-0.00078097265), np.complex128(0.0013870432860101423+0j))\n",
      "Epoch 2600: <Test loss>: 2.4894454782042885e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004102725), np.complex128(5.301337923922872e-05+0j)) <f>: (np.float32(-0.00079257676), np.complex128(0.001382352359539485+0j))\n",
      "Epoch 2700: <Test loss>: 2.463568989696796e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004070078), np.complex128(4.713675361324414e-05+0j)) <f>: (np.float32(-0.00075993396), np.complex128(0.001400362005683855+0j))\n",
      "Epoch 2800: <Test loss>: 2.426664877930307e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004047926), np.complex128(5.926011686420429e-05+0j)) <f>: (np.float32(-0.0007377844), np.complex128(0.0013948770583683526+0j))\n",
      "Epoch 2900: <Test loss>: 2.493799456715351e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041033253), np.complex128(5.500345820482914e-05+0j)) <f>: (np.float32(-0.0007931794), np.complex128(0.0013905313197422112+0j))\n",
      "Epoch 3000: <Test loss>: 2.853210162356845e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004072401), np.complex128(6.43731127012061e-05+0j)) <f>: (np.float32(-0.0007622562), np.complex128(0.0013812656711616382+0j))\n",
      "Epoch 3100: <Test loss>: 2.5843378352874424e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00406893), np.complex128(5.793805043264964e-05+0j)) <f>: (np.float32(-0.00075878407), np.complex128(0.001394090141720234+0j))\n",
      "Epoch 3200: <Test loss>: 3.1429754017153755e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004039831), np.complex128(7.323427004505809e-05+0j)) <f>: (np.float32(-0.0007296847), np.complex128(0.0013736004966150269+0j))\n",
      "Epoch 3300: <Test loss>: 2.6017278287326917e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004097942), np.complex128(5.9327265795093756e-05+0j)) <f>: (np.float32(-0.0007877938), np.complex128(0.001393347626301656+0j))\n",
      "Epoch 3400: <Test loss>: 3.1394656616612338e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004094398), np.complex128(7.046306244876124e-05+0j)) <f>: (np.float32(-0.0007842495), np.complex128(0.0013790484006190266+0j))\n",
      "Epoch 3500: <Test loss>: 3.451631073403405e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040663886), np.complex128(7.36356413025249e-05+0j)) <f>: (np.float32(-0.00075624156), np.complex128(0.00138317530461386+0j))\n",
      "Epoch 3600: <Test loss>: 2.7989383397653e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00408185), np.complex128(5.796646721955575e-05+0j)) <f>: (np.float32(-0.00077170494), np.complex128(0.0013872956118545898+0j))\n",
      "Epoch 3700: <Test loss>: 3.024885927516152e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040931827), np.complex128(6.614380598452456e-05+0j)) <f>: (np.float32(-0.00078303553), np.complex128(0.0013888340510278504+0j))\n",
      "Epoch 3800: <Test loss>: 2.8699016638711328e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040762294), np.complex128(6.189096503801081e-05+0j)) <f>: (np.float32(-0.0007660859), np.complex128(0.0013824220060395362+0j))\n",
      "Epoch 3900: <Test loss>: 2.993474026879994e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004093398), np.complex128(5.850357541436607e-05+0j)) <f>: (np.float32(-0.0007832539), np.complex128(0.0013826549222036417+0j))\n",
      "Epoch 4000: <Test loss>: 3.017811195604736e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004078024), np.complex128(6.484291735370266e-05+0j)) <f>: (np.float32(-0.00076787703), np.complex128(0.0013795864166604053+0j))\n",
      "Epoch 4100: <Test loss>: 3.1845004286878975e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040960945), np.complex128(6.124793215997724e-05+0j)) <f>: (np.float32(-0.0007859479), np.complex128(0.0013890761011591756+0j))\n",
      "Epoch 4200: <Test loss>: 3.4452539239282487e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040913196), np.complex128(6.317899316984785e-05+0j)) <f>: (np.float32(-0.0007811757), np.complex128(0.0013807506168989649+0j))\n",
      "Epoch 4300: <Test loss>: 2.9949449071864365e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040743146), np.complex128(6.302285148889305e-05+0j)) <f>: (np.float32(-0.0007641702), np.complex128(0.0013868187426493218+0j))\n",
      "Epoch 4400: <Test loss>: 3.1047072752699023e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041194716), np.complex128(5.8793138825695464e-05+0j)) <f>: (np.float32(-0.00080932374), np.complex128(0.0013893206885036175+0j))\n",
      "Epoch 4500: <Test loss>: 3.2314774216501974e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040954435), np.complex128(5.739729102958869e-05+0j)) <f>: (np.float32(-0.0007852987), np.complex128(0.001383336417646765+0j))\n",
      "Epoch 4600: <Test loss>: 3.610379508245387e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004104061), np.complex128(7.490053718665028e-05+0j)) <f>: (np.float32(-0.00079391454), np.complex128(0.00137370173141838+0j))\n",
      "Epoch 4700: <Test loss>: 3.1539909741695737e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004127954), np.complex128(6.112269690629922e-05+0j)) <f>: (np.float32(-0.00081780757), np.complex128(0.001380135596439497+0j))\n",
      "Epoch 4800: <Test loss>: 3.2497687243449036e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004099178), np.complex128(6.286810923954788e-05+0j)) <f>: (np.float32(-0.00078903587), np.complex128(0.00138455795890176+0j))\n",
      "Epoch 4900: <Test loss>: 3.3041035294445464e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041162916), np.complex128(6.108526111964263e-05+0j)) <f>: (np.float32(-0.00080614357), np.complex128(0.0013883631442734063+0j))\n",
      "Epoch 5000: <Test loss>: 3.4604959182615858e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004109047), np.complex128(6.215889077873006e-05+0j)) <f>: (np.float32(-0.0007989014), np.complex128(0.0013788824668811998+0j))\n",
      "Epoch 5100: <Test loss>: 3.3667913612589473e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004104459), np.complex128(6.429656815299417e-05+0j)) <f>: (np.float32(-0.0007943122), np.complex128(0.0013885157576423708+0j))\n",
      "Epoch 5200: <Test loss>: 3.2596699384157546e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00410538), np.complex128(5.6482213403916865e-05+0j)) <f>: (np.float32(-0.0007952335), np.complex128(0.001386939133411705+0j))\n",
      "Epoch 5300: <Test loss>: 3.420427901801304e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041310247), np.complex128(6.356483192576637e-05+0j)) <f>: (np.float32(-0.00082087854), np.complex128(0.0013838791275324093+0j))\n",
      "Epoch 5400: <Test loss>: 3.6746548630617326e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040882896), np.complex128(6.248664321186003e-05+0j)) <f>: (np.float32(-0.0007781418), np.complex128(0.0013880983860846874+0j))\n",
      "Epoch 5500: <Test loss>: 3.462235099505051e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004133485), np.complex128(6.018292109669532e-05+0j)) <f>: (np.float32(-0.00082333654), np.complex128(0.001384141855950635+0j))\n",
      "Epoch 5600: <Test loss>: 3.5637438031699276e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041043623), np.complex128(6.079772351441104e-05+0j)) <f>: (np.float32(-0.0007942175), np.complex128(0.0013923294426779578+0j))\n",
      "Epoch 5700: <Test loss>: 3.7084726045577554e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040946), np.complex128(7.030506543071491e-05+0j)) <f>: (np.float32(-0.00078445213), np.complex128(0.0013757772986084285+0j))\n",
      "Epoch 5800: <Test loss>: 3.6267285850044573e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00410153), np.complex128(6.765480361217079e-05+0j)) <f>: (np.float32(-0.0007913827), np.complex128(0.001379627138930927+0j))\n",
      "Epoch 5900: <Test loss>: 3.693009375638212e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041131317), np.complex128(6.975590150920516e-05+0j)) <f>: (np.float32(-0.0008029859), np.complex128(0.0013787180554712432+0j))\n",
      "Epoch 6000: <Test loss>: 3.5343684885447146e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004106425), np.complex128(6.542115218737514e-05+0j)) <f>: (np.float32(-0.0007962789), np.complex128(0.0013844106736803407+0j))\n",
      "Epoch 6100: <Test loss>: 3.8309131014102604e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00409488), np.complex128(6.650069276014518e-05+0j)) <f>: (np.float32(-0.0007847354), np.complex128(0.0013877263037811356+0j))\n",
      "Epoch 6200: <Test loss>: 4.008243649877841e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00412818), np.complex128(6.941287029583856e-05+0j)) <f>: (np.float32(-0.0008180352), np.complex128(0.0013788942649221921+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs1_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88e441e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 2.3292124751606025e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004075275), np.complex128(0.00014047548413284735+0j)) <f>: (np.float32(-0.0007651298), np.complex128(0.0013784689011431914+0j))\n",
      "Epoch 400: <Test loss>: 1.4952930541767273e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041482123), np.complex128(0.00010646453874414771+0j)) <f>: (np.float32(-0.0008380651), np.complex128(0.0014177388708769406+0j))\n",
      "Epoch 600: <Test loss>: 1.203709325636737e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004107687), np.complex128(0.00011027122265729085+0j)) <f>: (np.float32(-0.00079754356), np.complex128(0.001404019525252114+0j))\n",
      "Epoch 800: <Test loss>: 1.0726522305049002e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004080908), np.complex128(9.629402482069712e-05+0j)) <f>: (np.float32(-0.00077076146), np.complex128(0.0013916049414725077+0j))\n",
      "Epoch 1000: <Test loss>: 9.508264156465884e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004029041), np.complex128(8.897586174052919e-05+0j)) <f>: (np.float32(-0.00071889587), np.complex128(0.00139692192527969+0j))\n",
      "Epoch 1200: <Test loss>: 8.853740837366786e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004022366), np.complex128(8.226282398867438e-05+0j)) <f>: (np.float32(-0.0007122197), np.complex128(0.0014065455746310178+0j))\n",
      "Epoch 1400: <Test loss>: 8.208512554119807e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004017315), np.complex128(8.163830483760112e-05+0j)) <f>: (np.float32(-0.0007071709), np.complex128(0.0014211991221253827+0j))\n",
      "Epoch 1600: <Test loss>: 9.078175025933888e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004011265), np.complex128(8.600221625269026e-05+0j)) <f>: (np.float32(-0.0007011216), np.complex128(0.0013925086967846467+0j))\n",
      "Epoch 1800: <Test loss>: 7.6273458944342565e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040118475), np.complex128(7.513607778057322e-05+0j)) <f>: (np.float32(-0.00070170313), np.complex128(0.0014177110883933136+0j))\n",
      "Epoch 2000: <Test loss>: 7.554245712526608e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040356573), np.complex128(8.142937326623871e-05+0j)) <f>: (np.float32(-0.0007255113), np.complex128(0.0014241960782587306+0j))\n",
      "Epoch 2200: <Test loss>: 7.272021321114153e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040631043), np.complex128(7.705335458730872e-05+0j)) <f>: (np.float32(-0.0007529593), np.complex128(0.0014158009474984686+0j))\n",
      "Epoch 2400: <Test loss>: 7.2816669671738055e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040370617), np.complex128(7.572253873489723e-05+0j)) <f>: (np.float32(-0.0007269197), np.complex128(0.0014080824914764086+0j))\n",
      "Epoch 2600: <Test loss>: 6.843134087830549e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040383493), np.complex128(7.496113693618249e-05+0j)) <f>: (np.float32(-0.00072820164), np.complex128(0.0014085776286161162+0j))\n",
      "Epoch 2800: <Test loss>: 6.982227660046192e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040342435), np.complex128(7.17911983698183e-05+0j)) <f>: (np.float32(-0.0007240967), np.complex128(0.0014142491879563458+0j))\n",
      "Epoch 3000: <Test loss>: 6.704627594444901e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004058756), np.complex128(7.430583821848826e-05+0j)) <f>: (np.float32(-0.0007486111), np.complex128(0.0014176975143031397+0j))\n",
      "Epoch 3200: <Test loss>: 6.8554527388187125e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040833633), np.complex128(7.581591610637971e-05+0j)) <f>: (np.float32(-0.0007732173), np.complex128(0.0014190304392139544+0j))\n",
      "Epoch 3400: <Test loss>: 6.522116564156022e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004075987), np.complex128(7.242454226526279e-05+0j)) <f>: (np.float32(-0.00076584046), np.complex128(0.0014112872453640073+0j))\n",
      "Epoch 3600: <Test loss>: 6.653011041635182e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040633203), np.complex128(7.536560835092616e-05+0j)) <f>: (np.float32(-0.00075317465), np.complex128(0.0014123925822582613+0j))\n",
      "Epoch 3800: <Test loss>: 7.301998266484588e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040650675), np.complex128(8.528352685103557e-05+0j)) <f>: (np.float32(-0.00075492007), np.complex128(0.0013914628575379773+0j))\n",
      "Epoch 4000: <Test loss>: 7.41023495720583e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040602824), np.complex128(8.067429864273353e-05+0j)) <f>: (np.float32(-0.00075013685), np.complex128(0.0014017001318815587+0j))\n",
      "Epoch 4200: <Test loss>: 6.84066708345199e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004084063), np.complex128(8.167888438988501e-05+0j)) <f>: (np.float32(-0.0007739193), np.complex128(0.001405181188277557+0j))\n",
      "Epoch 4400: <Test loss>: 6.907765055075288e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004096841), np.complex128(7.755870400981088e-05+0j)) <f>: (np.float32(-0.0007866939), np.complex128(0.001409681823764468+0j))\n",
      "Epoch 4600: <Test loss>: 6.713863967888756e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041180146), np.complex128(7.674703367622316e-05+0j)) <f>: (np.float32(-0.0008078698), np.complex128(0.0014094152895265672+0j))\n",
      "Epoch 4800: <Test loss>: 6.643366759817582e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040947683), np.complex128(8.152351973044717e-05+0j)) <f>: (np.float32(-0.0007846218), np.complex128(0.0014190521323861015+0j))\n",
      "Epoch 5000: <Test loss>: 6.866182502562879e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004113511), np.complex128(8.156262452760703e-05+0j)) <f>: (np.float32(-0.000803365), np.complex128(0.0014072740085167986+0j))\n",
      "Epoch 5200: <Test loss>: 6.8801468842139e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004121419), np.complex128(8.039436474806079e-05+0j)) <f>: (np.float32(-0.00081127434), np.complex128(0.001409294771903528+0j))\n",
      "Epoch 5400: <Test loss>: 7.092198302416364e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004107015), np.complex128(8.250676117225511e-05+0j)) <f>: (np.float32(-0.0007968691), np.complex128(0.001413703052832994+0j))\n",
      "Epoch 5600: <Test loss>: 6.991811005718773e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004120991), np.complex128(8.591610165375408e-05+0j)) <f>: (np.float32(-0.000810846), np.complex128(0.0014022093505540638+0j))\n",
      "Epoch 5800: <Test loss>: 7.313241894735256e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041489066), np.complex128(8.011321774836667e-05+0j)) <f>: (np.float32(-0.000838761), np.complex128(0.0013966687114106517+0j))\n",
      "Epoch 6000: <Test loss>: 7.240877948788693e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041192938), np.complex128(8.671169239921524e-05+0j)) <f>: (np.float32(-0.00080914877), np.complex128(0.001403949878752063+0j))\n",
      "Epoch 6200: <Test loss>: 7.360913969023386e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041365004), np.complex128(8.818498862570674e-05+0j)) <f>: (np.float32(-0.0008263528), np.complex128(0.0014022018657753696+0j))\n",
      "Epoch 6400: <Test loss>: 7.253851890709484e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004145955), np.complex128(8.5658178082868e-05+0j)) <f>: (np.float32(-0.0008358107), np.complex128(0.00142851327323731+0j))\n",
      "Epoch 6600: <Test loss>: 7.58193255023798e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00413644), np.complex128(9.151284492220737e-05+0j)) <f>: (np.float32(-0.00082629273), np.complex128(0.0014008249202169818+0j))\n",
      "Epoch 6800: <Test loss>: 7.76588603912387e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004140915), np.complex128(9.130640299121563e-05+0j)) <f>: (np.float32(-0.00083076814), np.complex128(0.001399764365134236+0j))\n",
      "Epoch 7000: <Test loss>: 7.602623099955963e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041493713), np.complex128(8.958431716105788e-05+0j)) <f>: (np.float32(-0.00083922537), np.complex128(0.0014121298538400358+0j))\n",
      "Epoch 7200: <Test loss>: 7.653362445125822e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004147855), np.complex128(8.844808176831792e-05+0j)) <f>: (np.float32(-0.00083770807), np.complex128(0.0014142283828087895+0j))\n",
      "Epoch 7400: <Test loss>: 7.539017587987473e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004152913), np.complex128(9.365182658258923e-05+0j)) <f>: (np.float32(-0.0008427685), np.complex128(0.0014084087770832054+0j))\n",
      "Epoch 7600: <Test loss>: 7.519323389715282e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004162969), np.complex128(8.947080066046223e-05+0j)) <f>: (np.float32(-0.00085282495), np.complex128(0.0014156775120803453+0j))\n",
      "Epoch 7800: <Test loss>: 7.93650633568177e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00416454), np.complex128(9.357187265425184e-05+0j)) <f>: (np.float32(-0.00085439713), np.complex128(0.001422553993929656+0j))\n",
      "Epoch 8000: <Test loss>: 8.148675988195464e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041714846), np.complex128(9.15482866179301e-05+0j)) <f>: (np.float32(-0.0008613347), np.complex128(0.0014038641209487212+0j))\n",
      "Epoch 8200: <Test loss>: 7.975154403538909e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041533723), np.complex128(9.469983831798981e-05+0j)) <f>: (np.float32(-0.00084322545), np.complex128(0.0014080803348452593+0j))\n",
      "Epoch 8400: <Test loss>: 8.088603863143362e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041637677), np.complex128(8.892859821744121e-05+0j)) <f>: (np.float32(-0.00085362216), np.complex128(0.0014070766133363257+0j))\n",
      "Epoch 8600: <Test loss>: 8.442414582532365e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041557127), np.complex128(9.275577805529203e-05+0j)) <f>: (np.float32(-0.0008455659), np.complex128(0.0014133246274966507+0j))\n",
      "Epoch 8800: <Test loss>: 8.484706995659508e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041485983), np.complex128(9.169845791926986e-05+0j)) <f>: (np.float32(-0.0008384528), np.complex128(0.0013990492516173173+0j))\n",
      "Epoch 9000: <Test loss>: 8.299715773318894e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00415027), np.complex128(9.217791191040055e-05+0j)) <f>: (np.float32(-0.0008401244), np.complex128(0.0014089634118704978+0j))\n",
      "Epoch 9200: <Test loss>: 8.302322385134175e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004157548), np.complex128(8.759074159863112e-05+0j)) <f>: (np.float32(-0.000847401), np.complex128(0.0014093054281986179+0j))\n",
      "Epoch 9400: <Test loss>: 8.824114956951234e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00418848), np.complex128(9.580354981009112e-05+0j)) <f>: (np.float32(-0.0008783329), np.complex128(0.0014010097561925274+0j))\n",
      "Epoch 9600: <Test loss>: 8.933105164032895e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004144031), np.complex128(9.480606825966616e-05+0j)) <f>: (np.float32(-0.00083388336), np.complex128(0.0014026833019642477+0j))\n",
      "Epoch 9800: <Test loss>: 8.482469638693146e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041430187), np.complex128(9.477604985698019e-05+0j)) <f>: (np.float32(-0.0008328709), np.complex128(0.0014044107635146964+0j))\n",
      "Epoch 10000: <Test loss>: 8.321964742208365e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041561592), np.complex128(9.16307777593841e-05+0j)) <f>: (np.float32(-0.0008460145), np.complex128(0.001408505825484916+0j))\n",
      "Epoch 10200: <Test loss>: 8.653400982439052e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004158363), np.complex128(9.190298901163314e-05+0j)) <f>: (np.float32(-0.0008482209), np.complex128(0.0014033434848171914+0j))\n",
      "Epoch 10400: <Test loss>: 8.62323122419184e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004167125), np.complex128(8.901645715039506e-05+0j)) <f>: (np.float32(-0.0008569769), np.complex128(0.0014106464721914058+0j))\n",
      "Epoch 10600: <Test loss>: 8.685336069902405e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004188651), np.complex128(9.337352601886032e-05+0j)) <f>: (np.float32(-0.00087850523), np.complex128(0.001423859770660123+0j))\n",
      "Epoch 10800: <Test loss>: 8.966572750068735e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004173975), np.complex128(9.143515862809296e-05+0j)) <f>: (np.float32(-0.00086383335), np.complex128(0.0014028096551774553+0j))\n",
      "Epoch 11000: <Test loss>: 8.894480743038002e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004145762), np.complex128(9.099621283012724e-05+0j)) <f>: (np.float32(-0.0008356171), np.complex128(0.001409582111288985+0j))\n",
      "Epoch 11200: <Test loss>: 9.379391485708766e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041672555), np.complex128(9.068810794106907e-05+0j)) <f>: (np.float32(-0.000857112), np.complex128(0.0013876259570023734+0j))\n",
      "Epoch 11400: <Test loss>: 8.87541682459414e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00415683), np.complex128(9.368656261591391e-05+0j)) <f>: (np.float32(-0.0008466839), np.complex128(0.0014028171399561493+0j))\n",
      "Epoch 11600: <Test loss>: 9.217671504302416e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041425438), np.complex128(9.047858171038246e-05+0j)) <f>: (np.float32(-0.0008323973), np.complex128(0.0013978822604443295+0j))\n",
      "Epoch 11800: <Test loss>: 9.032047273649368e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041704993), np.complex128(9.063575413416588e-05+0j)) <f>: (np.float32(-0.0008603521), np.complex128(0.0014021828366769951+0j))\n",
      "Epoch 12000: <Test loss>: 8.969928785518277e-06 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041760523), np.complex128(8.854035703785289e-05+0j)) <f>: (np.float32(-0.0008659042), np.complex128(0.0014049718681954362+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ee7ec97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 4.509095015237108e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004069002), np.complex128(0.0003037577232496521+0j)) <f>: (np.float32(-0.0007588562), np.complex128(0.0013845972857050677+0j))\n",
      "Epoch 800: <Test loss>: 3.3810218155849725e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004089366), np.complex128(0.00024528713953459304+0j)) <f>: (np.float32(-0.0007792186), np.complex128(0.0013894107595692575+0j))\n",
      "Epoch 1200: <Test loss>: 3.0252027499955148e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004117064), np.complex128(0.0002367529219227946+0j)) <f>: (np.float32(-0.0008069177), np.complex128(0.001391812485505447+0j))\n",
      "Epoch 1600: <Test loss>: 2.770103674265556e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004133666), np.complex128(0.0002350488661633468+0j)) <f>: (np.float32(-0.0008235188), np.complex128(0.0014093628960757092+0j))\n",
      "Epoch 2000: <Test loss>: 2.468193997628987e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041270787), np.complex128(0.0002117128802384271+0j)) <f>: (np.float32(-0.0008169332), np.complex128(0.0013969229401649367+0j))\n",
      "Epoch 2400: <Test loss>: 2.322937871213071e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004135556), np.complex128(0.00019999195466732184+0j)) <f>: (np.float32(-0.00082541065), np.complex128(0.0013817434283914971+0j))\n",
      "Epoch 2800: <Test loss>: 2.3737102310406044e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00406856), np.complex128(0.00019279148656060096+0j)) <f>: (np.float32(-0.00075841695), np.complex128(0.0013892653772576754+0j))\n",
      "Epoch 3200: <Test loss>: 2.1275574908941053e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041254926), np.complex128(0.00018824540324373875+0j)) <f>: (np.float32(-0.00081534527), np.complex128(0.0013814035686945264+0j))\n",
      "Epoch 3600: <Test loss>: 2.1257066691759974e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004118002), np.complex128(0.00017853301486365727+0j)) <f>: (np.float32(-0.00080785534), np.complex128(0.0014026250729232215+0j))\n",
      "Epoch 4000: <Test loss>: 2.0885199774056673e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004089942), np.complex128(0.00018326708981487885+0j)) <f>: (np.float32(-0.000779795), np.complex128(0.001377228711371789+0j))\n",
      "Epoch 4400: <Test loss>: 1.9349963622516952e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041389274), np.complex128(0.00017192552517745374+0j)) <f>: (np.float32(-0.0008287834), np.complex128(0.001389344284585602+0j))\n",
      "Epoch 4800: <Test loss>: 1.8854436348192394e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041125584), np.complex128(0.00015910944327972475+0j)) <f>: (np.float32(-0.00080241094), np.complex128(0.0013762169976415382+0j))\n",
      "Epoch 5200: <Test loss>: 1.7801539797801524e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041302275), np.complex128(0.00016235420584665618+0j)) <f>: (np.float32(-0.0008200822), np.complex128(0.0013889336366426776+0j))\n",
      "Epoch 5600: <Test loss>: 1.7897911675390787e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041204877), np.complex128(0.00016872578228576028+0j)) <f>: (np.float32(-0.00081034034), np.complex128(0.001397333080665238+0j))\n",
      "Epoch 6000: <Test loss>: 1.7401940567651764e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004112322), np.complex128(0.00015513055395508223+0j)) <f>: (np.float32(-0.00080217695), np.complex128(0.0013957578519017862+0j))\n",
      "Epoch 6400: <Test loss>: 1.705008980934508e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041014287), np.complex128(0.00015475249334312433+0j)) <f>: (np.float32(-0.0007912809), np.complex128(0.0013779136320526198+0j))\n",
      "Epoch 6800: <Test loss>: 1.6949961718637496e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00408843), np.complex128(0.00016228855545726372+0j)) <f>: (np.float32(-0.00077828584), np.complex128(0.0013878209418303853+0j))\n",
      "Epoch 7200: <Test loss>: 1.6233212591032498e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004096773), np.complex128(0.0001517486233040337+0j)) <f>: (np.float32(-0.0007866259), np.complex128(0.0013884343131013274+0j))\n",
      "Epoch 7600: <Test loss>: 1.6120389773277566e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004096546), np.complex128(0.00015578134911949445+0j)) <f>: (np.float32(-0.00078639877), np.complex128(0.0013879866218469005+0j))\n",
      "Epoch 8000: <Test loss>: 1.5832931239856407e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00409944), np.complex128(0.00015596453590651418+0j)) <f>: (np.float32(-0.00078929344), np.complex128(0.0013893748580036573+0j))\n",
      "Epoch 8400: <Test loss>: 1.562050783832092e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004110232), np.complex128(0.00015817370322473447+0j)) <f>: (np.float32(-0.0008000885), np.complex128(0.0013991551802649362+0j))\n",
      "Epoch 8800: <Test loss>: 1.5422672731801867e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041289357), np.complex128(0.0001589924143247208+0j)) <f>: (np.float32(-0.00081878953), np.complex128(0.00140265767611177+0j))\n",
      "Epoch 9200: <Test loss>: 1.579836680321023e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00411495), np.complex128(0.00016195710027874167+0j)) <f>: (np.float32(-0.00080480333), np.complex128(0.0013892141255527196+0j))\n",
      "Epoch 9600: <Test loss>: 1.560431883262936e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004093094), np.complex128(0.00016174969896404022+0j)) <f>: (np.float32(-0.00078294653), np.complex128(0.0013846948415494017+0j))\n",
      "Epoch 10000: <Test loss>: 1.501151746197138e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040983954), np.complex128(0.00015719308621274403+0j)) <f>: (np.float32(-0.00078825175), np.complex128(0.0013942261363432847+0j))\n",
      "Epoch 10400: <Test loss>: 1.5170563528954517e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040845927), np.complex128(0.0001607121532327457+0j)) <f>: (np.float32(-0.0007744454), np.complex128(0.0013930261613997806+0j))\n",
      "Epoch 10800: <Test loss>: 1.5149346836551558e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041090706), np.complex128(0.0001556032684738719+0j)) <f>: (np.float32(-0.000798928), np.complex128(0.001392362933891097+0j))\n",
      "Epoch 11200: <Test loss>: 1.6081201465567574e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004130752), np.complex128(0.0001605092078985803+0j)) <f>: (np.float32(-0.0008206054), np.complex128(0.0013795680218653098+0j))\n",
      "Epoch 11600: <Test loss>: 1.4629662473453209e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041103465), np.complex128(0.00015863262164720254+0j)) <f>: (np.float32(-0.00080019794), np.complex128(0.0013963221280989218+0j))\n",
      "Epoch 12000: <Test loss>: 1.4658259715361055e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040928717), np.complex128(0.0001575191973861392+0j)) <f>: (np.float32(-0.0007827256), np.complex128(0.0013959771939757177+0j))\n",
      "Epoch 12400: <Test loss>: 1.4742840903636534e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041073663), np.complex128(0.00016248509432830965+0j)) <f>: (np.float32(-0.00079721934), np.complex128(0.001396749521648416+0j))\n",
      "Epoch 12800: <Test loss>: 1.4680973436043132e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041134134), np.complex128(0.0001565078008075278+0j)) <f>: (np.float32(-0.0008032687), np.complex128(0.001395387545647416+0j))\n",
      "Epoch 13200: <Test loss>: 1.4499386452371255e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041134614), np.complex128(0.00015600590833789703+0j)) <f>: (np.float32(-0.00080331136), np.complex128(0.0013956784371312362+0j))\n",
      "Epoch 13600: <Test loss>: 1.4804709280724637e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004118219), np.complex128(0.00015913770149081108+0j)) <f>: (np.float32(-0.00080807606), np.complex128(0.0013915370710216382+0j))\n",
      "Epoch 14000: <Test loss>: 1.4820693650108296e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041149086), np.complex128(0.00015933887077579484+0j)) <f>: (np.float32(-0.00080476364), np.complex128(0.0014191540014927336+0j))\n",
      "Epoch 14400: <Test loss>: 1.4906394426361658e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004108986), np.complex128(0.0001615455643112264+0j)) <f>: (np.float32(-0.00079883746), np.complex128(0.001400849531184213+0j))\n",
      "Epoch 14800: <Test loss>: 1.4532442946801893e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041067083), np.complex128(0.00015608592569656233+0j)) <f>: (np.float32(-0.00079656084), np.complex128(0.0013974576578292637+0j))\n",
      "Epoch 15200: <Test loss>: 1.4923377420927864e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004108831), np.complex128(0.00016007188750276742+0j)) <f>: (np.float32(-0.0007986833), np.complex128(0.0013964634508695174+0j))\n",
      "Epoch 15600: <Test loss>: 1.4654439837613609e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004139457), np.complex128(0.00015741912018627064+0j)) <f>: (np.float32(-0.0008293088), np.complex128(0.0014008185771841903+0j))\n",
      "Epoch 16000: <Test loss>: 1.754290315147955e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00410944), np.complex128(0.00015003034329355087+0j)) <f>: (np.float32(-0.0007992926), np.complex128(0.0013751419804440275+0j))\n",
      "Epoch 16400: <Test loss>: 1.4756389646208845e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004128797), np.complex128(0.00015927621746939638+0j)) <f>: (np.float32(-0.00081865536), np.complex128(0.0014102467342648826+0j))\n",
      "Epoch 16800: <Test loss>: 1.5036276636237744e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00413293), np.complex128(0.00015975492615417407+0j)) <f>: (np.float32(-0.00082278287), np.complex128(0.0014045339452115081+0j))\n",
      "Epoch 17200: <Test loss>: 1.5173927749856375e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041483287), np.complex128(0.00016026458883897448+0j)) <f>: (np.float32(-0.0008381821), np.complex128(0.0014025537772346444+0j))\n",
      "Epoch 17600: <Test loss>: 1.5138329217734281e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041303164), np.complex128(0.00015495223544573+0j)) <f>: (np.float32(-0.0008201745), np.complex128(0.0014033039042925723+0j))\n",
      "Epoch 18000: <Test loss>: 1.58660659508314e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041487454), np.complex128(0.00015990367027313572+0j)) <f>: (np.float32(-0.00083859736), np.complex128(0.0013943177297367944+0j))\n",
      "Epoch 18400: <Test loss>: 1.5119588169909548e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041361847), np.complex128(0.00015717816422810193+0j)) <f>: (np.float32(-0.00082603813), np.complex128(0.0014113407805607678+0j))\n",
      "Epoch 18800: <Test loss>: 1.5258591702149715e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004137664), np.complex128(0.0001580905143496734+0j)) <f>: (np.float32(-0.0008275152), np.complex128(0.0014048033972444928+0j))\n",
      "Epoch 19200: <Test loss>: 1.5248920135491062e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041528875), np.complex128(0.00015846868596470514+0j)) <f>: (np.float32(-0.0008427404), np.complex128(0.0014183082215003095+0j))\n",
      "Epoch 19600: <Test loss>: 1.5487841665162705e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041330256), np.complex128(0.00016099235670631208+0j)) <f>: (np.float32(-0.0008228815), np.complex128(0.0014099925055105977+0j))\n",
      "Epoch 20000: <Test loss>: 1.5700912626925856e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041409484), np.complex128(0.00016009212177737245+0j)) <f>: (np.float32(-0.00083080365), np.complex128(0.001407807584435223+0j))\n",
      "Epoch 20400: <Test loss>: 1.6916314052650705e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041190744), np.complex128(0.00016245179340615404+0j)) <f>: (np.float32(-0.00080893154), np.complex128(0.0013931504848424948+0j))\n",
      "Epoch 20800: <Test loss>: 1.5789180906722322e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041281222), np.complex128(0.00016067769470710564+0j)) <f>: (np.float32(-0.00081797387), np.complex128(0.0014019019671849856+0j))\n",
      "Epoch 21200: <Test loss>: 1.5701905795140192e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041462365), np.complex128(0.00016300728450287336+0j)) <f>: (np.float32(-0.0008360909), np.complex128(0.001408864333698294+0j))\n",
      "Epoch 21600: <Test loss>: 1.5711146261310205e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041551776), np.complex128(0.00016255894308758517+0j)) <f>: (np.float32(-0.0008450279), np.complex128(0.0014082573054601433+0j))\n",
      "Epoch 22000: <Test loss>: 1.6251011402346194e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041512633), np.complex128(0.00016348832425220193+0j)) <f>: (np.float32(-0.00084111537), np.complex128(0.001399522061281599+0j))\n",
      "Epoch 22400: <Test loss>: 1.5971631000866182e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004144198), np.complex128(0.00015841332714601697+0j)) <f>: (np.float32(-0.0008340525), np.complex128(0.0014033151948909413+0j))\n",
      "Epoch 22800: <Test loss>: 1.6320806025760248e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004152174), np.complex128(0.00016609833952768374+0j)) <f>: (np.float32(-0.0008420282), np.complex128(0.0014278841712450448+0j))\n",
      "Epoch 23200: <Test loss>: 1.6143923858180642e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004158088), np.complex128(0.00016504230385580186+0j)) <f>: (np.float32(-0.0008479425), np.complex128(0.0014071973846806766+0j))\n",
      "Epoch 23600: <Test loss>: 1.6171195966308005e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004150228), np.complex128(0.0001678823650730509+0j)) <f>: (np.float32(-0.0008400815), np.complex128(0.0014063418364177534+0j))\n",
      "Epoch 24000: <Test loss>: 1.643345967750065e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004151198), np.complex128(0.00016533072155683326+0j)) <f>: (np.float32(-0.0008410517), np.complex128(0.0014033395521368607+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe55879e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 6.935628334758803e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004074216), np.complex128(0.00030544027612793657+0j)) <f>: (np.float32(-0.0007640732), np.complex128(0.0012804257850302266+0j))\n",
      "Epoch 1600: <Test loss>: 5.510691335075535e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004141088), np.complex128(0.00027262253449523587+0j)) <f>: (np.float32(-0.00083094166), np.complex128(0.001358651617513888+0j))\n",
      "Epoch 2400: <Test loss>: 4.875925515079871e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004144915), np.complex128(0.00024233333188180964+0j)) <f>: (np.float32(-0.00083477166), np.complex128(0.0013761550896414926+0j))\n",
      "Epoch 3200: <Test loss>: 4.439277472556569e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041338075), np.complex128(0.00024653595583059185+0j)) <f>: (np.float32(-0.00082365854), np.complex128(0.0013688206408246325+0j))\n",
      "Epoch 4000: <Test loss>: 4.0113220165949315e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040948056), np.complex128(0.00023119853425580684+0j)) <f>: (np.float32(-0.0007846612), np.complex128(0.001371556898310248+0j))\n",
      "Epoch 4800: <Test loss>: 3.6777455534320325e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040813247), np.complex128(0.00023154321465769924+0j)) <f>: (np.float32(-0.00077117985), np.complex128(0.0013660388403635736+0j))\n",
      "Epoch 5600: <Test loss>: 3.440296131884679e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041064383), np.complex128(0.00021804593861090457+0j)) <f>: (np.float32(-0.00079628965), np.complex128(0.0013805348269233966+0j))\n",
      "Epoch 6400: <Test loss>: 3.236977136111818e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041364273), np.complex128(0.00022174322899476658+0j)) <f>: (np.float32(-0.0008262842), np.complex128(0.0013905725494553563+0j))\n",
      "Epoch 7200: <Test loss>: 3.157065293635242e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041135396), np.complex128(0.00021941054697051298+0j)) <f>: (np.float32(-0.00080339285), np.complex128(0.0013727931554013194+0j))\n",
      "Epoch 8000: <Test loss>: 3.0300043363240547e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004180811), np.complex128(0.00021717388260514137+0j)) <f>: (np.float32(-0.000870664), np.complex128(0.0013796096321604223+0j))\n",
      "Epoch 8800: <Test loss>: 2.9186867323005572e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041507906), np.complex128(0.0002167655181540219+0j)) <f>: (np.float32(-0.0008406451), np.complex128(0.0013850435814922806+0j))\n",
      "Epoch 9600: <Test loss>: 2.8922744604642503e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041764574), np.complex128(0.0002203781290501168+0j)) <f>: (np.float32(-0.0008663117), np.complex128(0.0013832525627532608+0j))\n",
      "Epoch 10400: <Test loss>: 2.8547456167871132e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004186606), np.complex128(0.0002148078838011502+0j)) <f>: (np.float32(-0.0008764629), np.complex128(0.0013823887685477086+0j))\n",
      "Epoch 11200: <Test loss>: 2.7749629225581884e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004176633), np.complex128(0.00021191122687381863+0j)) <f>: (np.float32(-0.0008664891), np.complex128(0.0013933075383344134+0j))\n",
      "Epoch 12000: <Test loss>: 2.7600113753578626e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041858777), np.complex128(0.00021156334324036648+0j)) <f>: (np.float32(-0.0008757305), np.complex128(0.0013830768607449352+0j))\n",
      "Epoch 12800: <Test loss>: 2.7146566935698502e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004237011), np.complex128(0.00020571532072787626+0j)) <f>: (np.float32(-0.0009268678), np.complex128(0.0013939219244906024+0j))\n",
      "Epoch 13600: <Test loss>: 2.738017610681709e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041951654), np.complex128(0.00020487283911250364+0j)) <f>: (np.float32(-0.00088501885), np.complex128(0.0013878989611337213+0j))\n",
      "Epoch 14400: <Test loss>: 2.7063109882874414e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004231593), np.complex128(0.00020529994722552213+0j)) <f>: (np.float32(-0.0009214473), np.complex128(0.0013888112161098008+0j))\n",
      "Epoch 15200: <Test loss>: 2.7425146981840953e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004179182), np.complex128(0.00020850359108238239+0j)) <f>: (np.float32(-0.00086903555), np.complex128(0.0013861867228619723+0j))\n",
      "Epoch 16000: <Test loss>: 2.7308888093102723e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004209594), np.complex128(0.00020972102522364504+0j)) <f>: (np.float32(-0.00089944905), np.complex128(0.001387025525518326+0j))\n",
      "Epoch 16800: <Test loss>: 2.7644637157209218e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00419641), np.complex128(0.0002119042812529119+0j)) <f>: (np.float32(-0.0008862645), np.complex128(0.0013838753217127345+0j))\n",
      "Epoch 17600: <Test loss>: 2.8747175747412257e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004240038), np.complex128(0.00021419113486524657+0j)) <f>: (np.float32(-0.00092988875), np.complex128(0.0013795620594144857+0j))\n",
      "Epoch 18400: <Test loss>: 2.797518391162157e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041894), np.complex128(0.00021461610220469793+0j)) <f>: (np.float32(-0.0008792539), np.complex128(0.0013734870831887142+0j))\n",
      "Epoch 19200: <Test loss>: 2.9664046451216564e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041637393), np.complex128(0.0002241148889555242+0j)) <f>: (np.float32(-0.0008535955), np.complex128(0.0013582978031447756+0j))\n",
      "Epoch 20000: <Test loss>: 2.825596311595291e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041788435), np.complex128(0.00021447777651709636+0j)) <f>: (np.float32(-0.00086869474), np.complex128(0.0013718944745154137+0j))\n",
      "Epoch 20800: <Test loss>: 2.8664906494668685e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041892235), np.complex128(0.00021704302583865184+0j)) <f>: (np.float32(-0.00087907916), np.complex128(0.0013823451284821029+0j))\n",
      "Epoch 21600: <Test loss>: 2.9241629817988724e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041711647), np.complex128(0.00022326171932989783+0j)) <f>: (np.float32(-0.00086101884), np.complex128(0.0013616731846144668+0j))\n",
      "Epoch 22400: <Test loss>: 2.9261180316098034e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041884496), np.complex128(0.00022035875108493866+0j)) <f>: (np.float32(-0.00087830116), np.complex128(0.0013767688414944022+0j))\n",
      "Epoch 23200: <Test loss>: 2.985734499816317e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004176807), np.complex128(0.0002266403833190587+0j)) <f>: (np.float32(-0.0008666599), np.complex128(0.001369339881488948+0j))\n",
      "Epoch 24000: <Test loss>: 2.9406188332359307e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041703125), np.complex128(0.00022653813363045903+0j)) <f>: (np.float32(-0.00086016615), np.complex128(0.0013696966136531445+0j))\n",
      "Epoch 24800: <Test loss>: 2.9919070584583096e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004168146), np.complex128(0.00022821242470853252+0j)) <f>: (np.float32(-0.0008579979), np.complex128(0.0013668487187903978+0j))\n",
      "Epoch 25600: <Test loss>: 3.050507075386122e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041828537), np.complex128(0.00023221755833135007+0j)) <f>: (np.float32(-0.000872714), np.complex128(0.0013664411155032133+0j))\n",
      "Epoch 26400: <Test loss>: 3.026291960850358e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004174037), np.complex128(0.00023435587397328882+0j)) <f>: (np.float32(-0.0008638905), np.complex128(0.0013623346360739689+0j))\n",
      "Epoch 27200: <Test loss>: 3.0270432034740224e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041724597), np.complex128(0.00023950996869838434+0j)) <f>: (np.float32(-0.00086231314), np.complex128(0.0013749291082635433+0j))\n",
      "Epoch 28000: <Test loss>: 3.088266385020688e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004182391), np.complex128(0.00023835282508380528+0j)) <f>: (np.float32(-0.0008722434), np.complex128(0.0013659227628634884+0j))\n",
      "Epoch 28800: <Test loss>: 3.131133780698292e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041704956), np.complex128(0.00023926584122381986+0j)) <f>: (np.float32(-0.0008603478), np.complex128(0.0013647055348707914+0j))\n",
      "Epoch 29600: <Test loss>: 3.148696123389527e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041814893), np.complex128(0.00024462116866422705+0j)) <f>: (np.float32(-0.00087134267), np.complex128(0.001369452280030014+0j))\n",
      "Epoch 30400: <Test loss>: 3.181739884894341e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004173939), np.complex128(0.00024369305610616858+0j)) <f>: (np.float32(-0.0008637944), np.complex128(0.0013632709945746563+0j))\n",
      "Epoch 31200: <Test loss>: 3.474531331448816e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004177162), np.complex128(0.0002619346828173071+0j)) <f>: (np.float32(-0.0008670137), np.complex128(0.0013518828403613773+0j))\n",
      "Epoch 32000: <Test loss>: 3.134655344183557e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041356566), np.complex128(0.0002444941970053223+0j)) <f>: (np.float32(-0.00082550995), np.complex128(0.0013785787624711408+0j))\n",
      "Epoch 32800: <Test loss>: 3.285426282673143e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041724434), np.complex128(0.0002510489602314462+0j)) <f>: (np.float32(-0.0008622987), np.complex128(0.0013644932969935865+0j))\n",
      "Epoch 33600: <Test loss>: 3.3104421163443476e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041651553), np.complex128(0.0002563110609473997+0j)) <f>: (np.float32(-0.00085500634), np.complex128(0.0013593822080308177+0j))\n",
      "Epoch 34400: <Test loss>: 3.367950193933211e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041662287), np.complex128(0.0002540237474651876+0j)) <f>: (np.float32(-0.00085608097), np.complex128(0.0013577127218000837+0j))\n",
      "Epoch 35200: <Test loss>: 3.3990934753092006e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041690515), np.complex128(0.00025557771121120554+0j)) <f>: (np.float32(-0.00085890526), np.complex128(0.0013645697939690526+0j))\n",
      "Epoch 36000: <Test loss>: 3.440317595959641e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041834065), np.complex128(0.00025949653201531213+0j)) <f>: (np.float32(-0.0008732595), np.complex128(0.0013618864373769184+0j))\n",
      "Epoch 36800: <Test loss>: 3.442497836658731e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004173315), np.complex128(0.00025729860772271486+0j)) <f>: (np.float32(-0.0008631716), np.complex128(0.0013600431520476963+0j))\n",
      "Epoch 37600: <Test loss>: 3.469946386758238e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041594063), np.complex128(0.00025930657404078745+0j)) <f>: (np.float32(-0.0008492591), np.complex128(0.0013570236147176105+0j))\n",
      "Epoch 38400: <Test loss>: 3.5131732147419825e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041706148), np.complex128(0.000254990885532496+0j)) <f>: (np.float32(-0.0008604702), np.complex128(0.0013601295441543172+0j))\n",
      "Epoch 39200: <Test loss>: 3.545311119523831e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004166839), np.complex128(0.000259788390811633+0j)) <f>: (np.float32(-0.00085669226), np.complex128(0.001361287528220741+0j))\n",
      "Epoch 40000: <Test loss>: 3.557673335308209e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041532842), np.complex128(0.00026308296204356+0j)) <f>: (np.float32(-0.0008431367), np.complex128(0.0013648268136577657+0j))\n",
      "Epoch 40800: <Test loss>: 3.594073496060446e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004169083), np.complex128(0.0002576195493243849+0j)) <f>: (np.float32(-0.0008589351), np.complex128(0.0013645704282723317+0j))\n",
      "Epoch 41600: <Test loss>: 3.624898454290815e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041725794), np.complex128(0.0002624136452233965+0j)) <f>: (np.float32(-0.00086243264), np.complex128(0.0013605300432447752+0j))\n",
      "Epoch 42400: <Test loss>: 3.680130248540081e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004175511), np.complex128(0.0002643750061080332+0j)) <f>: (np.float32(-0.000865366), np.complex128(0.0013536798215512211+0j))\n",
      "Epoch 43200: <Test loss>: 3.692721656989306e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041745924), np.complex128(0.000264264891058772+0j)) <f>: (np.float32(-0.0008644481), np.complex128(0.0013628134081890744+0j))\n",
      "Epoch 44000: <Test loss>: 3.7462847103597596e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004188986), np.complex128(0.00026473307030911575+0j)) <f>: (np.float32(-0.00087883865), np.complex128(0.001356829137332222+0j))\n",
      "Epoch 44800: <Test loss>: 3.74761366401799e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004175114), np.complex128(0.000264819858855286+0j)) <f>: (np.float32(-0.00086496753), np.complex128(0.0013587013468909735+0j))\n",
      "Epoch 45600: <Test loss>: 3.7947185774100944e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004172995), np.complex128(0.0002646642166881635+0j)) <f>: (np.float32(-0.00086285046), np.complex128(0.0013564702485368764+0j))\n",
      "Epoch 46400: <Test loss>: 3.8349211536115035e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041784844), np.complex128(0.00026884243581828027+0j)) <f>: (np.float32(-0.00086833694), np.complex128(0.0013568540020207649+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72e98714",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0016032276907935739 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 9.092804975807667e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039144373), np.complex128(0.000357244205518429+0j)) <f>: (np.float32(-0.0006042919), np.complex128(0.0012416073123705793+0j))\n",
      "Epoch 3200: <Test loss>: 8.228858496295288e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0039762864), np.complex128(0.0003670331860194683+0j)) <f>: (np.float32(-0.00066614186), np.complex128(0.001318094392705423+0j))\n",
      "Epoch 4800: <Test loss>: 7.240556442411616e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040291143), np.complex128(0.0003515861885532094+0j)) <f>: (np.float32(-0.00071896997), np.complex128(0.0012915269804400162+0j))\n",
      "Epoch 6400: <Test loss>: 6.544490315718576e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0040836595), np.complex128(0.00035365601529858146+0j)) <f>: (np.float32(-0.00077351346), np.complex128(0.001304896571236717+0j))\n",
      "Epoch 8000: <Test loss>: 5.625550329568796e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004140812), np.complex128(0.00032039530797088345+0j)) <f>: (np.float32(-0.0008306635), np.complex128(0.0012998047650936344+0j))\n",
      "Epoch 9600: <Test loss>: 5.124777089804411e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041637574), np.complex128(0.0002975228391672057+0j)) <f>: (np.float32(-0.0008536132), np.complex128(0.0013141853084566513+0j))\n",
      "Epoch 11200: <Test loss>: 4.838738823309541e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041870833), np.complex128(0.0002806715576712273+0j)) <f>: (np.float32(-0.0008769395), np.complex128(0.0013115946870039298+0j))\n",
      "Epoch 12800: <Test loss>: 4.689320849138312e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004188326), np.complex128(0.00027621297649172436+0j)) <f>: (np.float32(-0.0008781799), np.complex128(0.0013153373300722512+0j))\n",
      "Epoch 14400: <Test loss>: 4.533419996732846e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041988324), np.complex128(0.0002670829419522344+0j)) <f>: (np.float32(-0.0008886853), np.complex128(0.0013191359186897943+0j))\n",
      "Epoch 16000: <Test loss>: 4.4412605348043144e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004196859), np.complex128(0.00026009632919608036+0j)) <f>: (np.float32(-0.00088671484), np.complex128(0.001313299186775673+0j))\n",
      "Epoch 17600: <Test loss>: 4.418391836225055e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00418782), np.complex128(0.0002637410675532645+0j)) <f>: (np.float32(-0.00087767304), np.complex128(0.0013092476380104033+0j))\n",
      "Epoch 19200: <Test loss>: 4.408489621710032e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0041743177), np.complex128(0.0002606459847026315+0j)) <f>: (np.float32(-0.000864174), np.complex128(0.001309770684494394+0j))\n",
      "Epoch 20800: <Test loss>: 4.332486059865914e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00421843), np.complex128(0.00025895236323212576+0j)) <f>: (np.float32(-0.0009082809), np.complex128(0.001309952729535511+0j))\n",
      "Epoch 22400: <Test loss>: 4.409422763274051e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042130654), np.complex128(0.0002622019623615607+0j)) <f>: (np.float32(-0.0009029191), np.complex128(0.0013083060782228268+0j))\n",
      "Epoch 24000: <Test loss>: 4.692586662713438e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042635608), np.complex128(0.000270599804768341+0j)) <f>: (np.float32(-0.00095341203), np.complex128(0.0012963923403124407+0j))\n",
      "Epoch 25600: <Test loss>: 4.3229640141362324e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00423555), np.complex128(0.0002560086885742269+0j)) <f>: (np.float32(-0.0009254057), np.complex128(0.0013095877514286858+0j))\n",
      "Epoch 27200: <Test loss>: 4.4569755118573084e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004233865), np.complex128(0.00025846667721127735+0j)) <f>: (np.float32(-0.00092371606), np.complex128(0.0013081070338538281+0j))\n",
      "Epoch 28800: <Test loss>: 4.5415486965794116e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042589046), np.complex128(0.0002607847544025284+0j)) <f>: (np.float32(-0.00094876), np.complex128(0.0013072863722712585+0j))\n",
      "Epoch 30400: <Test loss>: 4.671406713896431e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042821877), np.complex128(0.00026460952388791847+0j)) <f>: (np.float32(-0.00097204413), np.complex128(0.0012972561345179928+0j))\n",
      "Epoch 32000: <Test loss>: 4.5807370042894036e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004297919), np.complex128(0.0002510184978164648+0j)) <f>: (np.float32(-0.0009877688), np.complex128(0.0013229282911352016+0j))\n",
      "Epoch 33600: <Test loss>: 4.745026308228262e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042268126), np.complex128(0.0002528649070893368+0j)) <f>: (np.float32(-0.0009166661), np.complex128(0.0013005772196269885+0j))\n",
      "Epoch 35200: <Test loss>: 4.736944902106188e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004263902), np.complex128(0.0002567727385891321+0j)) <f>: (np.float32(-0.00095375587), np.complex128(0.0013040695666213559+0j))\n",
      "Epoch 36800: <Test loss>: 4.775222987518646e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0042857546), np.complex128(0.00025045263586113135+0j)) <f>: (np.float32(-0.00097561127), np.complex128(0.0013064046907132342+0j))\n",
      "Epoch 38400: <Test loss>: 4.991157038602978e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004303727), np.complex128(0.0002527791017132492+0j)) <f>: (np.float32(-0.0009935793), np.complex128(0.0013098622778879038+0j))\n",
      "Epoch 40000: <Test loss>: 4.9432932428317145e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004291629), np.complex128(0.0002480420613941976+0j)) <f>: (np.float32(-0.0009814856), np.complex128(0.001302872509472936+0j))\n",
      "Epoch 41600: <Test loss>: 5.000552846468054e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00430141), np.complex128(0.00024589220143257824+0j)) <f>: (np.float32(-0.000991262), np.complex128(0.0013038615151457931+0j))\n",
      "Epoch 43200: <Test loss>: 5.132340811542235e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004307091), np.complex128(0.00024539378177340084+0j)) <f>: (np.float32(-0.0009969465), np.complex128(0.0013030993363255614+0j))\n",
      "Epoch 44800: <Test loss>: 5.143534508533776e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004306214), np.complex128(0.00025027599825546885+0j)) <f>: (np.float32(-0.0009960664), np.complex128(0.001292366924842272+0j))\n",
      "Epoch 46400: <Test loss>: 5.3050425776746124e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004307429), np.complex128(0.0002495957714189039+0j)) <f>: (np.float32(-0.000997282), np.complex128(0.001298134263977654+0j))\n",
      "Epoch 48000: <Test loss>: 5.2930063247913495e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004315445), np.complex128(0.00024075596234479093+0j)) <f>: (np.float32(-0.0010053001), np.complex128(0.0013106356204458487+0j))\n",
      "Epoch 49600: <Test loss>: 5.358163980417885e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043206285), np.complex128(0.00023945619563789402+0j)) <f>: (np.float32(-0.0010104823), np.complex128(0.0013056062297454349+0j))\n",
      "Epoch 51200: <Test loss>: 5.4603857279289514e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004327443), np.complex128(0.00023898108662422554+0j)) <f>: (np.float32(-0.0010172981), np.complex128(0.0013075879200501685+0j))\n",
      "Epoch 52800: <Test loss>: 5.467916707857512e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004334175), np.complex128(0.00023581984594157676+0j)) <f>: (np.float32(-0.0010240304), np.complex128(0.0013076508429354604+0j))\n",
      "Epoch 54400: <Test loss>: 5.539854464586824e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004332999), np.complex128(0.00023529247033770597+0j)) <f>: (np.float32(-0.0010228532), np.complex128(0.0013085915146984462+0j))\n",
      "Epoch 56000: <Test loss>: 5.542597864405252e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004331305), np.complex128(0.0002336000857585946+0j)) <f>: (np.float32(-0.0010211588), np.complex128(0.001311696429249906+0j))\n",
      "Epoch 57600: <Test loss>: 5.6945355026982725e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043359343), np.complex128(0.00023477367368568574+0j)) <f>: (np.float32(-0.0010257907), np.complex128(0.0013122864581601753+0j))\n",
      "Epoch 59200: <Test loss>: 5.750940181314945e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043358617), np.complex128(0.00023470870517231836+0j)) <f>: (np.float32(-0.0010257149), np.complex128(0.0013134148836937906+0j))\n",
      "Epoch 60800: <Test loss>: 5.836136915604584e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043562194), np.complex128(0.0002355371528276397+0j)) <f>: (np.float32(-0.0010460755), np.complex128(0.0013172927602212278+0j))\n",
      "Epoch 62400: <Test loss>: 5.861586396349594e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004338345), np.complex128(0.00023515328834067755+0j)) <f>: (np.float32(-0.0010281968), np.complex128(0.0013081392564604092+0j))\n",
      "Epoch 64000: <Test loss>: 5.896815855521709e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004331824), np.complex128(0.00023549784188191413+0j)) <f>: (np.float32(-0.0010216773), np.complex128(0.0013079929861242363+0j))\n",
      "Epoch 65600: <Test loss>: 6.000700159347616e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043330537), np.complex128(0.00023437139854604614+0j)) <f>: (np.float32(-0.0010229065), np.complex128(0.0013066509272462018+0j))\n",
      "Epoch 67200: <Test loss>: 6.105327338445932e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043436415), np.complex128(0.00023668822298832085+0j)) <f>: (np.float32(-0.001033491), np.complex128(0.0013101648405520604+0j))\n",
      "Epoch 68800: <Test loss>: 6.157065217848867e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043562325), np.complex128(0.00023428925627139564+0j)) <f>: (np.float32(-0.0010460896), np.complex128(0.0013181030192300194+0j))\n",
      "Epoch 70400: <Test loss>: 6.310284516075626e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004340894), np.complex128(0.00023639785480470603+0j)) <f>: (np.float32(-0.0010307471), np.complex128(0.001313213048390364+0j))\n",
      "Epoch 72000: <Test loss>: 6.411411595763639e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043432824), np.complex128(0.0002399083111576932+0j)) <f>: (np.float32(-0.0010331353), np.complex128(0.00130831724196054+0j))\n",
      "Epoch 73600: <Test loss>: 6.492011016234756e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004351533), np.complex128(0.00023755537900125258+0j)) <f>: (np.float32(-0.0010413864), np.complex128(0.001312070033881328+0j))\n",
      "Epoch 75200: <Test loss>: 6.731572648277506e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043660486), np.complex128(0.00024376712687159183+0j)) <f>: (np.float32(-0.0010559013), np.complex128(0.0013123604179225248+0j))\n",
      "Epoch 76800: <Test loss>: 6.294775084825233e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004347327), np.complex128(0.00023376308584375526+0j)) <f>: (np.float32(-0.0010371825), np.complex128(0.001309141455641473+0j))\n",
      "Epoch 78400: <Test loss>: 6.612273136852309e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043537025), np.complex128(0.0002379783324277926+0j)) <f>: (np.float32(-0.0010435575), np.complex128(0.0013104713358965477+0j))\n",
      "Epoch 80000: <Test loss>: 6.497575668618083e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.00433099), np.complex128(0.0002233891508586799+0j)) <f>: (np.float32(-0.0010208461), np.complex128(0.0013341468326516345+0j))\n",
      "Epoch 81600: <Test loss>: 6.740032404195517e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043468275), np.complex128(0.00023572622277757358+0j)) <f>: (np.float32(-0.0010366791), np.complex128(0.0013160615775563895+0j))\n",
      "Epoch 83200: <Test loss>: 6.81635137880221e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004345578), np.complex128(0.00023702870113098887+0j)) <f>: (np.float32(-0.0010354299), np.complex128(0.0013141667868009001+0j))\n",
      "Epoch 84800: <Test loss>: 6.874925020383671e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043387255), np.complex128(0.0002364202298528782+0j)) <f>: (np.float32(-0.001028582), np.complex128(0.0013134188163741215+0j))\n",
      "Epoch 86400: <Test loss>: 6.881522131152451e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043382975), np.complex128(0.00023497600057415395+0j)) <f>: (np.float32(-0.0010281503), np.complex128(0.0013128060794064585+0j))\n",
      "Epoch 88000: <Test loss>: 7.06832215655595e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.0043644817), np.complex128(0.00023794103539497835+0j)) <f>: (np.float32(-0.001054335), np.complex128(0.0013171907642539398+0j))\n",
      "Epoch 89600: <Test loss>: 7.102236850187182e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004341662), np.complex128(0.00023719579247230007+0j)) <f>: (np.float32(-0.0010315173), np.complex128(0.0013136975292349817+0j))\n",
      "Epoch 91200: <Test loss>: 7.085438119247556e-05 <O>: (np.float32(0.0033101444), np.complex128(0.0014146631342763219+0j)) <O-f>: (np.float32(0.004342761), np.complex128(0.00023428730578881226+0j)) <f>: (np.float32(-0.0010326148), np.complex128(0.0013138925140629939+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b8ae18",
   "metadata": {},
   "source": [
    "## m^2=0.01, lambda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5096bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0655574), np.complex128(0.00043881599360227723+0j))\n",
      "bin size 1: (np.float32(0.0655574), np.complex128(0.0004388045557918774+0j))\n",
      "jack bin size 2: (np.float32(0.0655574), np.complex128(0.000618114463713151+0j))\n",
      "bin size 2: (np.float32(0.0655574), np.complex128(0.0006180990855351279+0j))\n",
      "jack bin size 4: (np.float32(0.0655574), np.complex128(0.0008692468800647937+0j))\n",
      "bin size 4: (np.float32(0.0655574), np.complex128(0.0008692373835591271+0j))\n",
      "jack bin size 5: (np.float32(0.0655574), np.complex128(0.0009693452423373169+0j))\n",
      "bin size 5: (np.float32(0.0655574), np.complex128(0.000969348539370731+0j))\n",
      "jack bin size 10: (np.float32(0.0655574), np.complex128(0.001354533404222054+0j))\n",
      "bin size 10: (np.float32(0.0655574), np.complex128(0.0013545271031914542+0j))\n",
      "jack bin size 20: (np.float32(0.0655574), np.complex128(0.0018751387205107606+0j))\n",
      "bin size 20: (np.float32(0.0655574), np.complex128(0.0018751393911356272+0j))\n",
      "jack bin size 50: (np.float32(0.0655574), np.complex128(0.002798822489714116+0j))\n",
      "bin size 50: (np.float32(0.0655574), np.complex128(0.002798821883685075+0j))\n",
      "jack bin size 100: (np.float32(0.0655574), np.complex128(0.0036262815829836083+0j))\n",
      "bin size 100: (np.float32(0.0655574), np.complex128(0.0036262739412275097+0j))\n",
      "jack bin size 200: (np.float32(0.0655574), np.complex128(0.004513166039777178+0j))\n",
      "bin size 200: (np.float32(0.0655574), np.complex128(0.0045131621761269145+0j))\n",
      "jack bin size 500: (np.float32(0.0655574), np.complex128(0.004906173328066253+0j))\n",
      "bin size 500: (np.float32(0.0655574), np.complex128(0.004906171068954407+0j))\n",
      "jack bin size 1000: (np.float32(0.0655574), np.complex128(0.004735562645066791+0j))\n",
      "bin size 1000: (np.float32(0.0655574), np.complex128(0.004735566450550359+0j))\n",
      "jack bin size 2000: (np.float32(0.0655574), np.complex128(0.005015448958147317+0j))\n",
      "bin size 2000: (np.float32(0.0655574), np.complex128(0.005015455718551363+0j))\n",
      "jack bin size 5000: (np.float32(0.0655574), np.complex128(0.005991750561120333+0j))\n",
      "bin size 5000: (np.float32(0.0655574), np.complex128(0.005991748317689787+0j))\n",
      "jack bin size 10000: (np.float32(0.0655574), np.complex128(0.006233953172340989+0j))\n",
      "bin size 10000: (np.float32(0.0655574), np.complex128(0.0062339454889297485+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_16x16_0.01_0.1_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(16,16), m2=0.01, lamda=0.1)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0ed2b2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a4eb14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.0005843210383318365 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07242963), np.complex128(0.0006564083678507742+0j)) <f>: (np.float32(0.006163756), np.complex128(0.005099431990825866+0j))\n",
      "Epoch 200: <Test loss>: 0.0003326138248667121 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07275199), np.complex128(0.0005071747887637508+0j)) <f>: (np.float32(0.0058413236), np.complex128(0.0051470630926641146+0j))\n",
      "Epoch 300: <Test loss>: 0.0002213131228927523 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0729363), np.complex128(0.00048483025057931316+0j)) <f>: (np.float32(0.0056570787), np.complex128(0.005053365842037947+0j))\n",
      "Epoch 400: <Test loss>: 0.00017062995175365359 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07271812), np.complex128(0.0004780699413755794+0j)) <f>: (np.float32(0.0058752457), np.complex128(0.0050603507897479935+0j))\n",
      "Epoch 500: <Test loss>: 0.0001345294585917145 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07250305), np.complex128(0.0003760458110219868+0j)) <f>: (np.float32(0.0060902885), np.complex128(0.0051279939063222455+0j))\n",
      "Epoch 600: <Test loss>: 0.00012031040387228131 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07256841), np.complex128(0.0003542391620182719+0j)) <f>: (np.float32(0.0060249586), np.complex128(0.005036317292222152+0j))\n",
      "Epoch 700: <Test loss>: 9.366893937112764e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072575204), np.complex128(0.00031465181863879786+0j)) <f>: (np.float32(0.006018148), np.complex128(0.005036983564386576+0j))\n",
      "Epoch 800: <Test loss>: 7.565678970422596e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072468996), np.complex128(0.00023917185333543117+0j)) <f>: (np.float32(0.006124431), np.complex128(0.005095482564888541+0j))\n",
      "Epoch 900: <Test loss>: 7.067436672514305e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07264648), np.complex128(0.0002706074798380188+0j)) <f>: (np.float32(0.0059468835), np.complex128(0.005054321863940289+0j))\n",
      "Epoch 1000: <Test loss>: 6.023977402946912e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07257341), np.complex128(0.0002537269093881254+0j)) <f>: (np.float32(0.0060199946), np.complex128(0.005122558688383829+0j))\n",
      "Epoch 1100: <Test loss>: 4.898570114164613e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07271352), np.complex128(0.00020828853055558514+0j)) <f>: (np.float32(0.0058798296), np.complex128(0.005083318657764856+0j))\n",
      "Epoch 1200: <Test loss>: 4.879548214375973e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0725338), np.complex128(0.0002236586028916646+0j)) <f>: (np.float32(0.0060595362), np.complex128(0.005095472923478697+0j))\n",
      "Epoch 1300: <Test loss>: 3.827643740805797e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07271414), np.complex128(0.00020428899465920613+0j)) <f>: (np.float32(0.0058792294), np.complex128(0.0050826691312070025+0j))\n",
      "Epoch 1400: <Test loss>: 3.283922706032172e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072582245), np.complex128(0.00015853953764098666+0j)) <f>: (np.float32(0.006011119), np.complex128(0.0051563320396217395+0j))\n",
      "Epoch 1500: <Test loss>: 3.172363358316943e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07263373), np.complex128(0.00017368620836214803+0j)) <f>: (np.float32(0.0059596025), np.complex128(0.005099509629547235+0j))\n",
      "Epoch 1600: <Test loss>: 2.8232600016053766e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07248967), np.complex128(0.00016260884689807262+0j)) <f>: (np.float32(0.006103699), np.complex128(0.005179425245966563+0j))\n",
      "Epoch 1700: <Test loss>: 2.3315604266826995e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07271588), np.complex128(0.00015591409293823946+0j)) <f>: (np.float32(0.005877498), np.complex128(0.005129909502225292+0j))\n",
      "Epoch 1800: <Test loss>: 2.6247387722833082e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07269638), np.complex128(0.0001662634962439935+0j)) <f>: (np.float32(0.0058970484), np.complex128(0.005102405604598541+0j))\n",
      "Epoch 1900: <Test loss>: 3.395730163902044e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0727564), np.complex128(0.00021122240522782108+0j)) <f>: (np.float32(0.0058368696), np.complex128(0.0050535693265299+0j))\n",
      "Epoch 2000: <Test loss>: 2.1295914848451503e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072795495), np.complex128(0.00015428317234687813+0j)) <f>: (np.float32(0.00579794), np.complex128(0.005142085587971936+0j))\n",
      "Epoch 2100: <Test loss>: 2.4260783902718686e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07267385), np.complex128(0.0001679236106437779+0j)) <f>: (np.float32(0.0059194705), np.complex128(0.005099954656727889+0j))\n",
      "Epoch 2200: <Test loss>: 2.778255657176487e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07272911), np.complex128(0.0001665945391253841+0j)) <f>: (np.float32(0.0058643008), np.complex128(0.005082734591305411+0j))\n",
      "Epoch 2300: <Test loss>: 2.0611114450730383e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0727216), np.complex128(0.0001399596369910752+0j)) <f>: (np.float32(0.005871752), np.complex128(0.005122248133498355+0j))\n",
      "Epoch 2400: <Test loss>: 1.8745822671917267e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0726313), np.complex128(0.00013221626077893525+0j)) <f>: (np.float32(0.005962053), np.complex128(0.005143133456989099+0j))\n",
      "Epoch 2500: <Test loss>: 2.51676046900684e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07263924), np.complex128(0.00012830971374328025+0j)) <f>: (np.float32(0.00595407), np.complex128(0.005129768940618631+0j))\n",
      "Epoch 2600: <Test loss>: 1.9940433048759587e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0726236), np.complex128(0.00013706922795104327+0j)) <f>: (np.float32(0.00596974), np.complex128(0.005135544652557298+0j))\n",
      "Epoch 2700: <Test loss>: 1.860342308646068e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072660916), np.complex128(0.00014151109523914047+0j)) <f>: (np.float32(0.0059324102), np.complex128(0.005156336606605349+0j))\n",
      "Epoch 2800: <Test loss>: 1.8182647181674838e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07266916), np.complex128(0.00012009485990374498+0j)) <f>: (np.float32(0.005924174), np.complex128(0.005139075438330381+0j))\n",
      "Epoch 2900: <Test loss>: 1.870168125606142e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07254413), np.complex128(0.0001233033165367358+0j)) <f>: (np.float32(0.006049273), np.complex128(0.00517044960084522+0j))\n",
      "Epoch 3000: <Test loss>: 2.0632367522921413e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07266658), np.complex128(0.0001427834600443779+0j)) <f>: (np.float32(0.0059267622), np.complex128(0.005124850806713381+0j))\n",
      "Epoch 3100: <Test loss>: 1.8759072190732695e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07261566), np.complex128(0.00012481283148046702+0j)) <f>: (np.float32(0.00597763), np.complex128(0.005158782480049768+0j))\n",
      "Epoch 3200: <Test loss>: 1.9856825019815005e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07255803), np.complex128(0.00013194063221776977+0j)) <f>: (np.float32(0.0060353386), np.complex128(0.0051850152339050935+0j))\n",
      "Epoch 3300: <Test loss>: 2.043459244305268e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07264539), np.complex128(0.00012866175999200183+0j)) <f>: (np.float32(0.005947988), np.complex128(0.005131682506751184+0j))\n",
      "Epoch 3400: <Test loss>: 1.9639615857158788e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07260799), np.complex128(0.00013779415731120665+0j)) <f>: (np.float32(0.005985377), np.complex128(0.005155258291030787+0j))\n",
      "Epoch 3500: <Test loss>: 1.8997114239027724e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0726612), np.complex128(0.0001391826789044392+0j)) <f>: (np.float32(0.005932142), np.complex128(0.005145700609220491+0j))\n",
      "Epoch 3600: <Test loss>: 2.001493157877121e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07242712), np.complex128(0.00013731979360389116+0j)) <f>: (np.float32(0.006166232), np.complex128(0.005160695031297074+0j))\n",
      "Epoch 3700: <Test loss>: 2.2978876586421393e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07256148), np.complex128(0.00014950802625833133+0j)) <f>: (np.float32(0.006031887), np.complex128(0.005139030783379529+0j))\n",
      "Epoch 3800: <Test loss>: 1.8824346625478938e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0725546), np.complex128(0.0001311940334718323+0j)) <f>: (np.float32(0.0060387766), np.complex128(0.005163902576119101+0j))\n",
      "Epoch 3900: <Test loss>: 1.874509507615585e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07258354), np.complex128(0.00014240980784021012+0j)) <f>: (np.float32(0.0060098236), np.complex128(0.005168599465040582+0j))\n",
      "Epoch 4000: <Test loss>: 1.9755376342800446e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07260412), np.complex128(0.0001472785612351781+0j)) <f>: (np.float32(0.005989256), np.complex128(0.005135665423901648+0j))\n",
      "Epoch 4100: <Test loss>: 2.6541174520389177e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072602205), np.complex128(0.00014902398942600873+0j)) <f>: (np.float32(0.00599116), np.complex128(0.005164557177103188+0j))\n",
      "Epoch 4200: <Test loss>: 2.157685230486095e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07257208), np.complex128(0.00014240446383508324+0j)) <f>: (np.float32(0.006021317), np.complex128(0.005145686908269662+0j))\n",
      "Epoch 4300: <Test loss>: 2.1714613467338495e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072520435), np.complex128(0.00016330367856764015+0j)) <f>: (np.float32(0.006072947), np.complex128(0.005206970753888426+0j))\n",
      "Epoch 4400: <Test loss>: 1.987134419323411e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07255501), np.complex128(0.00014139033975237147+0j)) <f>: (np.float32(0.0060384194), np.complex128(0.005169173890090185+0j))\n",
      "Epoch 4500: <Test loss>: 1.947581949934829e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07258626), np.complex128(0.0001221813053950309+0j)) <f>: (np.float32(0.0060071377), np.complex128(0.005189963814367743+0j))\n",
      "Epoch 4600: <Test loss>: 2.4998464141390286e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072576106), np.complex128(0.00015989623306718765+0j)) <f>: (np.float32(0.0060172016), np.complex128(0.005159687250247154+0j))\n",
      "Epoch 4700: <Test loss>: 2.0760524421348237e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07256355), np.complex128(0.00012878716175029062+0j)) <f>: (np.float32(0.0060298583), np.complex128(0.005165744592841765+0j))\n",
      "Epoch 4800: <Test loss>: 3.284751801402308e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072648115), np.complex128(0.00017635483326062368+0j)) <f>: (np.float32(0.00594521), np.complex128(0.005114311730869577+0j))\n",
      "Epoch 4900: <Test loss>: 2.3015649276203476e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07259341), np.complex128(0.00014458570595143877+0j)) <f>: (np.float32(0.005999944), np.complex128(0.005134098441080826+0j))\n",
      "Epoch 5000: <Test loss>: 2.1469428247655742e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07247556), np.complex128(0.0001398649830842434+0j)) <f>: (np.float32(0.0061178464), np.complex128(0.005181324096263039+0j))\n",
      "Epoch 5100: <Test loss>: 2.4727763957343996e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07262935), np.complex128(0.00014795677415883177+0j)) <f>: (np.float32(0.0059640324), np.complex128(0.005132759299997875+0j))\n",
      "Epoch 5200: <Test loss>: 2.035138641076628e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07254604), np.complex128(0.00012706008871060052+0j)) <f>: (np.float32(0.0060472833), np.complex128(0.005186699435971904+0j))\n",
      "Epoch 5300: <Test loss>: 2.468933780619409e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07253137), np.complex128(0.00014703633667045109+0j)) <f>: (np.float32(0.0060620117), np.complex128(0.0051323477640303605+0j))\n",
      "Epoch 5400: <Test loss>: 2.272218989674002e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072455056), np.complex128(0.00015480821688619803+0j)) <f>: (np.float32(0.006138307), np.complex128(0.005187436750103592+0j))\n",
      "Epoch 5500: <Test loss>: 2.1344223569030873e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07250369), np.complex128(0.00012848004796010612+0j)) <f>: (np.float32(0.0060896957), np.complex128(0.005204094061656806+0j))\n",
      "Epoch 5600: <Test loss>: 2.0908510123263113e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072563715), np.complex128(0.0001347913655876906+0j)) <f>: (np.float32(0.0060296524), np.complex128(0.005175758973013052+0j))\n",
      "Epoch 5700: <Test loss>: 2.0526154912658967e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072539456), np.complex128(0.00012978888520389492+0j)) <f>: (np.float32(0.0060539106), np.complex128(0.005185621120397342+0j))\n",
      "Epoch 5800: <Test loss>: 2.0949033569195308e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07246618), np.complex128(0.00013710143470004233+0j)) <f>: (np.float32(0.006127208), np.complex128(0.005176907823112256+0j))\n",
      "Epoch 5900: <Test loss>: 2.3829077690606937e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07250324), np.complex128(0.00012990436011586494+0j)) <f>: (np.float32(0.006090166), np.complex128(0.005173716009011552+0j))\n",
      "Epoch 6000: <Test loss>: 2.421415047137998e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07251689), np.complex128(0.00014617254246489887+0j)) <f>: (np.float32(0.006076443), np.complex128(0.00517601827619357+0j))\n",
      "Epoch 6100: <Test loss>: 3.0493058147840202e-05 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072534725), np.complex128(0.00014742635389922103+0j)) <f>: (np.float32(0.006058649), np.complex128(0.005137355715279939+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea56fdab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.001182002481073141 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072304554), np.complex128(0.0011533166091746104+0j)) <f>: (np.float32(0.006288764), np.complex128(0.004759756495527304+0j))\n",
      "Epoch 400: <Test loss>: 0.00109788216650486 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07199479), np.complex128(0.0010258764538679317+0j)) <f>: (np.float32(0.00659855), np.complex128(0.004867897085541124+0j))\n",
      "Epoch 600: <Test loss>: 0.0010320822475478053 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07202538), np.complex128(0.0009945013398981754+0j)) <f>: (np.float32(0.0065679546), np.complex128(0.004825604280100238+0j))\n",
      "Epoch 800: <Test loss>: 0.0009388949838466942 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07197797), np.complex128(0.000990793139497912+0j)) <f>: (np.float32(0.0066154054), np.complex128(0.0047409014499724764+0j))\n",
      "Epoch 1000: <Test loss>: 0.0008980854181572795 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072081156), np.complex128(0.0009911281150596332+0j)) <f>: (np.float32(0.006512181), np.complex128(0.00473239569672033+0j))\n",
      "Epoch 1200: <Test loss>: 0.000859235820826143 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07165402), np.complex128(0.0009551508136479736+0j)) <f>: (np.float32(0.006939322), np.complex128(0.004765084643072199+0j))\n",
      "Epoch 1400: <Test loss>: 0.0008169048232957721 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07175451), np.complex128(0.0009487807595367395+0j)) <f>: (np.float32(0.0068388865), np.complex128(0.004750241438897367+0j))\n",
      "Epoch 1600: <Test loss>: 0.0007881274214014411 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07147329), np.complex128(0.0009080321654289539+0j)) <f>: (np.float32(0.007120066), np.complex128(0.004794314353060871+0j))\n",
      "Epoch 1800: <Test loss>: 0.0007477688486687839 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071542665), np.complex128(0.0009076097194450372+0j)) <f>: (np.float32(0.0070507196), np.complex128(0.004761161096708663+0j))\n",
      "Epoch 2000: <Test loss>: 0.0007166019640862942 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071422346), np.complex128(0.000824923022290068+0j)) <f>: (np.float32(0.0071710623), np.complex128(0.004836905027321649+0j))\n",
      "Epoch 2200: <Test loss>: 0.0006833717925474048 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07143656), np.complex128(0.0008044652192094744+0j)) <f>: (np.float32(0.0071567968), np.complex128(0.004847034089525807+0j))\n",
      "Epoch 2400: <Test loss>: 0.0006645300891250372 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07158703), np.complex128(0.0007507822932151435+0j)) <f>: (np.float32(0.007006316), np.complex128(0.004813158742320608+0j))\n",
      "Epoch 2600: <Test loss>: 0.0006459221476688981 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071343385), np.complex128(0.0007181287409862519+0j)) <f>: (np.float32(0.0072499), np.complex128(0.0048660068617692445+0j))\n",
      "Epoch 2800: <Test loss>: 0.000623283616732806 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07140734), np.complex128(0.0007017981590521312+0j)) <f>: (np.float32(0.007186067), np.complex128(0.00485059836651203+0j))\n",
      "Epoch 3000: <Test loss>: 0.0006161986966617405 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07132802), np.complex128(0.0006920692153564638+0j)) <f>: (np.float32(0.007265295), np.complex128(0.004846983852706098+0j))\n",
      "Epoch 3200: <Test loss>: 0.0005944783915765584 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07148546), np.complex128(0.0006925580728937079+0j)) <f>: (np.float32(0.0071079307), np.complex128(0.004836970487420058+0j))\n",
      "Epoch 3400: <Test loss>: 0.0005909112514927983 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07154945), np.complex128(0.000655795694313439+0j)) <f>: (np.float32(0.007043978), np.complex128(0.004846187675230104+0j))\n",
      "Epoch 3600: <Test loss>: 0.0005789718707092106 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07143424), np.complex128(0.0006423978770208157+0j)) <f>: (np.float32(0.0071591535), np.complex128(0.004841344135390482+0j))\n",
      "Epoch 3800: <Test loss>: 0.0005684725474566221 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07155656), np.complex128(0.0005916480328196219+0j)) <f>: (np.float32(0.0070368424), np.complex128(0.00491448640767369+0j))\n",
      "Epoch 4000: <Test loss>: 0.0005533947260119021 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071485594), np.complex128(0.0006026361953850662+0j)) <f>: (np.float32(0.0071077417), np.complex128(0.004890328079262511+0j))\n",
      "Epoch 4200: <Test loss>: 0.0005415217601694167 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07158761), np.complex128(0.0005514106871548346+0j)) <f>: (np.float32(0.007005729), np.complex128(0.004910044254949117+0j))\n",
      "Epoch 4400: <Test loss>: 0.0005417025531642139 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07161201), np.complex128(0.0005438827758378322+0j)) <f>: (np.float32(0.0069814157), np.complex128(0.004886195973980789+0j))\n",
      "Epoch 4600: <Test loss>: 0.0005327833932824433 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07148817), np.complex128(0.0005432879896529693+0j)) <f>: (np.float32(0.007105126), np.complex128(0.004901695301467578+0j))\n",
      "Epoch 4800: <Test loss>: 0.0005422247922979295 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0717264), np.complex128(0.0006106499195835727+0j)) <f>: (np.float32(0.0068669543), np.complex128(0.004807581440447661+0j))\n",
      "Epoch 5000: <Test loss>: 0.0005117986584082246 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07173278), np.complex128(0.0004965032386800963+0j)) <f>: (np.float32(0.006860613), np.complex128(0.00492280186994209+0j))\n",
      "Epoch 5200: <Test loss>: 0.0005157411796972156 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0717935), np.complex128(0.0005055302940822156+0j)) <f>: (np.float32(0.0067998613), np.complex128(0.004901544083565827+0j))\n",
      "Epoch 5400: <Test loss>: 0.0005212030373513699 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071538165), np.complex128(0.0004743135339261002+0j)) <f>: (np.float32(0.007055194), np.complex128(0.0050118397824009025+0j))\n",
      "Epoch 5600: <Test loss>: 0.0005138439009897411 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07180557), np.complex128(0.00048105478231629537+0j)) <f>: (np.float32(0.006787846), np.complex128(0.004936353632640564+0j))\n",
      "Epoch 5800: <Test loss>: 0.0005098069668747485 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07180519), np.complex128(0.0004783497642671783+0j)) <f>: (np.float32(0.0067881607), np.complex128(0.004914858870559209+0j))\n",
      "Epoch 6000: <Test loss>: 0.0005015158676542342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0718596), np.complex128(0.0004812896648205662+0j)) <f>: (np.float32(0.006733781), np.complex128(0.004928406573716696+0j))\n",
      "Epoch 6200: <Test loss>: 0.0005015784990973771 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07182952), np.complex128(0.0004797693667210894+0j)) <f>: (np.float32(0.006763861), np.complex128(0.00490037493576169+0j))\n",
      "Epoch 6400: <Test loss>: 0.0004990492598153651 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07191061), np.complex128(0.00046303669993831277+0j)) <f>: (np.float32(0.006682707), np.complex128(0.0049412651697917104+0j))\n",
      "Epoch 6600: <Test loss>: 0.000500422902405262 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07196908), np.complex128(0.00047279691478564246+0j)) <f>: (np.float32(0.006624251), np.complex128(0.004949446667207554+0j))\n",
      "Epoch 6800: <Test loss>: 0.0004960090154781938 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0718812), np.complex128(0.0004171309347345277+0j)) <f>: (np.float32(0.0067121824), np.complex128(0.004991375636287518+0j))\n",
      "Epoch 7000: <Test loss>: 0.0004933230229653418 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071940914), np.complex128(0.0004316488999029734+0j)) <f>: (np.float32(0.0066523575), np.complex128(0.004966365819154402+0j))\n",
      "Epoch 7200: <Test loss>: 0.0005016133654862642 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07206881), np.complex128(0.00044064043432146005+0j)) <f>: (np.float32(0.0065245396), np.complex128(0.004956704111606324+0j))\n",
      "Epoch 7400: <Test loss>: 0.0004945119726471603 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07194829), np.complex128(0.00044183822091865095+0j)) <f>: (np.float32(0.006645061), np.complex128(0.004953292574849721+0j))\n",
      "Epoch 7600: <Test loss>: 0.0004883859655819833 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0720603), np.complex128(0.00040002808855803674+0j)) <f>: (np.float32(0.0065331506), np.complex128(0.004992665047993382+0j))\n",
      "Epoch 7800: <Test loss>: 0.00048724719090387225 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072201654), np.complex128(0.0004453516584970501+0j)) <f>: (np.float32(0.006391679), np.complex128(0.004939022780839245+0j))\n",
      "Epoch 8000: <Test loss>: 0.00048777571646496654 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072135), np.complex128(0.0003985583761450725+0j)) <f>: (np.float32(0.006458396), np.complex128(0.0049841532054297555+0j))\n",
      "Epoch 8200: <Test loss>: 0.0004856976156588644 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07218352), np.complex128(0.0004087385949086941+0j)) <f>: (np.float32(0.0064098053), np.complex128(0.004956997413442605+0j))\n",
      "Epoch 8400: <Test loss>: 0.00048350769793614745 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072155945), np.complex128(0.00039499359171622577+0j)) <f>: (np.float32(0.0064374004), np.complex128(0.0049839603772328924+0j))\n",
      "Epoch 8600: <Test loss>: 0.00048011288163252175 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07212607), np.complex128(0.0003878504170531945+0j)) <f>: (np.float32(0.006467279), np.complex128(0.005005594686035662+0j))\n",
      "Epoch 8800: <Test loss>: 0.0005028033629059792 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072431244), np.complex128(0.00048751008678824784+0j)) <f>: (np.float32(0.0061621354), np.complex128(0.004865531388031191+0j))\n",
      "Epoch 9000: <Test loss>: 0.00047417214955203235 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07226463), np.complex128(0.00038660981498466076+0j)) <f>: (np.float32(0.0063287276), np.complex128(0.005004371241870829+0j))\n",
      "Epoch 9200: <Test loss>: 0.00048046387382782996 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07231818), np.complex128(0.00040450306647730577+0j)) <f>: (np.float32(0.0062752087), np.complex128(0.004979988623820141+0j))\n",
      "Epoch 9400: <Test loss>: 0.0004786413337569684 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07232976), np.complex128(0.0004322691533644944+0j)) <f>: (np.float32(0.0062635946), np.complex128(0.0049458108408114414+0j))\n",
      "Epoch 9600: <Test loss>: 0.0004748973879031837 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07240505), np.complex128(0.00040572073848229814+0j)) <f>: (np.float32(0.006188322), np.complex128(0.004973442613979269+0j))\n",
      "Epoch 9800: <Test loss>: 0.0004702938604168594 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07227108), np.complex128(0.00039697004901890635+0j)) <f>: (np.float32(0.006322262), np.complex128(0.005017078112601469+0j))\n",
      "Epoch 10000: <Test loss>: 0.0004675423842854798 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07238384), np.complex128(0.000378418581013483+0j)) <f>: (np.float32(0.006209475), np.complex128(0.0049956391692086806+0j))\n",
      "Epoch 10200: <Test loss>: 0.0004710402572527528 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07229793), np.complex128(0.00035181422058206535+0j)) <f>: (np.float32(0.0062954375), np.complex128(0.0050256939809028775+0j))\n",
      "Epoch 10400: <Test loss>: 0.00046307238517329097 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07234266), np.complex128(0.00038634140955208664+0j)) <f>: (np.float32(0.0062506674), np.complex128(0.005003359908722546+0j))\n",
      "Epoch 10600: <Test loss>: 0.00046501681208610535 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07242439), np.complex128(0.0003881560878034189+0j)) <f>: (np.float32(0.0061689536), np.complex128(0.004982830302510751+0j))\n",
      "Epoch 10800: <Test loss>: 0.000479029054986313 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07245693), np.complex128(0.00036909210274843894+0j)) <f>: (np.float32(0.006136424), np.complex128(0.005046170305639222+0j))\n",
      "Epoch 11000: <Test loss>: 0.00046290556201711297 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07240919), np.complex128(0.0003853007715923062+0j)) <f>: (np.float32(0.0061842133), np.complex128(0.0049975136622592365+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0587bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cada93e6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.001718533574603498 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0725089), np.complex128(0.0013923143462599137+0j)) <f>: (np.float32(0.0060845097), np.complex128(0.005273428991937218+0j))\n",
      "Epoch 800: <Test loss>: 0.001601837924681604 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0722155), np.complex128(0.0012678768558160953+0j)) <f>: (np.float32(0.0063778628), np.complex128(0.0052979709469716295+0j))\n",
      "Epoch 1200: <Test loss>: 0.0014799785567447543 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07189159), np.complex128(0.0011543837610114595+0j)) <f>: (np.float32(0.006701796), np.complex128(0.005296636372872289+0j))\n",
      "Epoch 1600: <Test loss>: 0.0013916423777118325 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07202181), np.complex128(0.0010718578597377559+0j)) <f>: (np.float32(0.006571605), np.complex128(0.005286321586667995+0j))\n",
      "Epoch 2000: <Test loss>: 0.0013319267891347408 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07208101), np.complex128(0.0010587751008838886+0j)) <f>: (np.float32(0.00651233), np.complex128(0.005217322075961599+0j))\n",
      "Epoch 2400: <Test loss>: 0.0012773015769198537 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07220931), np.complex128(0.0009912856125638473+0j)) <f>: (np.float32(0.006384071), np.complex128(0.005242669849881848+0j))\n",
      "Epoch 2800: <Test loss>: 0.0012241842923685908 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07212661), np.complex128(0.0009708984074382237+0j)) <f>: (np.float32(0.006466747), np.complex128(0.005173392768060495+0j))\n",
      "Epoch 3200: <Test loss>: 0.001190998824313283 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072095655), np.complex128(0.0009627885862929254+0j)) <f>: (np.float32(0.0064977515), np.complex128(0.005201975996147054+0j))\n",
      "Epoch 3600: <Test loss>: 0.0011550845811143517 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07217351), np.complex128(0.0009291596659116784+0j)) <f>: (np.float32(0.006419847), np.complex128(0.0051519923903070785+0j))\n",
      "Epoch 4000: <Test loss>: 0.0011132637737318873 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072071426), np.complex128(0.000889810281407379+0j)) <f>: (np.float32(0.0065219617), np.complex128(0.005136521987049818+0j))\n",
      "Epoch 4400: <Test loss>: 0.0011056533548980951 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072032645), np.complex128(0.0009372965083766687+0j)) <f>: (np.float32(0.0065607266), np.complex128(0.005170284174550016+0j))\n",
      "Epoch 4800: <Test loss>: 0.0010706160683184862 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07230692), np.complex128(0.0009456761621369191+0j)) <f>: (np.float32(0.006286362), np.complex128(0.005116137001985672+0j))\n",
      "Epoch 5200: <Test loss>: 0.0010570557788014412 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07222753), np.complex128(0.0009176867053499765+0j)) <f>: (np.float32(0.0063658585), np.complex128(0.005101656111843893+0j))\n",
      "Epoch 5600: <Test loss>: 0.0010371444514021277 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072099626), np.complex128(0.0008943274087795479+0j)) <f>: (np.float32(0.00649375), np.complex128(0.005146696719090076+0j))\n",
      "Epoch 6000: <Test loss>: 0.0010327541967853904 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07230726), np.complex128(0.0009264666044793733+0j)) <f>: (np.float32(0.0062860516), np.complex128(0.0050883184899324614+0j))\n",
      "Epoch 6400: <Test loss>: 0.0010077423648908734 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072359644), np.complex128(0.000923967893571801+0j)) <f>: (np.float32(0.006233773), np.complex128(0.005023309000573257+0j))\n",
      "Epoch 6800: <Test loss>: 0.0009813077049329877 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072255805), np.complex128(0.0008716872218366964+0j)) <f>: (np.float32(0.0063375225), np.complex128(0.005080706343139988+0j))\n",
      "Epoch 7200: <Test loss>: 0.0009664923418313265 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07233924), np.complex128(0.0008810359641468387+0j)) <f>: (np.float32(0.006254177), np.complex128(0.005046395610163977+0j))\n",
      "Epoch 7600: <Test loss>: 0.0009718131041154265 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07241657), np.complex128(0.0008885349512343115+0j)) <f>: (np.float32(0.006176845), np.complex128(0.005055576262105144+0j))\n",
      "Epoch 8000: <Test loss>: 0.0009451555088162422 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07229208), np.complex128(0.0008605502517219625+0j)) <f>: (np.float32(0.0063013085), np.complex128(0.005049749298461521+0j))\n",
      "Epoch 8400: <Test loss>: 0.000940584228374064 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07235277), np.complex128(0.0008728044202022707+0j)) <f>: (np.float32(0.0062405807), np.complex128(0.005052781775578502+0j))\n",
      "Epoch 8800: <Test loss>: 0.0009289099252782762 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07225363), np.complex128(0.000854723414938996+0j)) <f>: (np.float32(0.0063397465), np.complex128(0.0050665933489001174+0j))\n",
      "Epoch 9200: <Test loss>: 0.0009162062779068947 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072439075), np.complex128(0.0008270908806072333+0j)) <f>: (np.float32(0.0061542816), np.complex128(0.0050032279736404815+0j))\n",
      "Epoch 9600: <Test loss>: 0.0009064896148629487 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07226947), np.complex128(0.0008574229462647484+0j)) <f>: (np.float32(0.006323922), np.complex128(0.005093849107084063+0j))\n",
      "Epoch 10000: <Test loss>: 0.0009011983056552708 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07225484), np.complex128(0.0008466366190027309+0j)) <f>: (np.float32(0.0063385237), np.complex128(0.005088491020424391+0j))\n",
      "Epoch 10400: <Test loss>: 0.0008905195281840861 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07228879), np.complex128(0.0008528314785482627+0j)) <f>: (np.float32(0.006304552), np.complex128(0.005062271967519896+0j))\n",
      "Epoch 10800: <Test loss>: 0.0008823438547551632 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072333276), np.complex128(0.0008333156158372132+0j)) <f>: (np.float32(0.0062601333), np.complex128(0.005076554955038579+0j))\n",
      "Epoch 11200: <Test loss>: 0.0008664330816827714 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07222138), np.complex128(0.0008087625604954163+0j)) <f>: (np.float32(0.0063719777), np.complex128(0.005068020277556903+0j))\n",
      "Epoch 11600: <Test loss>: 0.0008630053489468992 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072393425), np.complex128(0.0008149275443565+0j)) <f>: (np.float32(0.0061999457), np.complex128(0.005084463955765697+0j))\n",
      "Epoch 12000: <Test loss>: 0.0008492719498462975 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072430216), np.complex128(0.0008004243901696225+0j)) <f>: (np.float32(0.0061631766), np.complex128(0.005057532960860679+0j))\n",
      "Epoch 12400: <Test loss>: 0.0008547281613573432 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07259669), np.complex128(0.0007742935688512574+0j)) <f>: (np.float32(0.005996716), np.complex128(0.00501159671738433+0j))\n",
      "Epoch 12800: <Test loss>: 0.0008359947823919356 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07247798), np.complex128(0.0007981429915654885+0j)) <f>: (np.float32(0.0061153946), np.complex128(0.005082165748124665+0j))\n",
      "Epoch 13200: <Test loss>: 0.0008288135286420584 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07245341), np.complex128(0.0007752995104216682+0j)) <f>: (np.float32(0.006139982), np.complex128(0.005080123291565789+0j))\n",
      "Epoch 13600: <Test loss>: 0.0008238608133979142 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07259208), np.complex128(0.000774035914859265+0j)) <f>: (np.float32(0.0060012825), np.complex128(0.005059830153616464+0j))\n",
      "Epoch 14000: <Test loss>: 0.0008072697673924267 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07253043), np.complex128(0.0007507219075429681+0j)) <f>: (np.float32(0.0060629323), np.complex128(0.005089357732425028+0j))\n",
      "Epoch 14400: <Test loss>: 0.0008085989393293858 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072551996), np.complex128(0.0007529459651306664+0j)) <f>: (np.float32(0.006041376), np.complex128(0.005078073223367562+0j))\n",
      "Epoch 14800: <Test loss>: 0.0008036154904402792 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07250726), np.complex128(0.0007465833323677984+0j)) <f>: (np.float32(0.006086145), np.complex128(0.0050988895346615335+0j))\n",
      "Epoch 15200: <Test loss>: 0.000800456153228879 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07266517), np.complex128(0.0007428751319675349+0j)) <f>: (np.float32(0.005928131), np.complex128(0.0050103347075801255+0j))\n",
      "Epoch 15600: <Test loss>: 0.0007837375160306692 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072608985), np.complex128(0.000720851043799726+0j)) <f>: (np.float32(0.0059844065), np.complex128(0.005081345213402751+0j))\n",
      "Epoch 16000: <Test loss>: 0.0007867621607147157 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07260735), np.complex128(0.0007300776827286311+0j)) <f>: (np.float32(0.0059859706), np.complex128(0.0050666648983100065+0j))\n",
      "Epoch 16400: <Test loss>: 0.0007833617855794728 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072608165), np.complex128(0.0007150961370086156+0j)) <f>: (np.float32(0.005985201), np.complex128(0.005094241867674515+0j))\n",
      "Epoch 16800: <Test loss>: 0.0007801696192473173 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07266152), np.complex128(0.0007170065316247723+0j)) <f>: (np.float32(0.005931848), np.complex128(0.005028411843593397+0j))\n",
      "Epoch 17200: <Test loss>: 0.0007666187593713403 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07263149), np.complex128(0.0007103964571527062+0j)) <f>: (np.float32(0.005961887), np.complex128(0.005086126337799705+0j))\n",
      "Epoch 17600: <Test loss>: 0.0007652724161744118 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07257997), np.complex128(0.0006854744275934253+0j)) <f>: (np.float32(0.0060133785), np.complex128(0.005137016236164935+0j))\n",
      "Epoch 18000: <Test loss>: 0.0007479118648916483 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072782025), np.complex128(0.000685606679827129+0j)) <f>: (np.float32(0.005811325), np.complex128(0.005161480552477979+0j))\n",
      "Epoch 18400: <Test loss>: 0.0007595655042678118 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07277215), np.complex128(0.0006842255478670986+0j)) <f>: (np.float32(0.0058212117), np.complex128(0.005091309864196952+0j))\n",
      "Epoch 18800: <Test loss>: 0.0007649860344827175 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07274894), np.complex128(0.0006895585161169155+0j)) <f>: (np.float32(0.005844395), np.complex128(0.005086140546193157+0j))\n",
      "Epoch 19200: <Test loss>: 0.0007615635986439884 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072794825), np.complex128(0.0006815161214101913+0j)) <f>: (np.float32(0.005798575), np.complex128(0.00507599017139882+0j))\n",
      "Epoch 19600: <Test loss>: 0.0007677491521462798 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072784446), np.complex128(0.0006752525033891992+0j)) <f>: (np.float32(0.0058089453), np.complex128(0.005059349098009553+0j))\n",
      "Epoch 20000: <Test loss>: 0.0007553418981842697 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07279846), np.complex128(0.0006656350702100072+0j)) <f>: (np.float32(0.005794957), np.complex128(0.005085526540618936+0j))\n",
      "Epoch 20400: <Test loss>: 0.0007602121331728995 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07286149), np.complex128(0.0006657047167100583+0j)) <f>: (np.float32(0.005731863), np.complex128(0.005102946538435004+0j))\n",
      "Epoch 20800: <Test loss>: 0.0007592491456307471 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07283388), np.complex128(0.0006598216806565588+0j)) <f>: (np.float32(0.0057594236), np.complex128(0.005082098765698387+0j))\n",
      "Epoch 21200: <Test loss>: 0.0007581881363876164 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072845675), np.complex128(0.0006522248840034404+0j)) <f>: (np.float32(0.0057476503), np.complex128(0.005123775028351935+0j))\n",
      "Epoch 21600: <Test loss>: 0.0007557549979537725 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072854385), np.complex128(0.0006641979292704275+0j)) <f>: (np.float32(0.005738946), np.complex128(0.0050769304625798385+0j))\n",
      "Epoch 22000: <Test loss>: 0.000763558957260102 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072956696), np.complex128(0.0006579977415773509+0j)) <f>: (np.float32(0.0056366404), np.complex128(0.0050620928402738624+0j))\n",
      "Epoch 22400: <Test loss>: 0.0007505308603867888 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07285868), np.complex128(0.0006419316641106375+0j)) <f>: (np.float32(0.0057346597), np.complex128(0.005096134121216888+0j))\n",
      "Epoch 22800: <Test loss>: 0.0007529440335929394 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07283435), np.complex128(0.0006299449813231485+0j)) <f>: (np.float32(0.0057589896), np.complex128(0.005116988998150232+0j))\n",
      "Epoch 23200: <Test loss>: 0.0007557052886113524 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07286658), np.complex128(0.0006332696185305074+0j)) <f>: (np.float32(0.0057268315), np.complex128(0.00509632847174162+0j))\n",
      "Epoch 23600: <Test loss>: 0.0007486557005904615 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072921716), np.complex128(0.00062912945759714+0j)) <f>: (np.float32(0.0056716544), np.complex128(0.005082354516780542+0j))\n",
      "Epoch 24000: <Test loss>: 0.0007535082404501736 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072924376), np.complex128(0.0006194702238318516+0j)) <f>: (np.float32(0.0056689074), np.complex128(0.005103100800992495+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82d8f6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d46d5dc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0022141034714877605 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071263686), np.complex128(0.0010880032885446915+0j)) <f>: (np.float32(0.0073295855), np.complex128(0.005308499366520343+0j))\n",
      "Epoch 1600: <Test loss>: 0.001996763749048114 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07100971), np.complex128(0.0010902355286446911+0j)) <f>: (np.float32(0.007583634), np.complex128(0.005197135500963172+0j))\n",
      "Epoch 2400: <Test loss>: 0.0018621872877702117 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07098099), np.complex128(0.001042630433240888+0j)) <f>: (np.float32(0.00761237), np.complex128(0.005039752678782051+0j))\n",
      "Epoch 3200: <Test loss>: 0.0018223427468910813 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0707782), np.complex128(0.0009981756685033321+0j)) <f>: (np.float32(0.007815219), np.complex128(0.005090227489081404+0j))\n",
      "Epoch 4000: <Test loss>: 0.001748555339872837 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07084681), np.complex128(0.0010295112019484691+0j)) <f>: (np.float32(0.00774652), np.complex128(0.004931773455522447+0j))\n",
      "Epoch 4800: <Test loss>: 0.0017019856022670865 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07063789), np.complex128(0.0009629976526537346+0j)) <f>: (np.float32(0.00795551), np.complex128(0.005033490836810241+0j))\n",
      "Epoch 5600: <Test loss>: 0.0016734146047383547 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07048225), np.complex128(0.0009707944451307703+0j)) <f>: (np.float32(0.008111123), np.complex128(0.005033936371433519+0j))\n",
      "Epoch 6400: <Test loss>: 0.0016557550989091396 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0703844), np.complex128(0.0009859547375148515+0j)) <f>: (np.float32(0.008208925), np.complex128(0.005031793441235224+0j))\n",
      "Epoch 7200: <Test loss>: 0.0016435529105365276 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070249245), np.complex128(0.001051713148755753+0j)) <f>: (np.float32(0.008344155), np.complex128(0.004999368872490107+0j))\n",
      "Epoch 8000: <Test loss>: 0.001639132620766759 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07027681), np.complex128(0.0010992168824955472+0j)) <f>: (np.float32(0.008316599), np.complex128(0.005002641877410543+0j))\n",
      "Epoch 8800: <Test loss>: 0.0016388889634981751 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070239276), np.complex128(0.0011263480635154664+0j)) <f>: (np.float32(0.008354106), np.complex128(0.004992494547271946+0j))\n",
      "Epoch 9600: <Test loss>: 0.0016421974869444966 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070173986), np.complex128(0.0011858658819444091+0j)) <f>: (np.float32(0.0084193945), np.complex128(0.004998838087506111+0j))\n",
      "Epoch 10400: <Test loss>: 0.0016400315798819065 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070264146), np.complex128(0.0012126759786444204+0j)) <f>: (np.float32(0.008329196), np.complex128(0.005007622426758462+0j))\n",
      "Epoch 11200: <Test loss>: 0.0016431775875389576 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07035519), np.complex128(0.0012549992306427065+0j)) <f>: (np.float32(0.00823821), np.complex128(0.005039610594847521+0j))\n",
      "Epoch 12000: <Test loss>: 0.0016357508720830083 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07033178), np.complex128(0.001240367122599177+0j)) <f>: (np.float32(0.008261512), np.complex128(0.004975320659128189+0j))\n",
      "Epoch 12800: <Test loss>: 0.0016383256297558546 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0704267), np.complex128(0.001285358634771553+0j)) <f>: (np.float32(0.008166732), np.complex128(0.004962535642233557+0j))\n",
      "Epoch 13600: <Test loss>: 0.0016502400394529104 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07040978), np.complex128(0.0012953746641313655+0j)) <f>: (np.float32(0.00818363), np.complex128(0.004982134598674175+0j))\n",
      "Epoch 14400: <Test loss>: 0.0016383143374696374 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07054395), np.complex128(0.0013282126717538353+0j)) <f>: (np.float32(0.008049489), np.complex128(0.004959847718657813+0j))\n",
      "Epoch 15200: <Test loss>: 0.0016358676366508007 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07057133), np.complex128(0.0013444598428067472+0j)) <f>: (np.float32(0.008022043), np.complex128(0.004945948357762363+0j))\n",
      "Epoch 16000: <Test loss>: 0.001732966280542314 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07157552), np.complex128(0.0014371489318010273+0j)) <f>: (np.float32(0.007017797), np.complex128(0.004540048555464359+0j))\n",
      "Epoch 16800: <Test loss>: 0.0016279509291052818 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07051648), np.complex128(0.0013535452223953847+0j)) <f>: (np.float32(0.008076886), np.complex128(0.004976151850145193+0j))\n",
      "Epoch 17600: <Test loss>: 0.0016369270160794258 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07064508), np.complex128(0.0013663044865768834+0j)) <f>: (np.float32(0.007948272), np.complex128(0.004928637967552932+0j))\n",
      "Epoch 18400: <Test loss>: 0.0016457042656838894 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07059053), np.complex128(0.0013411900094027073+0j)) <f>: (np.float32(0.008002775), np.complex128(0.004921754508367551+0j))\n",
      "Epoch 19200: <Test loss>: 0.0016504311934113503 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0705905), np.complex128(0.0013655704708222462+0j)) <f>: (np.float32(0.008002834), np.complex128(0.004927015166043544+0j))\n",
      "Epoch 20000: <Test loss>: 0.0016496744938194752 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07061813), np.complex128(0.0013771628706914128+0j)) <f>: (np.float32(0.007975172), np.complex128(0.004924239962336588+0j))\n",
      "Epoch 20800: <Test loss>: 0.001651350292377174 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07057939), np.complex128(0.0013823318081132405+0j)) <f>: (np.float32(0.008013989), np.complex128(0.0049432903733013945+0j))\n",
      "Epoch 21600: <Test loss>: 0.0016567489365115762 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07058893), np.complex128(0.0013820439612851603+0j)) <f>: (np.float32(0.008004444), np.complex128(0.004919317261447728+0j))\n",
      "Epoch 22400: <Test loss>: 0.0016592300962656736 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07060853), np.complex128(0.0013830261164826027+0j)) <f>: (np.float32(0.007984904), np.complex128(0.004923437695549114+0j))\n",
      "Epoch 23200: <Test loss>: 0.0016633630730211735 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070558436), np.complex128(0.001393907208654526+0j)) <f>: (np.float32(0.008034821), np.complex128(0.004934746561852499+0j))\n",
      "Epoch 24000: <Test loss>: 0.001687713898718357 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07061096), np.complex128(0.0013997204713473185+0j)) <f>: (np.float32(0.007982351), np.complex128(0.004909909782653936+0j))\n",
      "Epoch 24800: <Test loss>: 0.0016855786088854074 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07059344), np.complex128(0.0013875773693711902+0j)) <f>: (np.float32(0.007999867), np.complex128(0.004913288843082646+0j))\n",
      "Epoch 25600: <Test loss>: 0.0016948068514466286 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070621684), np.complex128(0.0013846537386969125+0j)) <f>: (np.float32(0.007971643), np.complex128(0.004912887963410221+0j))\n",
      "Epoch 26400: <Test loss>: 0.0017308812821283937 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07077397), np.complex128(0.0014275268047775694+0j)) <f>: (np.float32(0.00781942), np.complex128(0.004848161627034831+0j))\n",
      "Epoch 27200: <Test loss>: 0.001714729587547481 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07066245), np.complex128(0.0014080658727304947+0j)) <f>: (np.float32(0.00793093), np.complex128(0.004877769889220503+0j))\n",
      "Epoch 28000: <Test loss>: 0.0017093975329771638 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07074077), np.complex128(0.0013929926701866412+0j)) <f>: (np.float32(0.007852552), np.complex128(0.004917585867216949+0j))\n",
      "Epoch 28800: <Test loss>: 0.0017283683409914374 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07066639), np.complex128(0.0013905244692667964+0j)) <f>: (np.float32(0.007926958), np.complex128(0.004928127987716492+0j))\n",
      "Epoch 29600: <Test loss>: 0.001743796980008483 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07075625), np.complex128(0.0014050993631545463+0j)) <f>: (np.float32(0.007837053), np.complex128(0.004891187179623797+0j))\n",
      "Epoch 30400: <Test loss>: 0.0017609837232157588 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07067443), np.complex128(0.0013997746408473581+0j)) <f>: (np.float32(0.007918955), np.complex128(0.004897457140677581+0j))\n",
      "Epoch 31200: <Test loss>: 0.0017623240128159523 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07063998), np.complex128(0.0013879181170927516+0j)) <f>: (np.float32(0.007953363), np.complex128(0.004907198516717519+0j))\n",
      "Epoch 32000: <Test loss>: 0.0017734239809215069 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07065576), np.complex128(0.001385640968320588+0j)) <f>: (np.float32(0.007937577), np.complex128(0.0048923471934607145+0j))\n",
      "Epoch 32800: <Test loss>: 0.0017847748240455985 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0706629), np.complex128(0.0013937899894085383+0j)) <f>: (np.float32(0.007930447), np.complex128(0.004897692594054803+0j))\n",
      "Epoch 33600: <Test loss>: 0.0017973791109398007 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07068366), np.complex128(0.0013930210869735474+0j)) <f>: (np.float32(0.00790967), np.complex128(0.004879366303713479+0j))\n",
      "Epoch 34400: <Test loss>: 0.0018079362343996763 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07064501), np.complex128(0.0013892237669625629+0j)) <f>: (np.float32(0.007948428), np.complex128(0.004875268704530143+0j))\n",
      "Epoch 35200: <Test loss>: 0.0018234244780614972 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07061866), np.complex128(0.0013655717394288045+0j)) <f>: (np.float32(0.007974673), np.complex128(0.0048758832175469875+0j))\n",
      "Epoch 36000: <Test loss>: 0.0018350702011957765 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07065914), np.complex128(0.0013715364737446592+0j)) <f>: (np.float32(0.007934178), np.complex128(0.004855928543827419+0j))\n",
      "Epoch 36800: <Test loss>: 0.0018371021142229438 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07064299), np.complex128(0.0013707335726539057+0j)) <f>: (np.float32(0.007950379), np.complex128(0.004881380850928073+0j))\n",
      "Epoch 37600: <Test loss>: 0.0018632576102390885 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070651785), np.complex128(0.0013839397669258965+0j)) <f>: (np.float32(0.007941589), np.complex128(0.004850255335298663+0j))\n",
      "Epoch 38400: <Test loss>: 0.0018732011085376143 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07061091), np.complex128(0.0013773135811505398+0j)) <f>: (np.float32(0.007982485), np.complex128(0.00486184684714324+0j))\n",
      "Epoch 39200: <Test loss>: 0.0018990610260516405 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07058647), np.complex128(0.001385834557681386+0j)) <f>: (np.float32(0.008006878), np.complex128(0.00484912018615029+0j))\n",
      "Epoch 40000: <Test loss>: 0.0019082899671047926 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07060063), np.complex128(0.0013698841137024624+0j)) <f>: (np.float32(0.00799272), np.complex128(0.004871965760494931+0j))\n",
      "Epoch 40800: <Test loss>: 0.001906463410705328 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0706197), np.complex128(0.0013693185688987684+0j)) <f>: (np.float32(0.007973625), np.complex128(0.004826906377871685+0j))\n",
      "Epoch 41600: <Test loss>: 0.0019299801206216216 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07061657), np.complex128(0.001367609882725383+0j)) <f>: (np.float32(0.007976752), np.complex128(0.004849869678904938+0j))\n",
      "Epoch 42400: <Test loss>: 0.0019191442988812923 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070601694), np.complex128(0.0013449524427333384+0j)) <f>: (np.float32(0.007991645), np.complex128(0.004859938355436921+0j))\n",
      "Epoch 43200: <Test loss>: 0.0019557783380150795 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07063298), np.complex128(0.0013703811537520076+0j)) <f>: (np.float32(0.007960413), np.complex128(0.00483159159561283+0j))\n",
      "Epoch 44000: <Test loss>: 0.001967230811715126 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07058738), np.complex128(0.0013581214668331708+0j)) <f>: (np.float32(0.008005987), np.complex128(0.004850735883462951+0j))\n",
      "Epoch 44800: <Test loss>: 0.0019748015329241753 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07058371), np.complex128(0.001361677624737421+0j)) <f>: (np.float32(0.008009577), np.complex128(0.004835376102697576+0j))\n",
      "Epoch 45600: <Test loss>: 0.00199565920047462 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07061165), np.complex128(0.0013690591388575944+0j)) <f>: (np.float32(0.0079817055), np.complex128(0.004853393360481295+0j))\n",
      "Epoch 46400: <Test loss>: 0.002007886068895459 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07056972), np.complex128(0.0013527480300341438+0j)) <f>: (np.float32(0.008023658), np.complex128(0.004848788318674636+0j))\n",
      "Epoch 47200: <Test loss>: 0.0020083433482795954 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07063976), np.complex128(0.0013582695132185252+0j)) <f>: (np.float32(0.007953562), np.complex128(0.004803116452805039+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e921fa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40016d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.002859971486032009 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07241719), np.complex128(0.0011498931475163593+0j)) <f>: (np.float32(0.0061761583), np.complex128(0.005264426452356838+0j))\n",
      "Epoch 3200: <Test loss>: 0.0026999078691005707 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072157726), np.complex128(0.0011421948623385762+0j)) <f>: (np.float32(0.0064356313), np.complex128(0.005149635826764364+0j))\n",
      "Epoch 4800: <Test loss>: 0.002670699032023549 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07196334), np.complex128(0.0011725673331149735+0j)) <f>: (np.float32(0.0066300645), np.complex128(0.005163837623463316+0j))\n",
      "Epoch 6400: <Test loss>: 0.0026127349119633436 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07181169), np.complex128(0.0011698693875474189+0j)) <f>: (np.float32(0.006781666), np.complex128(0.005240539605749137+0j))\n",
      "Epoch 8000: <Test loss>: 0.0025308900512754917 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07189676), np.complex128(0.0011789376409475194+0j)) <f>: (np.float32(0.0066966005), np.complex128(0.005206029955264784+0j))\n",
      "Epoch 9600: <Test loss>: 0.002456829184666276 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07188891), np.complex128(0.0011558961937502747+0j)) <f>: (np.float32(0.006704474), np.complex128(0.00523033594947935+0j))\n",
      "Epoch 11200: <Test loss>: 0.002452331129461527 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07200191), np.complex128(0.0010962342616163082+0j)) <f>: (np.float32(0.006591451), np.complex128(0.005220341867012997+0j))\n",
      "Epoch 12800: <Test loss>: 0.002455935114994645 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0718543), np.complex128(0.0010895605030950151+0j)) <f>: (np.float32(0.0067390823), np.complex128(0.00519163837502471+0j))\n",
      "Epoch 14400: <Test loss>: 0.0024980048183351755 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07175388), np.complex128(0.0010606089350942514+0j)) <f>: (np.float32(0.006839442), np.complex128(0.005221804824096038+0j))\n",
      "Epoch 16000: <Test loss>: 0.0025094819720834494 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07163543), np.complex128(0.001047094786580231+0j)) <f>: (np.float32(0.0069579035), np.complex128(0.005186954172168812+0j))\n",
      "Epoch 17600: <Test loss>: 0.0025344821624457836 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071564905), np.complex128(0.0010689612503831704+0j)) <f>: (np.float32(0.0070284833), np.complex128(0.005171856739239695+0j))\n",
      "Epoch 19200: <Test loss>: 0.0025428428780287504 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07135829), np.complex128(0.0010618170925500562+0j)) <f>: (np.float32(0.0072350632), np.complex128(0.005161713976084707+0j))\n",
      "Epoch 20800: <Test loss>: 0.002550231758505106 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0713424), np.complex128(0.0010740930176328396+0j)) <f>: (np.float32(0.0072509283), np.complex128(0.005127534670748138+0j))\n",
      "Epoch 22400: <Test loss>: 0.002562577836215496 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07123089), np.complex128(0.001103967306613789+0j)) <f>: (np.float32(0.00736248), np.complex128(0.005140523679577347+0j))\n",
      "Epoch 24000: <Test loss>: 0.0025919321924448013 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0711272), np.complex128(0.0010976252887074935+0j)) <f>: (np.float32(0.0074661747), np.complex128(0.00512708152448551+0j))\n",
      "Epoch 25600: <Test loss>: 0.002602715278044343 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0711944), np.complex128(0.0011257544825068338+0j)) <f>: (np.float32(0.007398975), np.complex128(0.0050737386484791345+0j))\n",
      "Epoch 27200: <Test loss>: 0.0026228968054056168 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07123701), np.complex128(0.0011299270563377662+0j)) <f>: (np.float32(0.007356298), np.complex128(0.005024601964377485+0j))\n",
      "Epoch 28800: <Test loss>: 0.002668284811079502 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071265295), np.complex128(0.0011580530786207109+0j)) <f>: (np.float32(0.0073280213), np.complex128(0.0050147900538129045+0j))\n",
      "Epoch 30400: <Test loss>: 0.002687908476218581 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07114317), np.complex128(0.0011766531342573174+0j)) <f>: (np.float32(0.0074502123), np.complex128(0.00499783030645619+0j))\n",
      "Epoch 32000: <Test loss>: 0.002710087923333049 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071127795), np.complex128(0.0011987417310686164+0j)) <f>: (np.float32(0.007465611), np.complex128(0.004974944136701683+0j))\n",
      "Epoch 33600: <Test loss>: 0.002745915437117219 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07112138), np.complex128(0.001219952452141566+0j)) <f>: (np.float32(0.0074719163), np.complex128(0.004959745215247902+0j))\n",
      "Epoch 35200: <Test loss>: 0.0027748795691877604 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07104025), np.complex128(0.0012419011216494836+0j)) <f>: (np.float32(0.0075531234), np.complex128(0.004937607523362796+0j))\n",
      "Epoch 36800: <Test loss>: 0.0027789785526692867 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070993684), np.complex128(0.001256003713315575+0j)) <f>: (np.float32(0.007599731), np.complex128(0.00493739135280526+0j))\n",
      "Epoch 38400: <Test loss>: 0.002809894969686866 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07092897), np.complex128(0.0012853472173125283+0j)) <f>: (np.float32(0.007664331), np.complex128(0.004915446489117017+0j))\n",
      "Epoch 40000: <Test loss>: 0.00284717814065516 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0709958), np.complex128(0.0013114689680940013+0j)) <f>: (np.float32(0.007597624), np.complex128(0.004881664003911887+0j))\n",
      "Epoch 41600: <Test loss>: 0.0028584396932274103 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070902444), np.complex128(0.0013142535594894884+0j)) <f>: (np.float32(0.00769089), np.complex128(0.0048934518960516895+0j))\n",
      "Epoch 43200: <Test loss>: 0.002886031987145543 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07093734), np.complex128(0.0013346121575372222+0j)) <f>: (np.float32(0.007655985), np.complex128(0.004882646920273265+0j))\n",
      "Epoch 44800: <Test loss>: 0.002901555271819234 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07087784), np.complex128(0.0013318920113548972+0j)) <f>: (np.float32(0.00771545), np.complex128(0.004898914515891766+0j))\n",
      "Epoch 46400: <Test loss>: 0.0029114161152392626 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07090659), np.complex128(0.001355369605486888+0j)) <f>: (np.float32(0.0076868157), np.complex128(0.0048672637971472164+0j))\n",
      "Epoch 48000: <Test loss>: 0.00294013530947268 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07087485), np.complex128(0.00136579552162569+0j)) <f>: (np.float32(0.007718467), np.complex128(0.004866139304293932+0j))\n",
      "Epoch 49600: <Test loss>: 0.0029727632645517588 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0709288), np.complex128(0.001344171995978667+0j)) <f>: (np.float32(0.0076645925), np.complex128(0.004797635565030523+0j))\n",
      "Epoch 51200: <Test loss>: 0.002976935589686036 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070906304), np.complex128(0.0013816779682930885+0j)) <f>: (np.float32(0.007687069), np.complex128(0.00484486781696684+0j))\n",
      "Epoch 52800: <Test loss>: 0.0030005304142832756 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07088943), np.complex128(0.0013986292159858615+0j)) <f>: (np.float32(0.0077039134), np.complex128(0.004842675664834082+0j))\n",
      "Epoch 54400: <Test loss>: 0.003019181778654456 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07090247), np.complex128(0.0014043104167359341+0j)) <f>: (np.float32(0.0076909033), np.complex128(0.004844790178245471+0j))\n",
      "Epoch 56000: <Test loss>: 0.0030431949999183416 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07087466), np.complex128(0.0014286043591881965+0j)) <f>: (np.float32(0.0077186124), np.complex128(0.00482477714862422+0j))\n",
      "Epoch 57600: <Test loss>: 0.0030511589720845222 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07088787), np.complex128(0.0014414016815664445+0j)) <f>: (np.float32(0.007705505), np.complex128(0.004789679372139436+0j))\n",
      "Epoch 59200: <Test loss>: 0.003092758357524872 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070966616), np.complex128(0.0014560476174214595+0j)) <f>: (np.float32(0.0076267067), np.complex128(0.004784674972988221+0j))\n",
      "Epoch 60800: <Test loss>: 0.003093189327046275 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07092741), np.complex128(0.0014527468300173968+0j)) <f>: (np.float32(0.0076659103), np.complex128(0.0047937023772571434+0j))\n",
      "Epoch 62400: <Test loss>: 0.0031208416912704706 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.070969485), np.complex128(0.0014866856075505107+0j)) <f>: (np.float32(0.0076238993), np.complex128(0.004754396379097138+0j))\n",
      "Epoch 64000: <Test loss>: 0.003132220823317766 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07096831), np.complex128(0.0014802254823736366+0j)) <f>: (np.float32(0.0076250825), np.complex128(0.004785898924595677+0j))\n",
      "Epoch 65600: <Test loss>: 0.003174002282321453 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0709896), np.complex128(0.0014935109645555218+0j)) <f>: (np.float32(0.0076037813), np.complex128(0.004781036101936369+0j))\n",
      "Epoch 67200: <Test loss>: 0.003180381841957569 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07106292), np.complex128(0.0015014958279548267+0j)) <f>: (np.float32(0.007530485), np.complex128(0.004757075676148286+0j))\n",
      "Epoch 68800: <Test loss>: 0.003197717247530818 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07109097), np.complex128(0.0014969813646564304+0j)) <f>: (np.float32(0.007502419), np.complex128(0.004770795909797703+0j))\n",
      "Epoch 70400: <Test loss>: 0.0032087829895317554 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071067475), np.complex128(0.0015024807740866974+0j)) <f>: (np.float32(0.0075258673), np.complex128(0.004769978927174153+0j))\n",
      "Epoch 72000: <Test loss>: 0.0032217458356171846 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071093), np.complex128(0.0015133575529963222+0j)) <f>: (np.float32(0.007500373), np.complex128(0.004764820265465447+0j))\n",
      "Epoch 73600: <Test loss>: 0.003224767278879881 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07108057), np.complex128(0.0015181145738686673+0j)) <f>: (np.float32(0.0075128083), np.complex128(0.0047668977355653334+0j))\n",
      "Epoch 75200: <Test loss>: 0.003252240363508463 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07112391), np.complex128(0.0015343021204133386+0j)) <f>: (np.float32(0.0074694646), np.complex128(0.004727107129700054+0j))\n",
      "Epoch 76800: <Test loss>: 0.003278722520917654 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07115493), np.complex128(0.0015378273243175661+0j)) <f>: (np.float32(0.007438426), np.complex128(0.004719723332088076+0j))\n",
      "Epoch 78400: <Test loss>: 0.0033251207787543535 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07123657), np.complex128(0.0015554090694698186+0j)) <f>: (np.float32(0.0073567755), np.complex128(0.0047191783387106265+0j))\n",
      "Epoch 80000: <Test loss>: 0.0033133949618786573 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07115469), np.complex128(0.0015519474227541623+0j)) <f>: (np.float32(0.0074385996), np.complex128(0.004731635040227968+0j))\n",
      "Epoch 81600: <Test loss>: 0.0033293175511062145 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0711694), np.complex128(0.0015585266700868616+0j)) <f>: (np.float32(0.0074240146), np.complex128(0.004728987204619467+0j))\n",
      "Epoch 83200: <Test loss>: 0.003394546452909708 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07117985), np.complex128(0.0015779594390683419+0j)) <f>: (np.float32(0.0074135037), np.complex128(0.004701387400336909+0j))\n",
      "Epoch 84800: <Test loss>: 0.0033655406441539526 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.071145386), np.complex128(0.0015724547014905298+0j)) <f>: (np.float32(0.0074480036), np.complex128(0.004716756822512127+0j))\n",
      "Epoch 86400: <Test loss>: 0.0033883010037243366 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07120646), np.complex128(0.001590327591847914+0j)) <f>: (np.float32(0.00738687), np.complex128(0.00471067715244209+0j))\n",
      "Epoch 88000: <Test loss>: 0.003400889690965414 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07118411), np.complex128(0.001589681871109735+0j)) <f>: (np.float32(0.007409238), np.complex128(0.004727356284028106+0j))\n",
      "Epoch 89600: <Test loss>: 0.0034020005259662867 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07125095), np.complex128(0.0015841841108679938+0j)) <f>: (np.float32(0.007342447), np.complex128(0.00474342293236777+0j))\n",
      "Epoch 91200: <Test loss>: 0.0034291010815650225 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07123158), np.complex128(0.0016126539255282401+0j)) <f>: (np.float32(0.0073617212), np.complex128(0.004721991600614331+0j))\n",
      "Epoch 92800: <Test loss>: 0.0035108791198581457 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07127913), np.complex128(0.001631954759427656+0j)) <f>: (np.float32(0.007314238), np.complex128(0.004665641619622141+0j))\n",
      "Epoch 94400: <Test loss>: 0.0034730336628854275 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07124965), np.complex128(0.0016245055017172691+0j)) <f>: (np.float32(0.0073436825), np.complex128(0.004715041159002671+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb93688d",
   "metadata": {},
   "source": [
    "### l4, w64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95547ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea67b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.0016355796251446009 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07389028), np.complex128(0.0011700768047197023+0j)) <f>: (np.float32(0.004703042), np.complex128(0.005165851663235286+0j))\n",
      "Epoch 200: <Test loss>: 0.0011793015291914344 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.074036084), np.complex128(0.0011391896602625995+0j)) <f>: (np.float32(0.0045572543), np.complex128(0.004906514991503903+0j))\n",
      "Epoch 300: <Test loss>: 0.0010494478046894073 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07423557), np.complex128(0.000983772797664889+0j)) <f>: (np.float32(0.0043578064), np.complex128(0.0049304581642427925+0j))\n",
      "Epoch 400: <Test loss>: 0.0009221425862051547 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073878005), np.complex128(0.0009992024786516268+0j)) <f>: (np.float32(0.004715381), np.complex128(0.004886958660243643+0j))\n",
      "Epoch 500: <Test loss>: 0.0008396263583563268 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07385011), np.complex128(0.0010154490788315876+0j)) <f>: (np.float32(0.004743271), np.complex128(0.00481712897340549+0j))\n",
      "Epoch 600: <Test loss>: 0.0007423720671795309 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07402712), np.complex128(0.0008981005618355967+0j)) <f>: (np.float32(0.004566228), np.complex128(0.004834218118631152+0j))\n",
      "Epoch 700: <Test loss>: 0.0007000879268161952 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07394319), np.complex128(0.0009135106428510088+0j)) <f>: (np.float32(0.0046501714), np.complex128(0.004813381509632247+0j))\n",
      "Epoch 800: <Test loss>: 0.0006433062953874469 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07379759), np.complex128(0.0008565838264567553+0j)) <f>: (np.float32(0.004795841), np.complex128(0.004864906726161879+0j))\n",
      "Epoch 900: <Test loss>: 0.0005995909450575709 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07405019), np.complex128(0.0008971524052939169+0j)) <f>: (np.float32(0.004543203), np.complex128(0.00478657128607158+0j))\n",
      "Epoch 1000: <Test loss>: 0.0005733544821850955 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07363228), np.complex128(0.0008661800104760957+0j)) <f>: (np.float32(0.004961121), np.complex128(0.00478914402017183+0j))\n",
      "Epoch 1100: <Test loss>: 0.0005338561022654176 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073820904), np.complex128(0.000865413264672254+0j)) <f>: (np.float32(0.0047724517), np.complex128(0.004759318572543375+0j))\n",
      "Epoch 1200: <Test loss>: 0.0005092693609185517 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07362447), np.complex128(0.0008537594008153374+0j)) <f>: (np.float32(0.0049688923), np.complex128(0.004870720496297295+0j))\n",
      "Epoch 1300: <Test loss>: 0.0005015498609282076 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07376505), np.complex128(0.0008497844124558617+0j)) <f>: (np.float32(0.0048283157), np.complex128(0.004754203550900275+0j))\n",
      "Epoch 1400: <Test loss>: 0.00048518451512791216 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073391765), np.complex128(0.0008181302050432775+0j)) <f>: (np.float32(0.0052015637), np.complex128(0.004784852070463761+0j))\n",
      "Epoch 1500: <Test loss>: 0.00048147595953196287 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073611535), np.complex128(0.0008599724648649809+0j)) <f>: (np.float32(0.004981849), np.complex128(0.0046742371902186155+0j))\n",
      "Epoch 1600: <Test loss>: 0.0004483393859118223 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07351952), np.complex128(0.000820775693729646+0j)) <f>: (np.float32(0.005073864), np.complex128(0.004784350209709294+0j))\n",
      "Epoch 1700: <Test loss>: 0.00045142212184146047 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07358774), np.complex128(0.0008226992184236811+0j)) <f>: (np.float32(0.0050056763), np.complex128(0.004725212338944565+0j))\n",
      "Epoch 1800: <Test loss>: 0.0004320755251683295 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073445775), np.complex128(0.0007784518708584088+0j)) <f>: (np.float32(0.0051476373), np.complex128(0.004790656706631957+0j))\n",
      "Epoch 1900: <Test loss>: 0.00043466765782795846 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07324322), np.complex128(0.0008009723647724839+0j)) <f>: (np.float32(0.005350179), np.complex128(0.004793497877879945+0j))\n",
      "Epoch 2000: <Test loss>: 0.00041968628647737205 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07347914), np.complex128(0.0008209448624141965+0j)) <f>: (np.float32(0.005114182), np.complex128(0.004762903147234532+0j))\n",
      "Epoch 2100: <Test loss>: 0.00041780975880101323 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07335447), np.complex128(0.0007895602338952528+0j)) <f>: (np.float32(0.0052388604), np.complex128(0.004820443588621039+0j))\n",
      "Epoch 2200: <Test loss>: 0.000417792412918061 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073356904), np.complex128(0.0007860978260156616+0j)) <f>: (np.float32(0.005236451), np.complex128(0.004782382854658669+0j))\n",
      "Epoch 2300: <Test loss>: 0.0004131173191126436 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07326379), np.complex128(0.0007798913587201213+0j)) <f>: (np.float32(0.005329569), np.complex128(0.004798797608637934+0j))\n",
      "Epoch 2400: <Test loss>: 0.00040297716623172164 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07320482), np.complex128(0.0007655172852505519+0j)) <f>: (np.float32(0.0053885784), np.complex128(0.004829616628922856+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80695570",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c90ff07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.0026672708336263895 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07328862), np.complex128(0.001353078502042583+0j)) <f>: (np.float32(0.0053047216), np.complex128(0.005056632757646903+0j))\n",
      "Epoch 400: <Test loss>: 0.002237942535430193 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07372072), np.complex128(0.0013599363353754869+0j)) <f>: (np.float32(0.004872639), np.complex128(0.00546050532666473+0j))\n",
      "Epoch 600: <Test loss>: 0.0019440618343651295 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07336547), np.complex128(0.0012752370573460893+0j)) <f>: (np.float32(0.0052279206), np.complex128(0.005366777629481163+0j))\n",
      "Epoch 800: <Test loss>: 0.0016323286108672619 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07387751), np.complex128(0.0012436058751425384+0j)) <f>: (np.float32(0.004715899), np.complex128(0.005141791778693032+0j))\n",
      "Epoch 1000: <Test loss>: 0.0014604110037907958 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07374505), np.complex128(0.0012624979640088675+0j)) <f>: (np.float32(0.004848396), np.complex128(0.005035049193106466+0j))\n",
      "Epoch 1200: <Test loss>: 0.0013332191156223416 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07358379), np.complex128(0.0012381678662696934+0j)) <f>: (np.float32(0.0050096205), np.complex128(0.004941438715168887+0j))\n",
      "Epoch 1400: <Test loss>: 0.001223520957864821 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07314729), np.complex128(0.0011606968546882257+0j)) <f>: (np.float32(0.0054459916), np.complex128(0.005019017558307811+0j))\n",
      "Epoch 1600: <Test loss>: 0.0011426180135458708 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07305433), np.complex128(0.0010992720668808336+0j)) <f>: (np.float32(0.005539036), np.complex128(0.0049336819472287664+0j))\n",
      "Epoch 1800: <Test loss>: 0.0010790553642436862 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07303562), np.complex128(0.0010681636774399618+0j)) <f>: (np.float32(0.005557783), np.complex128(0.004975440415587293+0j))\n",
      "Epoch 2600: <Test loss>: 0.0009056740091182292 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0725744), np.complex128(0.0009603401756353901+0j)) <f>: (np.float32(0.0060189525), np.complex128(0.005014061873648435+0j))\n",
      "Epoch 2800: <Test loss>: 0.0008988483459688723 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072744824), np.complex128(0.0009919385009290807+0j)) <f>: (np.float32(0.0058486015), np.complex128(0.0048576771911073915+0j))\n",
      "Epoch 3000: <Test loss>: 0.0008714207215234637 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07267129), np.complex128(0.0009412998500927226+0j)) <f>: (np.float32(0.005922093), np.complex128(0.004949590273469954+0j))\n",
      "Epoch 3200: <Test loss>: 0.0008591327932663262 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072679624), np.complex128(0.0009481234944788799+0j)) <f>: (np.float32(0.0059137144), np.complex128(0.004925652682599921+0j))\n",
      "Epoch 3400: <Test loss>: 0.000835644081234932 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0726397), np.complex128(0.0009479825522902518+0j)) <f>: (np.float32(0.005953604), np.complex128(0.004965682801383408+0j))\n",
      "Epoch 3600: <Test loss>: 0.0008368022972717881 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07256042), np.complex128(0.0009650451202085165+0j)) <f>: (np.float32(0.006032956), np.complex128(0.0048955258140532115+0j))\n",
      "Epoch 3800: <Test loss>: 0.0008255702559836209 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07254047), np.complex128(0.0009646240696918139+0j)) <f>: (np.float32(0.0060529173), np.complex128(0.004933093313785711+0j))\n",
      "Epoch 4000: <Test loss>: 0.0008196252747438848 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072534), np.complex128(0.0009792975343091443+0j)) <f>: (np.float32(0.0060593686), np.complex128(0.004893353452182765+0j))\n",
      "Epoch 4200: <Test loss>: 0.0008171171066351235 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07250847), np.complex128(0.000989497955632207+0j)) <f>: (np.float32(0.00608487), np.complex128(0.004895556768053234+0j))\n",
      "Epoch 4400: <Test loss>: 0.0008048440795391798 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07253996), np.complex128(0.0009866419416874872+0j)) <f>: (np.float32(0.0060534263), np.complex128(0.0048986744955309336+0j))\n",
      "Epoch 4600: <Test loss>: 0.0008452592301182449 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07263806), np.complex128(0.0009995198840125157+0j)) <f>: (np.float32(0.005955317), np.complex128(0.004749579733716553+0j))\n",
      "Epoch 4800: <Test loss>: 0.0007908176048658788 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07257727), np.complex128(0.0009855909011539288+0j)) <f>: (np.float32(0.0060160905), np.complex128(0.004912630690000196+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b1a552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a655634d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.0034662124235183 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07289432), np.complex128(0.001836855904323992+0j)) <f>: (np.float32(0.005699052), np.complex128(0.005186323420988021+0j))\n",
      "Epoch 800: <Test loss>: 0.0027653377037495375 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07335595), np.complex128(0.001514909312538445+0j)) <f>: (np.float32(0.0052374047), np.complex128(0.005214449443270965+0j))\n",
      "Epoch 1200: <Test loss>: 0.0023438462521880865 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07333531), np.complex128(0.0013145882178995701+0j)) <f>: (np.float32(0.0052580494), np.complex128(0.005267813124424898+0j))\n",
      "Epoch 1600: <Test loss>: 0.0021326655987650156 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07351708), np.complex128(0.0012282825033853866+0j)) <f>: (np.float32(0.0050762487), np.complex128(0.005238826479452797+0j))\n",
      "Epoch 2000: <Test loss>: 0.001986957620829344 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07347911), np.complex128(0.0011814243635231154+0j)) <f>: (np.float32(0.0051142345), np.complex128(0.005305479575468946+0j))\n",
      "Epoch 2400: <Test loss>: 0.0018625478260219097 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073349856), np.complex128(0.001170185778023061+0j)) <f>: (np.float32(0.0052435505), np.complex128(0.005199808201260216+0j))\n",
      "Epoch 2800: <Test loss>: 0.0017711988184601068 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07332771), np.complex128(0.001074704739715256+0j)) <f>: (np.float32(0.005265682), np.complex128(0.005190973117745533+0j))\n",
      "Epoch 3200: <Test loss>: 0.0016694780206307769 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073382586), np.complex128(0.0010749946163138293+0j)) <f>: (np.float32(0.005210803), np.complex128(0.005272121312296913+0j))\n",
      "Epoch 3600: <Test loss>: 0.0015842554857954383 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07304935), np.complex128(0.0009865644932571024+0j)) <f>: (np.float32(0.0055440203), np.complex128(0.005235742243188238+0j))\n",
      "Epoch 4000: <Test loss>: 0.0016233932692557573 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07319491), np.complex128(0.0009590535548639537+0j)) <f>: (np.float32(0.0053984732), np.complex128(0.0050965542837089995+0j))\n",
      "Epoch 4400: <Test loss>: 0.0015563591150566936 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073331475), np.complex128(0.000996672179440753+0j)) <f>: (np.float32(0.0052618724), np.complex128(0.005097150528791405+0j))\n",
      "Epoch 4800: <Test loss>: 0.0015154242282733321 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07305564), np.complex128(0.0009397461876407622+0j)) <f>: (np.float32(0.0055376524), np.complex128(0.005112339808835343+0j))\n",
      "Epoch 5200: <Test loss>: 0.00150910927914083 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0729103), np.complex128(0.0009305942062078135+0j)) <f>: (np.float32(0.0056829983), np.complex128(0.005096073228102089+0j))\n",
      "Epoch 5600: <Test loss>: 0.001531059155240655 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073086284), np.complex128(0.0009256532373845138+0j)) <f>: (np.float32(0.005507114), np.complex128(0.005012196007122475+0j))\n",
      "Epoch 6000: <Test loss>: 0.0014725540531799197 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.072965845), np.complex128(0.0009009384981368605+0j)) <f>: (np.float32(0.0056274757), np.complex128(0.005047804778328947+0j))\n",
      "Epoch 6400: <Test loss>: 0.0014632133534178138 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073022544), np.complex128(0.000892355613605969+0j)) <f>: (np.float32(0.005570786), np.complex128(0.005069560365918689+0j))\n",
      "Epoch 6800: <Test loss>: 0.0014558759285137057 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07313587), np.complex128(0.0009020838595680293+0j)) <f>: (np.float32(0.005457517), np.complex128(0.0050284955716262455+0j))\n",
      "Epoch 7200: <Test loss>: 0.0013847507070749998 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07291805), np.complex128(0.0009040574307907898+0j)) <f>: (np.float32(0.0056753377), np.complex128(0.00512149610353059+0j))\n",
      "Epoch 7600: <Test loss>: 0.0014044368872419 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07296087), np.complex128(0.0008935337685166701+0j)) <f>: (np.float32(0.005632554), np.complex128(0.005067359079818713+0j))\n",
      "Epoch 8000: <Test loss>: 0.0014210703084245324 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07304532), np.complex128(0.0008917310151669858+0j)) <f>: (np.float32(0.0055480143), np.complex128(0.005018993201061892+0j))\n",
      "Epoch 8400: <Test loss>: 0.0013887463137507439 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07293768), np.complex128(0.000876928216111036+0j)) <f>: (np.float32(0.0056556617), np.complex128(0.005064161683849152+0j))\n",
      "Epoch 8800: <Test loss>: 0.0014217208372429013 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07305239), np.complex128(0.0008838156079767483+0j)) <f>: (np.float32(0.0055409702), np.complex128(0.004992492010058829+0j))\n",
      "Epoch 9200: <Test loss>: 0.001399421482346952 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073020875), np.complex128(0.0008517603306007549+0j)) <f>: (np.float32(0.0055725053), np.complex128(0.0050274040625434765+0j))\n",
      "Epoch 9600: <Test loss>: 0.0014147013425827026 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07307191), np.complex128(0.0008758063873315239+0j)) <f>: (np.float32(0.005521476), np.complex128(0.004990253680647349+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e0bacb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad79c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.004191759508103132 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07473144), np.complex128(0.002104332970618729+0j)) <f>: (np.float32(0.0038619256), np.complex128(0.005347277624352092+0j))\n",
      "Epoch 1600: <Test loss>: 0.0032691163942217827 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07526188), np.complex128(0.0018950040082519291+0j)) <f>: (np.float32(0.0033315192), np.complex128(0.005012199559220838+0j))\n",
      "Epoch 2400: <Test loss>: 0.0028734940569847822 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07484069), np.complex128(0.0017150890844067246+0j)) <f>: (np.float32(0.0037525923), np.complex128(0.004977714773425029+0j))\n",
      "Epoch 3200: <Test loss>: 0.0026806278619915247 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07470998), np.complex128(0.0016585008591028835+0j)) <f>: (np.float32(0.003883346), np.complex128(0.004931386784243475+0j))\n",
      "Epoch 4000: <Test loss>: 0.002568507334217429 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07475223), np.complex128(0.0015828439548997968+0j)) <f>: (np.float32(0.0038411918), np.complex128(0.0048918940471980865+0j))\n",
      "Epoch 4800: <Test loss>: 0.0024942054878920317 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0746315), np.complex128(0.0014842588900651225+0j)) <f>: (np.float32(0.0039618835), np.complex128(0.004850797284020373+0j))\n",
      "Epoch 5600: <Test loss>: 0.002479494083672762 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07447415), np.complex128(0.0015049120585556954+0j)) <f>: (np.float32(0.0041192584), np.complex128(0.004785618816267602+0j))\n",
      "Epoch 6400: <Test loss>: 0.002486759563907981 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07420888), np.complex128(0.0014894553562492655+0j)) <f>: (np.float32(0.0043845093), np.complex128(0.004720707770777323+0j))\n",
      "Epoch 7200: <Test loss>: 0.0024728202261030674 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07407363), np.complex128(0.0014624276935245044+0j)) <f>: (np.float32(0.004519782), np.complex128(0.004755653821917734+0j))\n",
      "Epoch 8000: <Test loss>: 0.00247460906393826 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07392706), np.complex128(0.0014993738297647441+0j)) <f>: (np.float32(0.004666351), np.complex128(0.004632546719171613+0j))\n",
      "Epoch 8800: <Test loss>: 0.0024575917050242424 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07378786), np.complex128(0.0015020125314060258+0j)) <f>: (np.float32(0.004805473), np.complex128(0.0046076713743336785+0j))\n",
      "Epoch 9600: <Test loss>: 0.002486861776560545 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073702484), np.complex128(0.0015292143738112427+0j)) <f>: (np.float32(0.004890853), np.complex128(0.004544821053336715+0j))\n",
      "Epoch 10400: <Test loss>: 0.002507179742679 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07361166), np.complex128(0.0015266184242109762+0j)) <f>: (np.float32(0.0049816994), np.complex128(0.004486639204474327+0j))\n",
      "Epoch 11200: <Test loss>: 0.002497325651347637 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07360713), np.complex128(0.0015313221636078722+0j)) <f>: (np.float32(0.004986242), np.complex128(0.0044993374486803715+0j))\n",
      "Epoch 12000: <Test loss>: 0.0025381699670106173 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07356705), np.complex128(0.0015438816222564373+0j)) <f>: (np.float32(0.0050263247), np.complex128(0.004473914065809247+0j))\n",
      "Epoch 12800: <Test loss>: 0.0025567796546965837 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073492326), np.complex128(0.0015565618522493535+0j)) <f>: (np.float32(0.0051010787), np.complex128(0.004455259460090633+0j))\n",
      "Epoch 13600: <Test loss>: 0.002597620477899909 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0733848), np.complex128(0.0015609225604328828+0j)) <f>: (np.float32(0.0052085756), np.complex128(0.004457501849043099+0j))\n",
      "Epoch 14400: <Test loss>: 0.00261120218783617 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07341371), np.complex128(0.0015730565284417914+0j)) <f>: (np.float32(0.005179641), np.complex128(0.004417431134849745+0j))\n",
      "Epoch 15200: <Test loss>: 0.0026368284597992897 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07340836), np.complex128(0.001608468285049757+0j)) <f>: (np.float32(0.005185015), np.complex128(0.004392210221585327+0j))\n",
      "Epoch 16000: <Test loss>: 0.002672201720997691 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073352605), np.complex128(0.0016016806056595278+0j)) <f>: (np.float32(0.0052407472), np.complex128(0.00437861684859174+0j))\n",
      "Epoch 16800: <Test loss>: 0.0027387363370507956 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0732805), np.complex128(0.0016449076129699527+0j)) <f>: (np.float32(0.005312894), np.complex128(0.004357258081133437+0j))\n",
      "Epoch 17600: <Test loss>: 0.0027261448558419943 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07324944), np.complex128(0.0016188988950328249+0j)) <f>: (np.float32(0.0053439466), np.complex128(0.00435095259909602+0j))\n",
      "Epoch 18400: <Test loss>: 0.0027405654545873404 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073264755), np.complex128(0.0016275962078752758+0j)) <f>: (np.float32(0.005328595), np.complex128(0.0043490030045372115+0j))\n",
      "Epoch 19200: <Test loss>: 0.0027898296248167753 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073316365), np.complex128(0.0016473638889881496+0j)) <f>: (np.float32(0.0052769613), np.complex128(0.0043251729915033225+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a08ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7224c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.03014635480940342 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.005425841547548771 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07336182), np.complex128(0.0026127819795248087+0j)) <f>: (np.float32(0.0052315597), np.complex128(0.004604380608921426+0j))\n",
      "Epoch 3200: <Test loss>: 0.004636659752577543 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073546745), np.complex128(0.002319015072079623+0j)) <f>: (np.float32(0.0050466075), np.complex128(0.004475444512761191+0j))\n",
      "Epoch 4800: <Test loss>: 0.004503382369875908 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.074074455), np.complex128(0.002312302621058301+0j)) <f>: (np.float32(0.004518892), np.complex128(0.004295815913416196+0j))\n",
      "Epoch 6400: <Test loss>: 0.004437440074980259 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07431292), np.complex128(0.002274863250588191+0j)) <f>: (np.float32(0.0042804694), np.complex128(0.004251396415940961+0j))\n",
      "Epoch 8000: <Test loss>: 0.00440643634647131 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07428492), np.complex128(0.0023435270879992583+0j)) <f>: (np.float32(0.004308445), np.complex128(0.004172811314079987+0j))\n",
      "Epoch 9600: <Test loss>: 0.004370136186480522 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07406705), np.complex128(0.002370538766281385+0j)) <f>: (np.float32(0.0045263036), np.complex128(0.004124175475847558+0j))\n",
      "Epoch 11200: <Test loss>: 0.0046012126840651035 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07437293), np.complex128(0.0024771478944580158+0j)) <f>: (np.float32(0.0042203795), np.complex128(0.003922150896322187+0j))\n",
      "Epoch 12800: <Test loss>: 0.004589862190186977 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.074044526), np.complex128(0.002472112033864155+0j)) <f>: (np.float32(0.004548853), np.complex128(0.003922556596699533+0j))\n",
      "Epoch 14400: <Test loss>: 0.0046112919226288795 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073427156), np.complex128(0.0025596146786661025+0j)) <f>: (np.float32(0.005166153), np.complex128(0.0039078405069018435+0j))\n",
      "Epoch 16000: <Test loss>: 0.004659627098590136 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07361858), np.complex128(0.00255369307697323+0j)) <f>: (np.float32(0.004974798), np.complex128(0.0038656063110839513+0j))\n",
      "Epoch 17600: <Test loss>: 0.004639959428459406 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0735437), np.complex128(0.0025864492594726888+0j)) <f>: (np.float32(0.005049659), np.complex128(0.0038838861704252408+0j))\n",
      "Epoch 19200: <Test loss>: 0.004828087519854307 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073626846), np.complex128(0.0026242273478938676+0j)) <f>: (np.float32(0.0049664937), np.complex128(0.0037849688575329454+0j))\n",
      "Epoch 20800: <Test loss>: 0.004802486393600702 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073509604), np.complex128(0.0026562350525239243+0j)) <f>: (np.float32(0.005083728), np.complex128(0.0037630767678775287+0j))\n",
      "Epoch 22400: <Test loss>: 0.004869908094406128 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07376661), np.complex128(0.0027118198100401433+0j)) <f>: (np.float32(0.004826756), np.complex128(0.0037395626378766583+0j))\n",
      "Epoch 24000: <Test loss>: 0.0048911296762526035 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07350713), np.complex128(0.002703649730083325+0j)) <f>: (np.float32(0.005086279), np.complex128(0.0036873810443137575+0j))\n",
      "Epoch 25600: <Test loss>: 0.004852112848311663 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073308095), np.complex128(0.00267018870978007+0j)) <f>: (np.float32(0.005285283), np.complex128(0.0036571529409636957+0j))\n",
      "Epoch 27200: <Test loss>: 0.004965873435139656 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07343328), np.complex128(0.002740547912995661+0j)) <f>: (np.float32(0.0051600863), np.complex128(0.003622756450184344+0j))\n",
      "Epoch 28800: <Test loss>: 0.004999863915145397 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07346834), np.complex128(0.0027511143907411237+0j)) <f>: (np.float32(0.005125021), np.complex128(0.003601251539250523+0j))\n",
      "Epoch 30400: <Test loss>: 0.005067886784672737 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.0735587), np.complex128(0.0027726025560683753+0j)) <f>: (np.float32(0.005034699), np.complex128(0.0035599898572202296+0j))\n",
      "Epoch 32000: <Test loss>: 0.005104296840727329 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073545106), np.complex128(0.0027642000673900754+0j)) <f>: (np.float32(0.0050482797), np.complex128(0.0035505732444592176+0j))\n",
      "Epoch 33600: <Test loss>: 0.005163499154150486 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07357375), np.complex128(0.002790198890196048+0j)) <f>: (np.float32(0.0050195544), np.complex128(0.003534825523808312+0j))\n",
      "Epoch 35200: <Test loss>: 0.005238648969680071 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07360643), np.complex128(0.002811165658227835+0j)) <f>: (np.float32(0.0049869153), np.complex128(0.003508291095313421+0j))\n",
      "Epoch 36800: <Test loss>: 0.005312564317137003 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07365079), np.complex128(0.002812329731605739+0j)) <f>: (np.float32(0.0049425433), np.complex128(0.0034937614906798025+0j))\n",
      "Epoch 38400: <Test loss>: 0.005343941505998373 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.07371258), np.complex128(0.0028211917095794584+0j)) <f>: (np.float32(0.004880779), np.complex128(0.003492132853580246+0j))\n",
      "Epoch 40000: <Test loss>: 0.005441440735012293 <O>: (np.float32(0.078593336), np.complex128(0.0051883481170550815+0j)) <O-f>: (np.float32(0.073736474), np.complex128(0.002830410165996062+0j)) <f>: (np.float32(0.0048569855), np.complex128(0.003440473672198057+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_10min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8c5bd5",
   "metadata": {},
   "source": [
    "# 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "333cdb40",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(-0.00026859494), np.complex128(4.165512866076346e-05+0j))\n",
      "bin size 1: (np.float32(-0.00026859494), np.complex128(4.165511880292293e-05+0j))\n",
      "jack bin size 2: (np.float32(-0.00026859494), np.complex128(5.771586174480367e-05+0j))\n",
      "bin size 2: (np.float32(-0.00026859494), np.complex128(5.771588137737261e-05+0j))\n",
      "jack bin size 4: (np.float32(-0.00026859494), np.complex128(7.934396848155021e-05+0j))\n",
      "bin size 4: (np.float32(-0.00026859494), np.complex128(7.934399817653766e-05+0j))\n",
      "jack bin size 5: (np.float32(-0.00026859494), np.complex128(8.761107903970432e-05+0j))\n",
      "bin size 5: (np.float32(-0.00026859494), np.complex128(8.761109916150745e-05+0j))\n",
      "jack bin size 10: (np.float32(-0.00026859494), np.complex128(0.00011723072219819025+0j))\n",
      "bin size 10: (np.float32(-0.00026859494), np.complex128(0.00011723072634455652+0j))\n",
      "jack bin size 20: (np.float32(-0.00026859494), np.complex128(0.00015163922532935542+0j))\n",
      "bin size 20: (np.float32(-0.00026859494), np.complex128(0.00015163922928166087+0j))\n",
      "jack bin size 50: (np.float32(-0.00026859494), np.complex128(0.0001948985297015352+0j))\n",
      "bin size 50: (np.float32(-0.00026859494), np.complex128(0.00019489852278525317+0j))\n",
      "jack bin size 100: (np.float32(-0.00026859494), np.complex128(0.0002172294583771019+0j))\n",
      "bin size 100: (np.float32(-0.00026859494), np.complex128(0.0002172294962164096+0j))\n",
      "jack bin size 200: (np.float32(-0.00026859494), np.complex128(0.00022576405381299875+0j))\n",
      "bin size 200: (np.float32(-0.00026859494), np.complex128(0.00022576412669208715+0j))\n",
      "jack bin size 500: (np.float32(-0.00026859494), np.complex128(0.0002490216586118482+0j))\n",
      "bin size 500: (np.float32(-0.00026859494), np.complex128(0.00024902164700682153+0j))\n",
      "jack bin size 1000: (np.float32(-0.00026859494), np.complex128(0.0002353523733190024+0j))\n",
      "bin size 1000: (np.float32(-0.00026859494), np.complex128(0.0002353523747815249+0j))\n",
      "jack bin size 2000: (np.float32(-0.00026859494), np.complex128(0.00021831642879988067+0j))\n",
      "bin size 2000: (np.float32(-0.00026859494), np.complex128(0.0002183163970974939+0j))\n",
      "jack bin size 5000: (np.float32(-0.00026859494), np.complex128(0.0002697519632902595+0j))\n",
      "bin size 5000: (np.float32(-0.00026859494), np.complex128(0.0002697519849901085+0j))\n",
      "jack bin size 10000: (np.float32(-0.00026859494), np.complex128(0.00024116023269016296+0j))\n",
      "bin size 10000: (np.float32(-0.00026859494), np.complex128(0.00024116021813824773+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_32x32_0.1_0.5_sweep1.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model = eval(\"scalar.Model(geom=(32,), nbeta=32, nt=0, m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 16))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d9c5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.0006155631), np.complex128(0.000617856441431765+0j)),\n",
       " (np.float32(0.00073884043), np.complex128(0.0007742307093962933+0j)),\n",
       " (np.float32(-2.4659561e-05), np.complex128(0.00038643335181240006+0j)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jackknife(jax.vmap(lambda x: model.observe(x, 16))(conf[:10000])), jackknife(jax.vmap(lambda x: model.observe(x, 16))(conf[:10000:10])), jackknife(jax.vmap(lambda x: model.observe(x, 16))(conf[:100000:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f3df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j) for i, j in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49972a3c",
   "metadata": {},
   "source": [
    "## Correlated, n_train=1e4, sweep=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d89ebfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 8.152605914801825e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00012160713), np.complex128(8.707070805521653e-05+0j)) <f>: (np.float32(0.00016599766), np.complex128(0.00037925421215344074+0j))\n",
      "Epoch 200: <Test loss>: 3.459967047092505e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.21149e-06), np.complex128(6.069086323385103e-05+0j)) <f>: (np.float32(0.00029081642), np.complex128(0.00036783082239300475+0j))\n",
      "Epoch 300: <Test loss>: 2.7249793674855027e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.0237556e-05), np.complex128(4.947342778376091e-05+0j)) <f>: (np.float32(0.00025736738), np.complex128(0.0003542925069240488+0j))\n",
      "Epoch 400: <Test loss>: 2.6972577416017884e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.008183e-05), np.complex128(5.706258504676099e-05+0j)) <f>: (np.float32(0.00023752327), np.complex128(0.00035069610247673625+0j))\n",
      "Epoch 500: <Test loss>: 2.5392594125150936e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.8514077e-05), np.complex128(5.3140846447570255e-05+0j)) <f>: (np.float32(0.00023909079), np.complex128(0.0003599685698175603+0j))\n",
      "Epoch 600: <Test loss>: 2.521014039302827e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.196847e-05), np.complex128(5.954255229243419e-05+0j)) <f>: (np.float32(0.00024563662), np.complex128(0.0003449937477123038+0j))\n",
      "Epoch 700: <Test loss>: 2.237090939161135e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.9038226e-05), np.complex128(5.090997009962092e-05+0j)) <f>: (np.float32(0.00023856676), np.complex128(0.0003511932059566094+0j))\n",
      "Epoch 800: <Test loss>: 2.4401974769716617e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.170465e-05), np.complex128(5.803159430874291e-05+0j)) <f>: (np.float32(0.00021590035), np.complex128(0.0003546786707603979+0j))\n",
      "Epoch 900: <Test loss>: 2.2553376766154543e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.2919077e-05), np.complex128(5.4640692230689855e-05+0j)) <f>: (np.float32(0.00025468596), np.complex128(0.00035900205019594904+0j))\n",
      "Epoch 1000: <Test loss>: 3.0473597689706367e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.75602e-05), np.complex128(6.432671341633597e-05+0j)) <f>: (np.float32(0.000260045), np.complex128(0.000348875366629088+0j))\n",
      "Epoch 1100: <Test loss>: 2.496991783118574e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.4722463e-05), np.complex128(6.0235901278086676e-05+0j)) <f>: (np.float32(0.00025288246), np.complex128(0.0003642463428473403+0j))\n",
      "Epoch 1200: <Test loss>: 2.4088624286378035e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.35346e-06), np.complex128(5.519798712735926e-05+0j)) <f>: (np.float32(0.00028225177), np.complex128(0.0003619098232882478+0j))\n",
      "Epoch 1300: <Test loss>: 2.4678931822563754e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00010819616), np.complex128(6.762271579503656e-05+0j)) <f>: (np.float32(0.0001794088), np.complex128(0.0003500042996053267+0j))\n",
      "Epoch 1400: <Test loss>: 2.0988188680348685e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.6571997e-05), np.complex128(5.2694840061228466e-05+0j)) <f>: (np.float32(0.00025103305), np.complex128(0.00036132477365871993+0j))\n",
      "Epoch 1500: <Test loss>: 2.348225280002225e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.425661e-05), np.complex128(5.761949143267191e-05+0j)) <f>: (np.float32(0.00025334858), np.complex128(0.000364812046226854+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_1h_cor.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 100 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "01408b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 8.152605914801825e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00012160713), np.complex128(8.707070805521653e-05+0j)) <f>: (np.float32(0.00016599766), np.complex128(0.00037925421215344074+0j))\n",
      "Epoch 200: <Test loss>: 3.459967047092505e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.21149e-06), np.complex128(6.069086323385103e-05+0j)) <f>: (np.float32(0.00029081642), np.complex128(0.00036783082239300475+0j))\n",
      "Epoch 300: <Test loss>: 2.7249793674855027e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.0237556e-05), np.complex128(4.947342778376091e-05+0j)) <f>: (np.float32(0.00025736738), np.complex128(0.0003542925069240488+0j))\n",
      "Epoch 400: <Test loss>: 2.6972577416017884e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.008183e-05), np.complex128(5.706258504676099e-05+0j)) <f>: (np.float32(0.00023752327), np.complex128(0.00035069610247673625+0j))\n",
      "Epoch 500: <Test loss>: 2.5392594125150936e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.8514077e-05), np.complex128(5.3140846447570255e-05+0j)) <f>: (np.float32(0.00023909079), np.complex128(0.0003599685698175603+0j))\n",
      "Epoch 600: <Test loss>: 2.521014039302827e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.196847e-05), np.complex128(5.954255229243419e-05+0j)) <f>: (np.float32(0.00024563662), np.complex128(0.0003449937477123038+0j))\n",
      "Epoch 700: <Test loss>: 2.237090939161135e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.9038226e-05), np.complex128(5.090997009962092e-05+0j)) <f>: (np.float32(0.00023856676), np.complex128(0.0003511932059566094+0j))\n",
      "Epoch 800: <Test loss>: 2.4401974769716617e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.170465e-05), np.complex128(5.803159430874291e-05+0j)) <f>: (np.float32(0.00021590035), np.complex128(0.0003546786707603979+0j))\n",
      "Epoch 900: <Test loss>: 2.2553376766154543e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.2919077e-05), np.complex128(5.4640692230689855e-05+0j)) <f>: (np.float32(0.00025468596), np.complex128(0.00035900205019594904+0j))\n",
      "Epoch 1000: <Test loss>: 3.0473597689706367e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.75602e-05), np.complex128(6.432671341633597e-05+0j)) <f>: (np.float32(0.000260045), np.complex128(0.000348875366629088+0j))\n",
      "Epoch 1100: <Test loss>: 2.496991783118574e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.4722463e-05), np.complex128(6.0235901278086676e-05+0j)) <f>: (np.float32(0.00025288246), np.complex128(0.0003642463428473403+0j))\n",
      "Epoch 1200: <Test loss>: 2.4088624286378035e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.35346e-06), np.complex128(5.519798712735926e-05+0j)) <f>: (np.float32(0.00028225177), np.complex128(0.0003619098232882478+0j))\n",
      "Epoch 1300: <Test loss>: 2.4678931822563754e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00010819616), np.complex128(6.762271579503656e-05+0j)) <f>: (np.float32(0.0001794088), np.complex128(0.0003500042996053267+0j))\n",
      "Epoch 1400: <Test loss>: 2.0988188680348685e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.6571997e-05), np.complex128(5.2694840061228466e-05+0j)) <f>: (np.float32(0.00025103305), np.complex128(0.00036132477365871993+0j))\n",
      "Epoch 1500: <Test loss>: 2.348225280002225e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.425661e-05), np.complex128(5.761949143267191e-05+0j)) <f>: (np.float32(0.00025334858), np.complex128(0.000364812046226854+0j))\n",
      "Epoch 1600: <Test loss>: 2.0964751001883997e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.879834e-05), np.complex128(5.901868517857622e-05+0j)) <f>: (np.float32(0.0002488067), np.complex128(0.00035651910172486385+0j))\n",
      "Epoch 1700: <Test loss>: 1.9772903669945663e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.7803466e-05), np.complex128(5.62247259165352e-05+0j)) <f>: (np.float32(0.00026980139), np.complex128(0.0003632225139244575+0j))\n",
      "Epoch 1800: <Test loss>: 2.22262974602927e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.5856267e-05), np.complex128(5.44378697927848e-05+0j)) <f>: (np.float32(0.0002717487), np.complex128(0.0003616507103987133+0j))\n",
      "Epoch 1900: <Test loss>: 2.4667599518579664e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.2927186e-05), np.complex128(6.194842895070668e-05+0j)) <f>: (np.float32(0.00027467767), np.complex128(0.0003713840625021708+0j))\n",
      "Epoch 2000: <Test loss>: 2.031130179602769e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.9518919e-05), np.complex128(5.569708073377076e-05+0j)) <f>: (np.float32(0.00026808624), np.complex128(0.00037015281640700456+0j))\n",
      "Epoch 2100: <Test loss>: 2.2627255020779558e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.4383651e-05), np.complex128(6.47312958341535e-05+0j)) <f>: (np.float32(0.00026322142), np.complex128(0.00036365022462559116+0j))\n",
      "Epoch 2200: <Test loss>: 2.2277995412878226e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.0625984e-06), np.complex128(6.253513173315587e-05+0j)) <f>: (np.float32(0.0002886676), np.complex128(0.00037374107005717997+0j))\n",
      "Epoch 2300: <Test loss>: 2.0921027044096263e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.2484303e-05), np.complex128(5.449509584175099e-05+0j)) <f>: (np.float32(0.00030008942), np.complex128(0.000363996807937321+0j))\n",
      "Epoch 2400: <Test loss>: 2.462696784277796e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.1768664e-05), np.complex128(6.313865148129364e-05+0j)) <f>: (np.float32(0.00027583656), np.complex128(0.00035358478304033244+0j))\n",
      "Epoch 2500: <Test loss>: 2.2937069843464997e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-7.730312e-06), np.complex128(6.112163444830664e-05+0j)) <f>: (np.float32(0.00029533557), np.complex128(0.0003652752144812924+0j))\n",
      "Epoch 2600: <Test loss>: 2.4592027330072597e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.730018e-06), np.complex128(5.918553072736675e-05+0j)) <f>: (np.float32(0.00027987477), np.complex128(0.00036442394776550345+0j))\n",
      "Epoch 2700: <Test loss>: 2.279934960824903e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.4046015e-05), np.complex128(5.7337337476522134e-05+0j)) <f>: (np.float32(0.00030165128), np.complex128(0.00037046191239493645+0j))\n",
      "Epoch 2800: <Test loss>: 2.6734205675893463e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(9.509941e-06), np.complex128(5.6674363689150146e-05+0j)) <f>: (np.float32(0.0002780952), np.complex128(0.0003554062483367517+0j))\n",
      "Epoch 2900: <Test loss>: 2.4278472210426116e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.1369625e-05), np.complex128(5.785731948279529e-05+0j)) <f>: (np.float32(0.0002989745), np.complex128(0.00036704349344775456+0j))\n",
      "Epoch 3000: <Test loss>: 2.4794801447569625e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.0473936e-05), np.complex128(6.020065383774318e-05+0j)) <f>: (np.float32(0.00029807884), np.complex128(0.00036774620633556556+0j))\n",
      "Epoch 3100: <Test loss>: 2.5251101760659367e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.9505896e-05), np.complex128(6.212624398183108e-05+0j)) <f>: (np.float32(0.00031711097), np.complex128(0.0003624869124116223+0j))\n",
      "Epoch 3200: <Test loss>: 2.5790536710701417e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.8455185e-05), np.complex128(6.17666336665056e-05+0j)) <f>: (np.float32(0.0003060601), np.complex128(0.000370887434749757+0j))\n",
      "Epoch 3300: <Test loss>: 2.620924533403013e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.5468117e-05), np.complex128(6.054166717380297e-05+0j)) <f>: (np.float32(0.00030307318), np.complex128(0.0003692458578633059+0j))\n",
      "Epoch 3400: <Test loss>: 2.780413296932238e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.663244e-06), np.complex128(6.382899149076564e-05+0j)) <f>: (np.float32(0.00029326818), np.complex128(0.0003652403595161029+0j))\n",
      "Epoch 3500: <Test loss>: 2.6955526664096396e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.6649076e-06), np.complex128(6.639696435202599e-05+0j)) <f>: (np.float32(0.00029126985), np.complex128(0.0003663408757054354+0j))\n",
      "Epoch 3600: <Test loss>: 2.712079322009231e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.2832091e-05), np.complex128(6.052830319659029e-05+0j)) <f>: (np.float32(0.00031043726), np.complex128(0.00036752121896244957+0j))\n",
      "Epoch 3700: <Test loss>: 3.0445858101302292e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.1791721e-05), np.complex128(6.598887740859113e-05+0j)) <f>: (np.float32(0.0002993966), np.complex128(0.00035694728815345686+0j))\n",
      "Epoch 3800: <Test loss>: 2.8310496418271214e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.3935755e-06), np.complex128(6.466603791991502e-05+0j)) <f>: (np.float32(0.00029299848), np.complex128(0.00037028954047882624+0j))\n",
      "Epoch 3900: <Test loss>: 2.8550325623655226e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-7.1290497e-06), np.complex128(6.478145733034811e-05+0j)) <f>: (np.float32(0.000294734), np.complex128(0.0003623008712598464+0j))\n",
      "Epoch 4000: <Test loss>: 2.916381845352589e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.8045033e-05), np.complex128(6.186865342016656e-05+0j)) <f>: (np.float32(0.0003056501), np.complex128(0.0003738228951801909+0j))\n",
      "Epoch 4100: <Test loss>: 2.945565256595728e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.4731675e-05), np.complex128(6.45939810674031e-05+0j)) <f>: (np.float32(0.00031233687), np.complex128(0.00036950195781226445+0j))\n",
      "Epoch 4200: <Test loss>: 2.976430323542445e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.329116e-05), np.complex128(6.52429208947238e-05+0j)) <f>: (np.float32(0.00030089583), np.complex128(0.00037134730462714383+0j))\n",
      "Epoch 4300: <Test loss>: 3.024430270670564e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-7.740535e-06), np.complex128(6.611603935847957e-05+0j)) <f>: (np.float32(0.00029534558), np.complex128(0.00037338462332945927+0j))\n",
      "Epoch 4400: <Test loss>: 3.1228953503159573e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.5371299e-05), np.complex128(6.784284281927605e-05+0j)) <f>: (np.float32(0.00027223403), np.complex128(0.0003687280126662044+0j))\n",
      "Epoch 4500: <Test loss>: 3.772131321966299e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.0421446e-05), np.complex128(7.325935673974865e-05+0j)) <f>: (np.float32(0.00027718348), np.complex128(0.0003720538867649577+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_30min_sweep1.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 100 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94edb1d8",
   "metadata": {},
   "source": [
    "## less Correlated, n_train=1e3, sweep=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3ac8527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1000: <Test loss>: 5.218342994339764e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019499287), np.complex128(0.00023833986943932854+0j)) <f>: (np.float32(9.261204e-05), np.complex128(0.00036190601746857287+0j))\n",
      "Epoch 2000: <Test loss>: 2.7066582333645783e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.2309981e-06), np.complex128(0.00017099213204459633+0j)) <f>: (np.float32(0.00029083583), np.complex128(0.0003254744597635469+0j))\n",
      "Epoch 3000: <Test loss>: 1.748030263115652e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.1631643e-05), np.complex128(0.00014530387998167916+0j)) <f>: (np.float32(0.0002359735), np.complex128(0.00035851274864640947+0j))\n",
      "Epoch 4000: <Test loss>: 1.4278291018854361e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00014146267), np.complex128(0.0001155911797610942+0j)) <f>: (np.float32(0.00014614225), np.complex128(0.0003578010286520345+0j))\n",
      "Epoch 5000: <Test loss>: 1.4820848264207598e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00025087903), np.complex128(0.0001237912305478522+0j)) <f>: (np.float32(3.6725807e-05), np.complex128(0.0003585045344189444+0j))\n",
      "Epoch 6000: <Test loss>: 1.1707817066053394e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019099453), np.complex128(0.00010600022081501582+0j)) <f>: (np.float32(9.6610485e-05), np.complex128(0.0003592476524256376+0j))\n",
      "Epoch 7000: <Test loss>: 1.109328877646476e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0002195344), np.complex128(8.430741339232712e-05+0j)) <f>: (np.float32(6.807077e-05), np.complex128(0.0003532262113966266+0j))\n",
      "Epoch 8000: <Test loss>: 1.0789513908093795e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.000216088), np.complex128(9.100319016619794e-05+0j)) <f>: (np.float32(7.151688e-05), np.complex128(0.0003563627459665523+0j))\n",
      "Epoch 9000: <Test loss>: 1.0227455277345143e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0001998512), np.complex128(9.757243136466848e-05+0j)) <f>: (np.float32(8.775362e-05), np.complex128(0.00035382229790321176+0j))\n",
      "Epoch 10000: <Test loss>: 2.0129255062784068e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0003567551), np.complex128(0.0001275619732514403+0j)) <f>: (np.float32(-6.914974e-05), np.complex128(0.00035543736091259425+0j))\n",
      "Epoch 11000: <Test loss>: 1.0937244041997474e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0002315701), np.complex128(0.00010627219420353517+0j)) <f>: (np.float32(5.6035104e-05), np.complex128(0.00035582866260550446+0j))\n",
      "Epoch 12000: <Test loss>: 1.0035689228971023e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019608553), np.complex128(9.116098103567854e-05+0j)) <f>: (np.float32(9.1519345e-05), np.complex128(0.00035272368462371665+0j))\n",
      "Epoch 13000: <Test loss>: 1.0154678420803975e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00021582698), np.complex128(9.59717749685138e-05+0j)) <f>: (np.float32(7.177792e-05), np.complex128(0.0003528586326463567+0j))\n",
      "Epoch 14000: <Test loss>: 1.0712763469200581e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019465241), np.complex128(0.00010376577651112125+0j)) <f>: (np.float32(9.295274e-05), np.complex128(0.00035764140623183535+0j))\n",
      "Epoch 15000: <Test loss>: 1.0528906386753079e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00018803403), np.complex128(0.00010112583004965437+0j)) <f>: (np.float32(9.957101e-05), np.complex128(0.00035530054169528066+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 1000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*10:10]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 1000 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c1584ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1000: <Test loss>: 5.218342994339764e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019499287), np.complex128(0.00023833986943932854+0j)) <f>: (np.float32(9.261204e-05), np.complex128(0.00036190601746857287+0j))\n",
      "Epoch 2000: <Test loss>: 2.7066582333645783e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.2309981e-06), np.complex128(0.00017099213204459633+0j)) <f>: (np.float32(0.00029083583), np.complex128(0.0003254744597635469+0j))\n",
      "Epoch 3000: <Test loss>: 1.748030263115652e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.1631643e-05), np.complex128(0.00014530387998167916+0j)) <f>: (np.float32(0.0002359735), np.complex128(0.00035851274864640947+0j))\n",
      "Epoch 4000: <Test loss>: 1.4278291018854361e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00014146267), np.complex128(0.0001155911797610942+0j)) <f>: (np.float32(0.00014614225), np.complex128(0.0003578010286520345+0j))\n",
      "Epoch 5000: <Test loss>: 1.4820848264207598e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00025087903), np.complex128(0.0001237912305478522+0j)) <f>: (np.float32(3.6725807e-05), np.complex128(0.0003585045344189444+0j))\n",
      "Epoch 6000: <Test loss>: 1.1707817066053394e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019099453), np.complex128(0.00010600022081501582+0j)) <f>: (np.float32(9.6610485e-05), np.complex128(0.0003592476524256376+0j))\n",
      "Epoch 7000: <Test loss>: 1.109328877646476e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0002195344), np.complex128(8.430741339232712e-05+0j)) <f>: (np.float32(6.807077e-05), np.complex128(0.0003532262113966266+0j))\n",
      "Epoch 8000: <Test loss>: 1.0789513908093795e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.000216088), np.complex128(9.100319016619794e-05+0j)) <f>: (np.float32(7.151688e-05), np.complex128(0.0003563627459665523+0j))\n",
      "Epoch 9000: <Test loss>: 1.0227455277345143e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0001998512), np.complex128(9.757243136466848e-05+0j)) <f>: (np.float32(8.775362e-05), np.complex128(0.00035382229790321176+0j))\n",
      "Epoch 10000: <Test loss>: 2.0129255062784068e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0003567551), np.complex128(0.0001275619732514403+0j)) <f>: (np.float32(-6.914974e-05), np.complex128(0.00035543736091259425+0j))\n",
      "Epoch 11000: <Test loss>: 1.0937244041997474e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0002315701), np.complex128(0.00010627219420353517+0j)) <f>: (np.float32(5.6035104e-05), np.complex128(0.00035582866260550446+0j))\n",
      "Epoch 12000: <Test loss>: 1.0035689228971023e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019608553), np.complex128(9.116098103567854e-05+0j)) <f>: (np.float32(9.1519345e-05), np.complex128(0.00035272368462371665+0j))\n",
      "Epoch 13000: <Test loss>: 1.0154678420803975e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00021582698), np.complex128(9.59717749685138e-05+0j)) <f>: (np.float32(7.177792e-05), np.complex128(0.0003528586326463567+0j))\n",
      "Epoch 14000: <Test loss>: 1.0712763469200581e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019465241), np.complex128(0.00010376577651112125+0j)) <f>: (np.float32(9.295274e-05), np.complex128(0.00035764140623183535+0j))\n",
      "Epoch 15000: <Test loss>: 1.0528906386753079e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00018803403), np.complex128(0.00010112583004965437+0j)) <f>: (np.float32(9.957101e-05), np.complex128(0.00035530054169528066+0j))\n",
      "Epoch 16000: <Test loss>: 1.0926736649707891e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0001949447), np.complex128(0.00010727528140918966+0j)) <f>: (np.float32(9.2660186e-05), np.complex128(0.00036249544379072694+0j))\n",
      "Epoch 17000: <Test loss>: 1.1166609510837588e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019672672), np.complex128(0.00010702929066874266+0j)) <f>: (np.float32(9.087835e-05), np.complex128(0.0003566578872823428+0j))\n",
      "Epoch 18000: <Test loss>: 1.6975276594166644e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00019971468), np.complex128(0.0001339768250317647+0j)) <f>: (np.float32(8.789023e-05), np.complex128(0.0003594227201306841+0j))\n",
      "Epoch 19000: <Test loss>: 1.380539015372051e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0002065255), np.complex128(0.00014162493682016666+0j)) <f>: (np.float32(8.107933e-05), np.complex128(0.00037235981123649373+0j))\n",
      "Epoch 20000: <Test loss>: 1.1172824088134803e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0001940186), np.complex128(0.00010896125952518157+0j)) <f>: (np.float32(9.3586335e-05), np.complex128(0.00035809347417888857+0j))\n",
      "Epoch 21000: <Test loss>: 1.1654356967483182e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00018392394), np.complex128(0.0001144589721941769+0j)) <f>: (np.float32(0.00010368095), np.complex128(0.00036186015734149+0j))\n",
      "Epoch 22000: <Test loss>: 1.1889011148014106e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00017862642), np.complex128(0.00011697706899571825+0j)) <f>: (np.float32(0.00010897865), np.complex128(0.00036237498959801556+0j))\n",
      "Epoch 23000: <Test loss>: 1.207570585393114e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00017724966), np.complex128(0.00011822725697255824+0j)) <f>: (np.float32(0.00011035571), np.complex128(0.00036595651963343195+0j))\n",
      "Epoch 24000: <Test loss>: 1.1763431757572107e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00018090132), np.complex128(0.0001196962399367515+0j)) <f>: (np.float32(0.000106703825), np.complex128(0.0003655163131576989+0j))\n",
      "Epoch 25000: <Test loss>: 1.2054361832269933e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0001771612), np.complex128(0.00012044084062735964+0j)) <f>: (np.float32(0.00011044383), np.complex128(0.0003650969435446861+0j))\n",
      "Epoch 26000: <Test loss>: 1.307887850998668e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00017808357), np.complex128(0.00012317668581584366+0j)) <f>: (np.float32(0.00010952144), np.complex128(0.000365694013221354+0j))\n",
      "Epoch 27000: <Test loss>: 1.2292586689000018e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00017395847), np.complex128(0.00012467632148600204+0j)) <f>: (np.float32(0.000113646616), np.complex128(0.00036917129551284136+0j))\n",
      "Epoch 28000: <Test loss>: 1.2116708603571169e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00015791375), np.complex128(0.00012008810457382198+0j)) <f>: (np.float32(0.00012969143), np.complex128(0.0003656520857746019+0j))\n",
      "Epoch 29000: <Test loss>: 1.2711836461676285e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00016737146), np.complex128(0.00012415896787394188+0j)) <f>: (np.float32(0.000120233504), np.complex128(0.0003674524287718253+0j))\n",
      "Epoch 30000: <Test loss>: 1.2814289220841601e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00017964124), np.complex128(0.00012808669271032963+0j)) <f>: (np.float32(0.00010796355), np.complex128(0.00036754364158336767+0j))\n",
      "Epoch 31000: <Test loss>: 1.3165465134079568e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00017019875), np.complex128(0.00012887047748600752+0j)) <f>: (np.float32(0.00011740621), np.complex128(0.0003645637164930651+0j))\n",
      "Epoch 32000: <Test loss>: 1.565334969200194e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00017326535), np.complex128(0.00014617496867494164+0j)) <f>: (np.float32(0.000114339644), np.complex128(0.0003902333358971571+0j))\n",
      "Epoch 33000: <Test loss>: 1.3762049093202222e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00013522219), np.complex128(0.00013436792057610922+0j)) <f>: (np.float32(0.00015238297), np.complex128(0.0003732529736838708+0j))\n",
      "Epoch 34000: <Test loss>: 1.3540438885684125e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00015876614), np.complex128(0.000131222006246443+0j)) <f>: (np.float32(0.00012883882), np.complex128(0.0003707626990099113+0j))\n",
      "Epoch 35000: <Test loss>: 1.407546915288549e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00015850499), np.complex128(0.00013425227123073743+0j)) <f>: (np.float32(0.00012910012), np.complex128(0.0003744679181847628+0j))\n",
      "Epoch 36000: <Test loss>: 1.3684856639883947e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00016304958), np.complex128(0.00013514567154184417+0j)) <f>: (np.float32(0.00012455549), np.complex128(0.00037174837459055305+0j))\n",
      "Epoch 37000: <Test loss>: 1.4014475709700491e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00015218245), np.complex128(0.00013313545344713058+0j)) <f>: (np.float32(0.00013542261), np.complex128(0.0003697690946382802+0j))\n",
      "Epoch 38000: <Test loss>: 1.391380556015065e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00018393094), np.complex128(0.00013560503397660765+0j)) <f>: (np.float32(0.00010367415), np.complex128(0.0003834381082979059+0j))\n",
      "Epoch 39000: <Test loss>: 1.4523914614983369e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00015316086), np.complex128(0.0001362885750478062+0j)) <f>: (np.float32(0.0001344442), np.complex128(0.000374501504543394+0j))\n",
      "Epoch 40000: <Test loss>: 1.4284212738857605e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00016246652), np.complex128(0.00013787537984610236+0j)) <f>: (np.float32(0.00012513828), np.complex128(0.0003619142316960379+0j))\n",
      "Epoch 41000: <Test loss>: 1.4678826119052246e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00014811968), np.complex128(0.00013663504735646223+0j)) <f>: (np.float32(0.00013948558), np.complex128(0.000374870478760878+0j))\n",
      "Epoch 42000: <Test loss>: 1.484037420595996e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00013313862), np.complex128(0.00013741977565826785+0j)) <f>: (np.float32(0.00015446641), np.complex128(0.0003756873662389368+0j))\n",
      "Epoch 43000: <Test loss>: 1.5217201507766731e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00013414033), np.complex128(0.00013888481008454836+0j)) <f>: (np.float32(0.0001534649), np.complex128(0.00036984473530431936+0j))\n",
      "Epoch 44000: <Test loss>: 1.5037795492389705e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.0001337488), np.complex128(0.00013894145336737685+0j)) <f>: (np.float32(0.00015385615), np.complex128(0.0003651095344647773+0j))\n",
      "Epoch 45000: <Test loss>: 1.4910142454027664e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00013772515), np.complex128(0.00013850586144799966+0j)) <f>: (np.float32(0.00014988), np.complex128(0.00037104670830315266+0j))\n",
      "Epoch 46000: <Test loss>: 1.545617305964697e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00014458715), np.complex128(0.00014281655481796776+0j)) <f>: (np.float32(0.00014301809), np.complex128(0.00037540960483299515+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 1000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:10*n_train:10]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_30min_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 1000 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a6bf36",
   "metadata": {},
   "source": [
    "## less less correlated, n_train=1e3, sweep=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa285607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1000: <Test loss>: 3.590980122680776e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016684472), np.complex128(0.00018375250625683357+0j)) <f>: (np.float32(0.00045444985), np.complex128(0.000366774675718049+0j))\n",
      "Epoch 2000: <Test loss>: 2.7706426408258267e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00011330716), np.complex128(0.00015434552435921898+0j)) <f>: (np.float32(0.00040091222), np.complex128(0.0003389207061115336+0j))\n",
      "Epoch 3000: <Test loss>: 2.6061983589897864e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.210746e-05), np.complex128(0.00013803352751117748+0j)) <f>: (np.float32(0.00032971255), np.complex128(0.0003372543279668676+0j))\n",
      "Epoch 4000: <Test loss>: 2.230539575975854e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.798206e-05), np.complex128(0.00013884059914599133+0j)) <f>: (np.float32(0.00034558686), np.complex128(0.0003455846963623278+0j))\n",
      "Epoch 5000: <Test loss>: 1.8221613572677597e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.2865377e-05), np.complex128(0.00012264257073963183+0j)) <f>: (np.float32(0.00031047047), np.complex128(0.0003320147291597421+0j))\n",
      "Epoch 6000: <Test loss>: 1.6285617675748654e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.18737e-05), np.complex128(0.00011949471385617321+0j)) <f>: (np.float32(0.00032947864), np.complex128(0.0003358293656502476+0j))\n",
      "Epoch 7000: <Test loss>: 1.487774989072932e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.8183643e-05), np.complex128(0.00010727279969760997+0j)) <f>: (np.float32(0.00033578888), np.complex128(0.0003417119894066157+0j))\n",
      "Epoch 8000: <Test loss>: 1.3650099390360992e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-8.391156e-06), np.complex128(0.00010383041994405806+0j)) <f>: (np.float32(0.00029599637), np.complex128(0.00033558877441646444+0j))\n",
      "Epoch 9000: <Test loss>: 1.260887711396208e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.442315e-06), np.complex128(0.00011329539040131442+0j)) <f>: (np.float32(0.0002841627), np.complex128(0.0003482823882085707+0j))\n",
      "Epoch 10000: <Test loss>: 1.2780730685335584e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.419006e-06), np.complex128(0.00011523695309485053+0j)) <f>: (np.float32(0.00028218626), np.complex128(0.0003448094508945455+0j))\n",
      "Epoch 11000: <Test loss>: 1.1477167390694376e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.929573e-06), np.complex128(0.00011316518379568603+0j)) <f>: (np.float32(0.00027967553), np.complex128(0.0003413280139165797+0j))\n",
      "Epoch 12000: <Test loss>: 1.2146736480644904e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.8378467e-05), np.complex128(0.00012550012286980357+0j)) <f>: (np.float32(0.00026922647), np.complex128(0.00034818200971464456+0j))\n",
      "Epoch 13000: <Test loss>: 1.1635172086243983e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.4745816e-06), np.complex128(0.00011728835332996318+0j)) <f>: (np.float32(0.00029207946), np.complex128(0.0003450667877348984+0j))\n",
      "Epoch 14000: <Test loss>: 1.0377466423960868e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.989283e-06), np.complex128(0.00012681497013716236+0j)) <f>: (np.float32(0.00028161565), np.complex128(0.00035299456383907946+0j))\n",
      "Epoch 15000: <Test loss>: 1.1085935511800926e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.234643e-05), np.complex128(0.00013232372761200594+0j)) <f>: (np.float32(0.00025525852), np.complex128(0.0003436360215432742+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 1000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*100:100]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 1000 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c09a4a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1000: <Test loss>: 3.590980122680776e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016684472), np.complex128(0.00018375250625683357+0j)) <f>: (np.float32(0.00045444985), np.complex128(0.000366774675718049+0j))\n",
      "Epoch 2000: <Test loss>: 2.7706426408258267e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00011330716), np.complex128(0.00015434552435921898+0j)) <f>: (np.float32(0.00040091222), np.complex128(0.0003389207061115336+0j))\n",
      "Epoch 3000: <Test loss>: 2.6061983589897864e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.210746e-05), np.complex128(0.00013803352751117748+0j)) <f>: (np.float32(0.00032971255), np.complex128(0.0003372543279668676+0j))\n",
      "Epoch 4000: <Test loss>: 2.230539575975854e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.798206e-05), np.complex128(0.00013884059914599133+0j)) <f>: (np.float32(0.00034558686), np.complex128(0.0003455846963623278+0j))\n",
      "Epoch 5000: <Test loss>: 1.8221613572677597e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.2865377e-05), np.complex128(0.00012264257073963183+0j)) <f>: (np.float32(0.00031047047), np.complex128(0.0003320147291597421+0j))\n",
      "Epoch 6000: <Test loss>: 1.6285617675748654e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.18737e-05), np.complex128(0.00011949471385617321+0j)) <f>: (np.float32(0.00032947864), np.complex128(0.0003358293656502476+0j))\n",
      "Epoch 7000: <Test loss>: 1.487774989072932e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.8183643e-05), np.complex128(0.00010727279969760997+0j)) <f>: (np.float32(0.00033578888), np.complex128(0.0003417119894066157+0j))\n",
      "Epoch 8000: <Test loss>: 1.3650099390360992e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-8.391156e-06), np.complex128(0.00010383041994405806+0j)) <f>: (np.float32(0.00029599637), np.complex128(0.00033558877441646444+0j))\n",
      "Epoch 9000: <Test loss>: 1.260887711396208e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.442315e-06), np.complex128(0.00011329539040131442+0j)) <f>: (np.float32(0.0002841627), np.complex128(0.0003482823882085707+0j))\n",
      "Epoch 10000: <Test loss>: 1.2780730685335584e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.419006e-06), np.complex128(0.00011523695309485053+0j)) <f>: (np.float32(0.00028218626), np.complex128(0.0003448094508945455+0j))\n",
      "Epoch 11000: <Test loss>: 1.1477167390694376e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.929573e-06), np.complex128(0.00011316518379568603+0j)) <f>: (np.float32(0.00027967553), np.complex128(0.0003413280139165797+0j))\n",
      "Epoch 12000: <Test loss>: 1.2146736480644904e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.8378467e-05), np.complex128(0.00012550012286980357+0j)) <f>: (np.float32(0.00026922647), np.complex128(0.00034818200971464456+0j))\n",
      "Epoch 13000: <Test loss>: 1.1635172086243983e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.4745816e-06), np.complex128(0.00011728835332996318+0j)) <f>: (np.float32(0.00029207946), np.complex128(0.0003450667877348984+0j))\n",
      "Epoch 14000: <Test loss>: 1.0377466423960868e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.989283e-06), np.complex128(0.00012681497013716236+0j)) <f>: (np.float32(0.00028161565), np.complex128(0.00035299456383907946+0j))\n",
      "Epoch 15000: <Test loss>: 1.1085935511800926e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.234643e-05), np.complex128(0.00013232372761200594+0j)) <f>: (np.float32(0.00025525852), np.complex128(0.0003436360215432742+0j))\n",
      "Epoch 16000: <Test loss>: 1.0815610039571766e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.0686173e-05), np.complex128(0.00013222058197002448+0j)) <f>: (np.float32(0.00025691887), np.complex128(0.0003476320370564539+0j))\n",
      "Epoch 17000: <Test loss>: 1.1697360605467111e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.0604507e-05), np.complex128(0.00013927270239733316+0j)) <f>: (np.float32(0.00024700043), np.complex128(0.00035227767427297937+0j))\n",
      "Epoch 18000: <Test loss>: 1.118409272748977e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.1093336e-05), np.complex128(0.00013191062967266576+0j)) <f>: (np.float32(0.00026651172), np.complex128(0.0003545516832439112+0j))\n",
      "Epoch 19000: <Test loss>: 1.2627997421077453e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.4112636e-05), np.complex128(0.00014151957904549916+0j)) <f>: (np.float32(0.00023349239), np.complex128(0.00034192337097439384+0j))\n",
      "Epoch 20000: <Test loss>: 1.1339784578012768e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.4779549e-05), np.complex128(0.00014601333234183118+0j)) <f>: (np.float32(0.0002628254), np.complex128(0.0003567270263397706+0j))\n",
      "Epoch 21000: <Test loss>: 1.147792863775976e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.8034692e-05), np.complex128(0.00014457110111843624+0j)) <f>: (np.float32(0.00026957042), np.complex128(0.0003543563495490957+0j))\n",
      "Epoch 22000: <Test loss>: 1.1775791790569201e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.120086e-05), np.complex128(0.00014480086162372787+0j)) <f>: (np.float32(0.00025640422), np.complex128(0.0003601527714898267+0j))\n",
      "Epoch 23000: <Test loss>: 1.2331468496995512e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.180558e-05), np.complex128(0.0001484477566120611+0j)) <f>: (np.float32(0.00025579944), np.complex128(0.00035643898922070664+0j))\n",
      "Epoch 24000: <Test loss>: 1.1962119970121421e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.2030725e-05), np.complex128(0.00014817009035161132+0j)) <f>: (np.float32(0.00026557414), np.complex128(0.0003588652944089634+0j))\n",
      "Epoch 25000: <Test loss>: 1.275507383979857e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.404129e-05), np.complex128(0.0001518208387323654+0j)) <f>: (np.float32(0.00025356383), np.complex128(0.0003556331386197052+0j))\n",
      "Epoch 26000: <Test loss>: 1.2562352821987588e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.5788238e-05), np.complex128(0.0001517962436227162+0j)) <f>: (np.float32(0.00025181688), np.complex128(0.0003561808912164188+0j))\n",
      "Epoch 27000: <Test loss>: 1.2953612895216793e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.0007523e-05), np.complex128(0.00015561912605585075+0j)) <f>: (np.float32(0.0002575975), np.complex128(0.00035877519162815955+0j))\n",
      "Epoch 28000: <Test loss>: 1.3071189641777892e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.1388576e-05), np.complex128(0.00015804609726255062+0j)) <f>: (np.float32(0.00025621627), np.complex128(0.0003608682655887126+0j))\n",
      "Epoch 29000: <Test loss>: 1.383714970870642e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.127025e-05), np.complex128(0.00016115553122487448+0j)) <f>: (np.float32(0.00025633478), np.complex128(0.0003640669301648315+0j))\n",
      "Epoch 30000: <Test loss>: 1.256677569472231e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.9658071e-05), np.complex128(0.00014977365661405912+0j)) <f>: (np.float32(0.00025794693), np.complex128(0.0003570084667047313+0j))\n",
      "Epoch 31000: <Test loss>: 1.2901136869913898e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.0926323e-05), np.complex128(0.00015243298896949517+0j)) <f>: (np.float32(0.0002566787), np.complex128(0.000355517029404456+0j))\n",
      "Epoch 32000: <Test loss>: 1.3680561096407473e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.9944778e-05), np.complex128(0.00015813704049519935+0j)) <f>: (np.float32(0.00025766017), np.complex128(0.00035523378127514966+0j))\n",
      "Epoch 33000: <Test loss>: 1.6286787285935134e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.118197e-05), np.complex128(0.00017011917215666034+0j)) <f>: (np.float32(0.00022642326), np.complex128(0.0003629807492296078+0j))\n",
      "Epoch 34000: <Test loss>: 1.3533504898077808e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.0153045e-05), np.complex128(0.00015680213338663734+0j)) <f>: (np.float32(0.000247452), np.complex128(0.00035553355300487795+0j))\n",
      "Epoch 35000: <Test loss>: 1.4300243492471054e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.692054e-05), np.complex128(0.00016301545115759246+0j)) <f>: (np.float32(0.00024068444), np.complex128(0.00036228009782745407+0j))\n",
      "Epoch 36000: <Test loss>: 1.5149420505622402e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.0131814e-05), np.complex128(0.00016778002023895938+0j)) <f>: (np.float32(0.00024747322), np.complex128(0.00035949880480901867+0j))\n",
      "Epoch 37000: <Test loss>: 1.5816258382983506e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.610586e-05), np.complex128(0.00017181114443864026+0j)) <f>: (np.float32(0.00023149922), np.complex128(0.0003519812643506306+0j))\n",
      "Epoch 38000: <Test loss>: 1.4548629224009346e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.1975927e-05), np.complex128(0.00016359690111601117+0j)) <f>: (np.float32(0.00023562883), np.complex128(0.00035598968049291774+0j))\n",
      "Epoch 39000: <Test loss>: 1.4737077435711399e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.9264603e-05), np.complex128(0.00016542943500465164+0j)) <f>: (np.float32(0.00023834057), np.complex128(0.0003587007561383508+0j))\n",
      "Epoch 40000: <Test loss>: 1.4511927474814001e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.3983622e-05), np.complex128(0.00016424576165542196+0j)) <f>: (np.float32(0.0002336215), np.complex128(0.0003545266599795486+0j))\n",
      "Epoch 41000: <Test loss>: 1.5576320947729982e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.8985977e-05), np.complex128(0.00016922420194493768+0j)) <f>: (np.float32(0.00022861896), np.complex128(0.00035864281253380006+0j))\n",
      "Epoch 42000: <Test loss>: 1.487041481595952e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.9850085e-05), np.complex128(0.00016594100059928951+0j)) <f>: (np.float32(0.00022775479), np.complex128(0.00035788634244308075+0j))\n",
      "Epoch 43000: <Test loss>: 1.5517553038080223e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.1149905e-05), np.complex128(0.00017151176914846142+0j)) <f>: (np.float32(0.00022645504), np.complex128(0.0003572211485942317+0j))\n",
      "Epoch 44000: <Test loss>: 1.5862406144151464e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.5924633e-05), np.complex128(0.00017286816328060482+0j)) <f>: (np.float32(0.00023168049), np.complex128(0.00035895549233525914+0j))\n",
      "Epoch 45000: <Test loss>: 1.5641555364709347e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.7922338e-05), np.complex128(0.00017084465653219297+0j)) <f>: (np.float32(0.00022968283), np.complex128(0.00036231634825985775+0j))\n"
     ]
    }
   ],
   "source": [
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 1000\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:100*n_train:100]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_30min_sweep100.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % 1000 == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccc2fb3",
   "metadata": {},
   "source": [
    "## again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "377b7f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: (np.float32(-0.00026859494), np.complex128(4.165512866076346e-05+0j))\n",
      "2: (np.float32(-0.00026859494), np.complex128(5.771586174480367e-05+0j))\n",
      "4: (np.float32(-0.00026859494), np.complex128(7.934396848155021e-05+0j))\n",
      "5: (np.float32(-0.00026859494), np.complex128(8.761107903970432e-05+0j))\n",
      "10: (np.float32(-0.00026859494), np.complex128(0.00011723072219819025+0j))\n",
      "20: (np.float32(-0.00026859494), np.complex128(0.00015163922532935542+0j))\n",
      "50: (np.float32(-0.00026859494), np.complex128(0.0001948985297015352+0j))\n",
      "100: (np.float32(-0.00026859494), np.complex128(0.0002172294583771019+0j))\n",
      "200: (np.float32(-0.00026859494), np.complex128(0.00022576405381299875+0j))\n",
      "500: (np.float32(-0.00026859494), np.complex128(0.0002490216586118482+0j))\n",
      "1000: (np.float32(-0.00026859494), np.complex128(0.0002353523733190024+0j))\n",
      "2000: (np.float32(-0.00026859494), np.complex128(0.00021831642879988067+0j))\n",
      "2500: (np.float32(-0.00026859494), np.complex128(0.0002671575415403588+0j))\n",
      "4000: (np.float32(-0.00026859494), np.complex128(0.0001992072325838269+0j))\n",
      "5000: (np.float32(-0.00026859494), np.complex128(0.0002697519632902595+0j))\n",
      "10000: (np.float32(-0.00026859494), np.complex128(0.00024116023269016296+0j))\n"
     ]
    }
   ],
   "source": [
    "td= 16\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100, 200, 500, 1000, 2000, 2500, 4000, 5000, 10000:\n",
    "    print(f'{i}: {jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:100000]), Bs=i)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdfdd11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.0006155631), np.complex128(0.0006962008487769509+0j)),\n",
       " (np.float32(0.00060639816), np.complex128(0.0007038296747755992+0j)),\n",
       " (np.float32(0.0005234944), np.complex128(0.000707765276777578+0j)),\n",
       " (np.float32(0.0005750865), np.complex128(0.000756457381971337+0j)),\n",
       " (np.float32(0.0008116119), np.complex128(0.0009316450670469894+0j)))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td= 16\n",
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=200), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:2]), Bs=50), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:4]), Bs=25), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:8]), Bs=50), jackknife(jax.vmap(lambda x: model.observe(x, 16))(conf[:10000:td]),Bs=50)\n",
    "\n",
    "# jackknife(jax.vmap(lambda x: model.observe(x, 16))(conf[:100000:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d813c62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.0006155631), np.complex128(0.0007032223293208517+0j)),\n",
       " (np.float32(0.0006155631), np.complex128(0.0007213954068235193+0j)),\n",
       " (np.float32(0.0006155631), np.complex128(0.0007548858548493038+0j)),\n",
       " (np.float32(0.0006155631), np.complex128(0.0006237048364710063+0j)),\n",
       " (np.float32(0.0006155631), np.complex128(0.000843146990519017+0j)))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=200), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=400), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=500), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=1000), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd1b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j) for i, j in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0bf958",
   "metadata": {},
   "source": [
    "## sweep10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a47c4e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(-5.634592e-06), np.complex128(4.2392649411142764e-05+0j))\n",
      "bin size 1: (np.float32(-5.634592e-06), np.complex128(4.239264930135705e-05+0j))\n",
      "jack bin size 2: (np.float32(-5.634592e-06), np.complex128(5.224849124010339e-05+0j))\n",
      "bin size 2: (np.float32(-5.634592e-06), np.complex128(5.224849630525457e-05+0j))\n",
      "jack bin size 4: (np.float32(-5.634592e-06), np.complex128(6.234789285932936e-05+0j))\n",
      "bin size 4: (np.float32(-5.634592e-06), np.complex128(6.234789516347085e-05+0j))\n",
      "jack bin size 5: (np.float32(-5.634592e-06), np.complex128(6.52058842152237e-05+0j))\n",
      "bin size 5: (np.float32(-5.634592e-06), np.complex128(6.520588680219783e-05+0j))\n",
      "jack bin size 10: (np.float32(-5.634592e-06), np.complex128(7.199594545833373e-05+0j))\n",
      "bin size 10: (np.float32(-5.634592e-06), np.complex128(7.199595714933581e-05+0j))\n",
      "jack bin size 20: (np.float32(-5.634592e-06), np.complex128(7.59989661085593e-05+0j))\n",
      "bin size 20: (np.float32(-5.634592e-06), np.complex128(7.59989674174188e-05+0j))\n",
      "jack bin size 50: (np.float32(-5.634592e-06), np.complex128(7.701667044210575e-05+0j))\n",
      "bin size 50: (np.float32(-5.634592e-06), np.complex128(7.70166752860459e-05+0j))\n",
      "jack bin size 100: (np.float32(-5.634592e-06), np.complex128(7.787014814402024e-05+0j))\n",
      "bin size 100: (np.float32(-5.634592e-06), np.complex128(7.787014841738407e-05+0j))\n",
      "jack bin size 200: (np.float32(-5.634592e-06), np.complex128(7.875740225818406e-05+0j))\n",
      "bin size 200: (np.float32(-5.634592e-06), np.complex128(7.875741305772494e-05+0j))\n",
      "jack bin size 500: (np.float32(-5.634592e-06), np.complex128(7.910366374216169e-05+0j))\n",
      "bin size 500: (np.float32(-5.634592e-06), np.complex128(7.910367396103233e-05+0j))\n",
      "jack bin size 1000: (np.float32(-5.634592e-06), np.complex128(7.909024890882194e-05+0j))\n",
      "bin size 1000: (np.float32(-5.634592e-06), np.complex128(7.909025713551106e-05+0j))\n",
      "jack bin size 2000: (np.float32(-5.634592e-06), np.complex128(7.87022272561444e-05+0j))\n",
      "bin size 2000: (np.float32(-5.634592e-06), np.complex128(7.870222075975366e-05+0j))\n",
      "jack bin size 5000: (np.float32(-5.634592e-06), np.complex128(8.591078936379116e-05+0j))\n",
      "bin size 5000: (np.float32(-5.634592e-06), np.complex128(8.591079061570552e-05+0j))\n",
      "jack bin size 10000: (np.float32(-5.634592e-06), np.complex128(7.985310367075726e-05+0j))\n",
      "bin size 10000: (np.float32(-5.634592e-06), np.complex128(7.985310124543805e-05+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_32x32_0.1_0.5_sweep10.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model = eval(\"scalar.Model(geom=(32,), nbeta=32, nt=0, m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 16))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67ed7abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: (np.float32(-4.4554712e-05), np.complex128(0.0001379328673389478+0j))\n",
      "2: (np.float32(-4.4554712e-05), np.complex128(0.00017078208066421225+0j))\n",
      "4: (np.float32(-4.4554712e-05), np.complex128(0.00020378611089827504+0j))\n",
      "8: (np.float32(-4.4554712e-05), np.complex128(0.0002317794193758113+0j))\n",
      "10: (np.float32(-4.4554712e-05), np.complex128(0.00023268485463712405+0j))\n",
      "16: (np.float32(-4.4554712e-05), np.complex128(0.00024141567643100735+0j))\n",
      "20: (np.float32(-4.4554712e-05), np.complex128(0.00024120670102742257+0j))\n",
      "50: (np.float32(-4.4554712e-05), np.complex128(0.00025072409718850976+0j))\n",
      "100: (np.float32(-4.4554712e-05), np.complex128(0.0002481313880548667+0j))\n"
     ]
    }
   ],
   "source": [
    "for i in 1, 2, 4, 8, 10, 16, 20, 50, 100:\n",
    "    print(f\"{i}: {jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f05cf51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: (np.float32(-4.4554712e-05), np.complex128(0.00025072409718850976+0j))\n",
      "2: (np.float32(1.920552e-06), np.complex128(0.0002614483509278133+0j))\n",
      "4: (np.float32(1.5256459e-05), np.complex128(0.0003023847057193052+0j))\n",
      "8: (np.float32(0.000103535625), np.complex128(0.0004354555447185949+0j))\n",
      "10: (np.float32(-3.0640753e-05), np.complex128(0.0003478549946590766+0j))\n",
      "16: (np.float32(9.481835e-05), np.complex128(0.0004536472135272082+0j))\n",
      "20: (np.float32(0.00017021084), np.complex128(0.00048702389176469296+0j))\n",
      "50: (np.float32(0.0004121821), np.complex128(0.0007615053637882347+0j))\n",
      "100: (np.float32(0.00016102486), np.complex128(7.955558248795569e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "for i in 1, 2, 4, 8, 10, 16, 20, 50, 100:\n",
    "    print(f\"{i}: {jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:i]), Bs=50)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43d38d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(-4.4554712e-05), np.complex128(0.00025072409718850976+0j)),\n",
       " (np.float32(1.920552e-06), np.complex128(0.0002614483509278133+0j)),\n",
       " (np.float32(1.5256459e-05), np.complex128(0.0003023847057193052+0j)),\n",
       " (np.float32(0.000103535625), np.complex128(0.0004354555447185949+0j)))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td= 16\n",
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=50), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:2]), Bs=50), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:4]), Bs=50), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:8]), Bs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dddf1a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j) for i, j in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59ce2fcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00017738199676387012 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 4.884750069322763e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00018545009), np.complex128(5.992152867975136e-05+0j)) <f>: (np.float32(-1.3157032e-05), np.complex128(0.0003487322995244748+0j))\n",
      "Epoch 200: <Test loss>: 3.1723993743071333e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000122063895), np.complex128(5.226491326301323e-05+0j)) <f>: (np.float32(5.0229046e-05), np.complex128(0.0003108785070069292+0j))\n",
      "Epoch 300: <Test loss>: 4.085505224793451e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00015430675), np.complex128(6.350894584247738e-05+0j)) <f>: (np.float32(1.7986316e-05), np.complex128(0.000327278767156265+0j))\n",
      "Epoch 400: <Test loss>: 3.0641715511592338e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.133828e-05), np.complex128(5.303982175717846e-05+0j)) <f>: (np.float32(9.095477e-05), np.complex128(0.0003266086257418386+0j))\n",
      "Epoch 500: <Test loss>: 2.270246568514267e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.251009e-05), np.complex128(4.0150105177530666e-05+0j)) <f>: (np.float32(7.978272e-05), np.complex128(0.0003252887674785745+0j))\n",
      "Epoch 600: <Test loss>: 2.4956177639978705e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.351402e-05), np.complex128(4.4565315870321406e-05+0j)) <f>: (np.float32(7.877892e-05), np.complex128(0.0003387155090007272+0j))\n",
      "Epoch 700: <Test loss>: 1.9653532490337966e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000117795236), np.complex128(4.6559386982185004e-05+0j)) <f>: (np.float32(5.4497654e-05), np.complex128(0.0003309118026179487+0j))\n",
      "Epoch 800: <Test loss>: 2.11278825190675e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000112981754), np.complex128(3.86575102737709e-05+0j)) <f>: (np.float32(5.9311245e-05), np.complex128(0.00033472218927648394+0j))\n",
      "Epoch 900: <Test loss>: 5.370623512135353e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(1.3019048e-05), np.complex128(7.135624075372026e-05+0j)) <f>: (np.float32(0.00015927388), np.complex128(0.00034238802984153826+0j))\n",
      "Epoch 1000: <Test loss>: 1.8836362869478762e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.247489e-05), np.complex128(4.4317430148827937e-05+0j)) <f>: (np.float32(9.981804e-05), np.complex128(0.000346405009078094+0j))\n",
      "Epoch 1100: <Test loss>: 1.8064051801047754e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.340364e-05), np.complex128(3.33661721410439e-05+0j)) <f>: (np.float32(8.888925e-05), np.complex128(0.00032554851467138813+0j))\n",
      "Epoch 1200: <Test loss>: 1.8633191984918085e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.983533e-05), np.complex128(4.077572643055146e-05+0j)) <f>: (np.float32(8.245767e-05), np.complex128(0.00032625579454280905+0j))\n",
      "Epoch 1300: <Test loss>: 2.3331624561251374e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.627643e-05), np.complex128(4.446856083387742e-05+0j)) <f>: (np.float32(9.6016556e-05), np.complex128(0.0003103271388815244+0j))\n",
      "Epoch 1400: <Test loss>: 1.562396619192441e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.3808886e-05), np.complex128(3.334545024579303e-05+0j)) <f>: (np.float32(9.848399e-05), np.complex128(0.0003308224927162438+0j))\n",
      "Epoch 1500: <Test loss>: 1.6192832390515832e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.634259e-05), np.complex128(4.137660192689425e-05+0j)) <f>: (np.float32(9.595036e-05), np.complex128(0.0003350230076066228+0j))\n",
      "Epoch 1600: <Test loss>: 1.7715676676743897e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010428334), np.complex128(4.504369947466853e-05+0j)) <f>: (np.float32(6.800951e-05), np.complex128(0.0003339268046795885+0j))\n",
      "Epoch 1700: <Test loss>: 1.584781671226665e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.3767794e-05), np.complex128(3.861974147789276e-05+0j)) <f>: (np.float32(8.852522e-05), np.complex128(0.00033514200290179217+0j))\n",
      "Epoch 1800: <Test loss>: 1.698053551990597e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.084278e-05), np.complex128(3.8721892056605054e-05+0j)) <f>: (np.float32(9.145001e-05), np.complex128(0.0003394050283803318+0j))\n",
      "Epoch 1900: <Test loss>: 1.5690169448134839e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.5680564e-05), np.complex128(3.686361338362189e-05+0j)) <f>: (np.float32(0.00010661229), np.complex128(0.0003328275888119782+0j))\n",
      "Epoch 2000: <Test loss>: 2.064226691800286e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.74389e-05), np.complex128(4.120985945238659e-05+0j)) <f>: (np.float32(8.485397e-05), np.complex128(0.0003289253550386215+0j))\n",
      "Epoch 2100: <Test loss>: 1.747529836393369e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.4541436e-05), np.complex128(4.1741913042941145e-05+0j)) <f>: (np.float32(9.775142e-05), np.complex128(0.00033240187616617387+0j))\n",
      "Epoch 2200: <Test loss>: 2.139834805348073e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000118863674), np.complex128(4.1944711694473235e-05+0j)) <f>: (np.float32(5.3429412e-05), np.complex128(0.0003154779986598962+0j))\n",
      "Epoch 2300: <Test loss>: 1.6775660469647846e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.3316525e-05), np.complex128(4.018759250132868e-05+0j)) <f>: (np.float32(0.00010897642), np.complex128(0.00032774447262382006+0j))\n",
      "Epoch 2400: <Test loss>: 1.6993793678921065e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.435337e-05), np.complex128(4.383179962951648e-05+0j)) <f>: (np.float32(9.7939526e-05), np.complex128(0.0003407551111948476+0j))\n",
      "Epoch 2500: <Test loss>: 1.8350436903347145e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.105692e-05), np.complex128(4.380428276038767e-05+0j)) <f>: (np.float32(0.000101236066), np.complex128(0.0003348503185388731+0j))\n",
      "Epoch 2600: <Test loss>: 1.7959783917831373e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(4.3550812e-05), np.complex128(4.503618298081055e-05+0j)) <f>: (np.float32(0.00012874209), np.complex128(0.00034090620223594216+0j))\n",
      "Epoch 2700: <Test loss>: 1.8883937400460127e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.3186744e-05), np.complex128(4.404045369319426e-05+0j)) <f>: (np.float32(0.00010910616), np.complex128(0.00032756724828762436+0j))\n",
      "Epoch 2800: <Test loss>: 1.6869490764293005e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(4.5096873e-05), np.complex128(4.218347930632886e-05+0j)) <f>: (np.float32(0.00012719605), np.complex128(0.00033244538937112384+0j))\n",
      "Epoch 2900: <Test loss>: 1.7521182371638133e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.56626e-05), np.complex128(3.980351790140531e-05+0j)) <f>: (np.float32(0.00010663043), np.complex128(0.0003320556734364115+0j))\n",
      "Epoch 3000: <Test loss>: 1.7837696759670507e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.169396e-05), np.complex128(4.2837124871101784e-05+0j)) <f>: (np.float32(0.00011059889), np.complex128(0.00033509601591405345+0j))\n",
      "Epoch 3100: <Test loss>: 1.9641220205812715e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.119646e-05), np.complex128(4.528703010573857e-05+0j)) <f>: (np.float32(8.109637e-05), np.complex128(0.000336777046464468+0j))\n",
      "Epoch 3200: <Test loss>: 2.0065513126610313e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.3298205e-05), np.complex128(4.785642618498153e-05+0j)) <f>: (np.float32(0.00010899488), np.complex128(0.0003317961482497455+0j))\n",
      "Epoch 3300: <Test loss>: 1.8554400185166742e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.339907e-05), np.complex128(4.438042439323894e-05+0j)) <f>: (np.float32(9.8893805e-05), np.complex128(0.00033840860135910845+0j))\n",
      "Epoch 3400: <Test loss>: 1.96486757886305e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.376922e-05), np.complex128(4.0311872335692465e-05+0j)) <f>: (np.float32(8.852376e-05), np.complex128(0.0003324588048854779+0j))\n",
      "Epoch 3500: <Test loss>: 1.9729761788767064e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(5.3289397e-05), np.complex128(4.434022938731804e-05+0j)) <f>: (np.float32(0.000119003606), np.complex128(0.0003371458621061322+0j))\n",
      "Epoch 3600: <Test loss>: 1.976969542738516e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.4049e-05), np.complex128(4.274391003983458e-05+0j)) <f>: (np.float32(9.824389e-05), np.complex128(0.00033368281992326185+0j))\n",
      "Epoch 3700: <Test loss>: 2.0804702671739506e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.881399e-05), np.complex128(4.289642429891172e-05+0j)) <f>: (np.float32(0.00010347899), np.complex128(0.00033018238556208534+0j))\n",
      "Epoch 3800: <Test loss>: 2.1017040126025677e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.691465e-05), np.complex128(4.329276870289123e-05+0j)) <f>: (np.float32(9.537821e-05), np.complex128(0.00034022622912068884+0j))\n",
      "Epoch 3900: <Test loss>: 2.21942877942638e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.68657e-05), np.complex128(4.312135220609529e-05+0j)) <f>: (np.float32(8.5427244e-05), np.complex128(0.00033906434408909807+0j))\n",
      "Epoch 4000: <Test loss>: 2.1422138161142357e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.239782e-05), np.complex128(4.267931417964371e-05+0j)) <f>: (np.float32(9.989529e-05), np.complex128(0.0003337800903311201+0j))\n",
      "Epoch 4100: <Test loss>: 2.1758262391813332e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.7346155e-05), np.complex128(4.400968998415528e-05+0j)) <f>: (np.float32(0.000104946856), np.complex128(0.0003363622438350651+0j))\n",
      "Epoch 4200: <Test loss>: 2.25286794375279e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.987804e-05), np.complex128(4.305727171731874e-05+0j)) <f>: (np.float32(0.00010241498), np.complex128(0.00033406264072681935+0j))\n",
      "Epoch 4300: <Test loss>: 2.3522154606325785e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.2660404e-05), np.complex128(4.362358957813411e-05+0j)) <f>: (np.float32(7.9632344e-05), np.complex128(0.0003346323719321557+0j))\n",
      "Epoch 4400: <Test loss>: 2.320889507245738e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.6547094e-05), np.complex128(4.682926320548765e-05+0j)) <f>: (np.float32(0.00010574575), np.complex128(0.0003353489443466162+0j))\n",
      "Epoch 4500: <Test loss>: 2.3390603018924594e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.916947e-05), np.complex128(4.546094813609167e-05+0j)) <f>: (np.float32(0.00010312349), np.complex128(0.00033749913731745716+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48fda0f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00017738199676387012 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 200: <Test loss>: 6.879963166284142e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.94477e-05), np.complex128(9.631093693187756e-05+0j)) <f>: (np.float32(7.2845236e-05), np.complex128(0.00034295424066367526+0j))\n",
      "Epoch 400: <Test loss>: 4.915495082968846e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000103349295), np.complex128(6.276928875305115e-05+0j)) <f>: (np.float32(6.89438e-05), np.complex128(0.0003076467000844745+0j))\n",
      "Epoch 600: <Test loss>: 4.301931767258793e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012542585), np.complex128(5.971339395188339e-05+0j)) <f>: (np.float32(4.6866968e-05), np.complex128(0.00030887061997676653+0j))\n",
      "Epoch 800: <Test loss>: 1.3974373359815218e-05 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00032848152), np.complex128(9.890928102702293e-05+0j)) <f>: (np.float32(-0.00015618846), np.complex128(0.00033696289732526016+0j))\n",
      "Epoch 1000: <Test loss>: 4.01945226258249e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00015223434), np.complex128(5.645904944104125e-05+0j)) <f>: (np.float32(2.0058485e-05), np.complex128(0.0003301005921542384+0j))\n",
      "Epoch 1200: <Test loss>: 3.2017430839914596e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00015354285), np.complex128(5.0961443810724284e-05+0j)) <f>: (np.float32(1.8749983e-05), np.complex128(0.00032360183790766385+0j))\n",
      "Epoch 1400: <Test loss>: 2.99138400805532e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00013479579), np.complex128(4.320074319027243e-05+0j)) <f>: (np.float32(3.749706e-05), np.complex128(0.00033074701062602447+0j))\n",
      "Epoch 1600: <Test loss>: 3.111893647655961e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.0001370284), np.complex128(5.1137530365412986e-05+0j)) <f>: (np.float32(3.5264573e-05), np.complex128(0.00033050797343527516+0j))\n",
      "Epoch 1800: <Test loss>: 3.1418560411111685e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00014259457), np.complex128(4.3627605510770255e-05+0j)) <f>: (np.float32(2.9698422e-05), np.complex128(0.00033491612750408535+0j))\n",
      "Epoch 2000: <Test loss>: 3.019017185579287e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00013686942), np.complex128(4.525136640386813e-05+0j)) <f>: (np.float32(3.5423258e-05), np.complex128(0.000331000858798342+0j))\n",
      "Epoch 2200: <Test loss>: 3.02110674965661e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012196957), np.complex128(4.3671328828681456e-05+0j)) <f>: (np.float32(5.0323342e-05), np.complex128(0.000331095338271772+0j))\n",
      "Epoch 2400: <Test loss>: 3.0117575988697354e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010782039), np.complex128(4.69735909878682e-05+0j)) <f>: (np.float32(6.447242e-05), np.complex128(0.00032725707398411795+0j))\n",
      "Epoch 2600: <Test loss>: 3.0615021842095302e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.80825e-05), np.complex128(4.4160174472739124e-05+0j)) <f>: (np.float32(7.421041e-05), np.complex128(0.0003318935772334236+0j))\n",
      "Epoch 2800: <Test loss>: 3.846197614620905e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00014498919), np.complex128(5.106009382821474e-05+0j)) <f>: (np.float32(2.7303755e-05), np.complex128(0.00031713289591520956+0j))\n",
      "Epoch 3000: <Test loss>: 3.101192532994901e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011053366), np.complex128(4.1645875562081705e-05+0j)) <f>: (np.float32(6.175919e-05), np.complex128(0.00032948398593657255+0j))\n",
      "Epoch 3200: <Test loss>: 3.21291872751317e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012595797), np.complex128(4.267841426186641e-05+0j)) <f>: (np.float32(4.6334935e-05), np.complex128(0.0003303272921462081+0j))\n",
      "Epoch 3400: <Test loss>: 3.128588787149056e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.57343e-05), np.complex128(4.375574270195039e-05+0j)) <f>: (np.float32(7.655871e-05), np.complex128(0.000331481533823285+0j))\n",
      "Epoch 3600: <Test loss>: 3.4318600228289142e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012288326), np.complex128(4.603230077918533e-05+0j)) <f>: (np.float32(4.9409606e-05), np.complex128(0.0003313144583395558+0j))\n",
      "Epoch 3800: <Test loss>: 3.111709020231501e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.2247545e-05), np.complex128(3.8586979713524445e-05+0j)) <f>: (np.float32(8.004549e-05), np.complex128(0.0003288026173541051+0j))\n",
      "Epoch 4000: <Test loss>: 3.4008498914772645e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011144025), np.complex128(4.124003643089235e-05+0j)) <f>: (np.float32(6.0852566e-05), np.complex128(0.00032827934886396685+0j))\n",
      "Epoch 4200: <Test loss>: 3.2429982184112305e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010450352), np.complex128(3.779391032359793e-05+0j)) <f>: (np.float32(6.778934e-05), np.complex128(0.0003318675707989783+0j))\n",
      "Epoch 4400: <Test loss>: 3.2169402857107343e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010694687), np.complex128(4.145712672818288e-05+0j)) <f>: (np.float32(6.53461e-05), np.complex128(0.0003318469876575697+0j))\n",
      "Epoch 4600: <Test loss>: 3.325963916722685e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000102478814), np.complex128(4.187962424924102e-05+0j)) <f>: (np.float32(6.981419e-05), np.complex128(0.0003337769188147244+0j))\n",
      "Epoch 4800: <Test loss>: 3.401474032216356e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(8.815548e-05), np.complex128(4.030361053548148e-05+0j)) <f>: (np.float32(8.413745e-05), np.complex128(0.0003349472083647639+0j))\n",
      "Epoch 5000: <Test loss>: 3.692842710734112e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012641937), np.complex128(4.7665956803438e-05+0j)) <f>: (np.float32(4.587356e-05), np.complex128(0.0003373670436595733+0j))\n",
      "Epoch 5200: <Test loss>: 3.6360440844873665e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000109903856), np.complex128(4.2401457628210204e-05+0j)) <f>: (np.float32(6.238923e-05), np.complex128(0.0003407653869079699+0j))\n",
      "Epoch 5400: <Test loss>: 4.5441797738021705e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.093049e-05), np.complex128(5.1551512364948484e-05+0j)) <f>: (np.float32(0.000101362486), np.complex128(0.0003308110435420551+0j))\n",
      "Epoch 5600: <Test loss>: 3.5105108509014826e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.329669e-05), np.complex128(4.182786906605753e-05+0j)) <f>: (np.float32(7.899617e-05), np.complex128(0.00033526971986704983+0j))\n",
      "Epoch 5800: <Test loss>: 3.6608455502573634e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011122813), np.complex128(4.2812438580356205e-05+0j)) <f>: (np.float32(6.106501e-05), np.complex128(0.0003340477663149232+0j))\n",
      "Epoch 6000: <Test loss>: 3.55099473381415e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.274595e-05), np.complex128(4.023017010894191e-05+0j)) <f>: (np.float32(7.954686e-05), np.complex128(0.0003362966568760005+0j))\n",
      "Epoch 6200: <Test loss>: 3.729555373865878e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011287315), np.complex128(4.694490462206845e-05+0j)) <f>: (np.float32(5.94199e-05), np.complex128(0.00033471698798959485+0j))\n",
      "Epoch 6400: <Test loss>: 3.75255876861047e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010175397), np.complex128(4.7516800387344886e-05+0j)) <f>: (np.float32(7.053893e-05), np.complex128(0.000337814227471377+0j))\n",
      "Epoch 6600: <Test loss>: 3.7322881780710304e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.9479556e-05), np.complex128(4.654741847218646e-05+0j)) <f>: (np.float32(9.281332e-05), np.complex128(0.0003382598572401468+0j))\n",
      "Epoch 6800: <Test loss>: 3.721504981513135e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.527136e-05), np.complex128(4.69882909663626e-05+0j)) <f>: (np.float32(7.702157e-05), np.complex128(0.00033706714506918917+0j))\n",
      "Epoch 7000: <Test loss>: 4.00855105908704e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00014493996), np.complex128(5.2277535898268396e-05+0j)) <f>: (np.float32(2.7353013e-05), np.complex128(0.00034126867484481484+0j))\n",
      "Epoch 7200: <Test loss>: 3.914758963219356e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.163118e-05), np.complex128(4.762681632671869e-05+0j)) <f>: (np.float32(8.06618e-05), np.complex128(0.00034139737498015523+0j))\n",
      "Epoch 7400: <Test loss>: 4.112583610549336e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.9848185e-05), np.complex128(4.5774313777971324e-05+0j)) <f>: (np.float32(9.244469e-05), np.complex128(0.00033663914893157986+0j))\n",
      "Epoch 7600: <Test loss>: 4.135251856496325e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.2019014e-05), np.complex128(4.9778182758626046e-05+0j)) <f>: (np.float32(8.027389e-05), np.complex128(0.0003410603379327766+0j))\n",
      "Epoch 7800: <Test loss>: 4.309807081881445e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.480355e-05), np.complex128(5.806731351215028e-05+0j)) <f>: (np.float32(9.748939e-05), np.complex128(0.00034322410499379143+0j))\n",
      "Epoch 8000: <Test loss>: 4.199285285721999e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.6329154e-05), np.complex128(5.2924545064983176e-05+0j)) <f>: (np.float32(9.59637e-05), np.complex128(0.0003424784497739817+0j))\n",
      "Epoch 8200: <Test loss>: 4.378413905214984e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.844314e-05), np.complex128(5.370725152508651e-05+0j)) <f>: (np.float32(9.384971e-05), np.complex128(0.00034241241880262176+0j))\n",
      "Epoch 8400: <Test loss>: 4.420757704792777e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.143504e-05), np.complex128(5.229997437676848e-05+0j)) <f>: (np.float32(8.085796e-05), np.complex128(0.0003430053655079751+0j))\n",
      "Epoch 8600: <Test loss>: 4.539160727290437e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.495824e-05), np.complex128(5.495432745146315e-05+0j)) <f>: (np.float32(9.733457e-05), np.complex128(0.0003446303870788403+0j))\n",
      "Epoch 8800: <Test loss>: 4.638036898541031e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.800108e-05), np.complex128(5.5509612399616735e-05+0j)) <f>: (np.float32(9.4291776e-05), np.complex128(0.00034328718645490334+0j))\n",
      "Epoch 9000: <Test loss>: 4.795706900040386e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.844256e-05), np.complex128(5.877056952214406e-05+0j)) <f>: (np.float32(9.3850336e-05), np.complex128(0.0003478875978476251+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b4f3a011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00017738199676387012 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 1.5499184883083217e-05 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(-0.00014563023), np.complex128(0.00011132129587834868+0j)) <f>: (np.float32(0.00031792317), np.complex128(0.0003248509396401383+0j))\n",
      "Epoch 800: <Test loss>: 6.907230726937996e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(-9.391424e-06), np.complex128(6.062942699686945e-05+0j)) <f>: (np.float32(0.00018168429), np.complex128(0.00032820675285366764+0j))\n",
      "Epoch 1200: <Test loss>: 2.5364470275235362e-05 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010276806), np.complex128(0.00017170673811889146+0j)) <f>: (np.float32(6.952493e-05), np.complex128(0.0004279910631830748+0j))\n",
      "Epoch 1600: <Test loss>: 4.692985839938046e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(4.1992476e-05), np.complex128(5.4939009027271576e-05+0j)) <f>: (np.float32(0.00013030048), np.complex128(0.0003278438045173356+0j))\n",
      "Epoch 2000: <Test loss>: 5.737489573220955e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.245033e-05), np.complex128(6.43650134412104e-05+0j)) <f>: (np.float32(0.000109842644), np.complex128(0.0003145373268969105+0j))\n",
      "Epoch 2400: <Test loss>: 4.671459464589134e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(7.94411e-05), np.complex128(6.773118958456293e-05+0j)) <f>: (np.float32(9.285184e-05), np.complex128(0.0003349213922213023+0j))\n",
      "Epoch 2800: <Test loss>: 4.0659410842636134e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.674079e-05), np.complex128(5.775296866458343e-05+0j)) <f>: (np.float32(0.00010555212), np.complex128(0.0003350844398792089+0j))\n",
      "Epoch 3200: <Test loss>: 4.357377292762976e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(6.0702638e-05), np.complex128(5.983189766201138e-05+0j)) <f>: (np.float32(0.0001115903), np.complex128(0.00033811713900233707+0j))\n",
      "Epoch 3600: <Test loss>: 4.189374521956779e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010413013), np.complex128(5.37605012853715e-05+0j)) <f>: (np.float32(6.81628e-05), np.complex128(0.00033239255190797027+0j))\n",
      "Epoch 4000: <Test loss>: 4.227129011269426e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.051564e-05), np.complex128(5.8757447373056555e-05+0j)) <f>: (np.float32(8.177735e-05), np.complex128(0.0003393391242696277+0j))\n",
      "Epoch 4400: <Test loss>: 4.265542429493507e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.8460405e-05), np.complex128(6.0233831863638436e-05+0j)) <f>: (np.float32(7.383238e-05), np.complex128(0.00033753938386051947+0j))\n",
      "Epoch 4800: <Test loss>: 4.260712557879742e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.385897e-05), np.complex128(6.297143724372211e-05+0j)) <f>: (np.float32(7.843381e-05), np.complex128(0.00034119141670541385+0j))\n",
      "Epoch 5200: <Test loss>: 4.636046924133552e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000108828644), np.complex128(6.589169818182921e-05+0j)) <f>: (np.float32(6.346412e-05), np.complex128(0.0003492346677215649+0j))\n",
      "Epoch 5600: <Test loss>: 4.402120339364046e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.541115e-05), np.complex128(5.858380685038809e-05+0j)) <f>: (np.float32(7.688186e-05), np.complex128(0.00034112113590208353+0j))\n",
      "Epoch 6000: <Test loss>: 4.6630334509245586e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.163581e-05), np.complex128(6.522868475050228e-05+0j)) <f>: (np.float32(8.065685e-05), np.complex128(0.00034433959074051205+0j))\n",
      "Epoch 6400: <Test loss>: 4.6588038458139636e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.507082e-05), np.complex128(6.123350176037647e-05+0j)) <f>: (np.float32(7.722218e-05), np.complex128(0.0003377040489917879+0j))\n",
      "Epoch 6800: <Test loss>: 4.61187892142334e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010123375), np.complex128(6.258035755695957e-05+0j)) <f>: (np.float32(7.1059265e-05), np.complex128(0.0003391498164559641+0j))\n",
      "Epoch 7200: <Test loss>: 4.8786482693685684e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.231477e-05), np.complex128(6.166594198533536e-05+0j)) <f>: (np.float32(7.9978134e-05), np.complex128(0.0003380752749859129+0j))\n",
      "Epoch 7600: <Test loss>: 5.090320428280393e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011990819), np.complex128(6.177516108121472e-05+0j)) <f>: (np.float32(5.238486e-05), np.complex128(0.00033955145729232456+0j))\n",
      "Epoch 8000: <Test loss>: 5.021054676035419e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000105093844), np.complex128(6.982222584583172e-05+0j)) <f>: (np.float32(6.7199166e-05), np.complex128(0.0003427871334647821+0j))\n",
      "Epoch 8400: <Test loss>: 5.055253950558836e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000114703784), np.complex128(6.802287394748199e-05+0j)) <f>: (np.float32(5.7589135e-05), np.complex128(0.0003390627583309002+0j))\n",
      "Epoch 8800: <Test loss>: 5.223513198870933e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012545277), np.complex128(6.920664244220355e-05+0j)) <f>: (np.float32(4.6840123e-05), np.complex128(0.0003413920785477743+0j))\n",
      "Epoch 9200: <Test loss>: 5.4531669775315095e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011718386), np.complex128(6.943569728509712e-05+0j)) <f>: (np.float32(5.5109125e-05), np.complex128(0.0003390338023862068+0j))\n",
      "Epoch 9600: <Test loss>: 5.341523319657426e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012844706), np.complex128(7.03745929989012e-05+0j)) <f>: (np.float32(4.3845757e-05), np.complex128(0.0003417085958840722+0j))\n",
      "Epoch 10000: <Test loss>: 5.7203774304070976e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012981782), np.complex128(7.377035146143528e-05+0j)) <f>: (np.float32(4.2475076e-05), np.complex128(0.0003333435628144062+0j))\n",
      "Epoch 10400: <Test loss>: 6.035585101926699e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012082286), np.complex128(7.949205247588193e-05+0j)) <f>: (np.float32(5.1469826e-05), np.complex128(0.000352573418176885+0j))\n",
      "Epoch 10800: <Test loss>: 5.535038326343056e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012606758), np.complex128(7.316379895074407e-05+0j)) <f>: (np.float32(4.6225254e-05), np.complex128(0.0003381837091314843+0j))\n",
      "Epoch 11200: <Test loss>: 6.029412361385766e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011835147), np.complex128(6.959495498091077e-05+0j)) <f>: (np.float32(5.394133e-05), np.complex128(0.00033612745647628616+0j))\n",
      "Epoch 11600: <Test loss>: 5.890154625376454e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00013242835), np.complex128(7.618533433615913e-05+0j)) <f>: (np.float32(3.9864688e-05), np.complex128(0.0003427821541840407+0j))\n",
      "Epoch 12000: <Test loss>: 5.936862180533353e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00013293231), np.complex128(7.569244104430136e-05+0j)) <f>: (np.float32(3.9360522e-05), np.complex128(0.00034458589070380765+0j))\n",
      "Epoch 12400: <Test loss>: 6.1246664699865505e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011997287), np.complex128(7.923865624465081e-05+0j)) <f>: (np.float32(5.2320043e-05), np.complex128(0.0003399204315098086+0j))\n",
      "Epoch 12800: <Test loss>: 6.158394626254449e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012687962), np.complex128(8.089740689238503e-05+0j)) <f>: (np.float32(4.541325e-05), np.complex128(0.00034599337796508686+0j))\n",
      "Epoch 13200: <Test loss>: 6.530478458444122e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012419918), np.complex128(8.070074116068326e-05+0j)) <f>: (np.float32(4.8093738e-05), np.complex128(0.00033883840526106337+0j))\n",
      "Epoch 13600: <Test loss>: 6.324120931822108e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00013194393), np.complex128(8.03935401537979e-05+0j)) <f>: (np.float32(4.0349143e-05), np.complex128(0.00033952830522263543+0j))\n",
      "Epoch 14000: <Test loss>: 6.4069131440191995e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011314587), np.complex128(8.157702321204383e-05+0j)) <f>: (np.float32(5.9147158e-05), np.complex128(0.000340581565817671+0j))\n",
      "Epoch 14400: <Test loss>: 6.470382231782423e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011150424), np.complex128(7.794519292659053e-05+0j)) <f>: (np.float32(6.078866e-05), np.complex128(0.0003349885966537287+0j))\n",
      "Epoch 14800: <Test loss>: 6.416174073820002e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011870265), np.complex128(7.872700343331206e-05+0j)) <f>: (np.float32(5.3590164e-05), np.complex128(0.0003407680192665784+0j))\n",
      "Epoch 15200: <Test loss>: 6.5993690441246144e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010702022), np.complex128(8.272293172979087e-05+0j)) <f>: (np.float32(6.527257e-05), np.complex128(0.0003397854200568406+0j))\n",
      "Epoch 15600: <Test loss>: 6.616847713303287e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(9.588698e-05), np.complex128(8.282967704288153e-05+0j)) <f>: (np.float32(7.640595e-05), np.complex128(0.00033801365242234306+0j))\n",
      "Epoch 16000: <Test loss>: 6.843456503702328e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000112159425), np.complex128(8.504511603477446e-05+0j)) <f>: (np.float32(6.013364e-05), np.complex128(0.00034008236913697664+0j))\n",
      "Epoch 16400: <Test loss>: 6.841004960733699e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000113282025), np.complex128(8.353873296348408e-05+0j)) <f>: (np.float32(5.9010865e-05), np.complex128(0.00033979150936832046+0j))\n",
      "Epoch 16800: <Test loss>: 6.929179107828531e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00012135505), np.complex128(8.666243874958891e-05+0j)) <f>: (np.float32(5.093789e-05), np.complex128(0.00034215051497865894+0j))\n",
      "Epoch 17200: <Test loss>: 7.123356226657052e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00011707139), np.complex128(8.821758388546428e-05+0j)) <f>: (np.float32(5.5221502e-05), np.complex128(0.00034405380539808913+0j))\n",
      "Epoch 17600: <Test loss>: 7.306393399630906e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.00010567279), np.complex128(8.920628033547295e-05+0j)) <f>: (np.float32(6.661992e-05), np.complex128(0.00033967984027602537+0j))\n",
      "Epoch 18000: <Test loss>: 7.120916507119546e-06 <O>: (np.float32(0.00017229297), np.complex128(0.00033261509721346154+0j)) <O-f>: (np.float32(0.000105710074), np.complex128(8.720012178174596e-05+0j)) <f>: (np.float32(6.65829e-05), np.complex128(0.0003419350738698942+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3896356b",
   "metadata": {},
   "source": [
    "## sweep1 - bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f057bf0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.00065247447), np.complex128(4.173847599230104e-05+0j))\n",
      "bin size 1: (np.float32(0.00065247447), np.complex128(4.173851268769346e-05+0j))\n",
      "jack bin size 2: (np.float32(0.00065247447), np.complex128(5.783930561343171e-05+0j))\n",
      "bin size 2: (np.float32(0.00065247447), np.complex128(5.783936656696748e-05+0j))\n",
      "jack bin size 4: (np.float32(0.00065247447), np.complex128(7.956363328698433e-05+0j))\n",
      "bin size 4: (np.float32(0.00065247447), np.complex128(7.956354803482777e-05+0j))\n",
      "jack bin size 5: (np.float32(0.00065247447), np.complex128(8.786031807790393e-05+0j))\n",
      "bin size 5: (np.float32(0.00065247447), np.complex128(8.786031180461224e-05+0j))\n",
      "jack bin size 10: (np.float32(0.00065247447), np.complex128(0.00011768341191849774+0j))\n",
      "bin size 10: (np.float32(0.00065247447), np.complex128(0.00011768340900342145+0j))\n",
      "jack bin size 20: (np.float32(0.00065247447), np.complex128(0.00015181776692822145+0j))\n",
      "bin size 20: (np.float32(0.00065247447), np.complex128(0.00015181779172580246+0j))\n",
      "jack bin size 50: (np.float32(0.00065247447), np.complex128(0.0001963028088756356+0j))\n",
      "bin size 50: (np.float32(0.00065247447), np.complex128(0.00019630293747779703+0j))\n",
      "jack bin size 100: (np.float32(0.00065247447), np.complex128(0.00021548487030421303+0j))\n",
      "bin size 100: (np.float32(0.00065247447), np.complex128(0.00021548499167774909+0j))\n",
      "jack bin size 200: (np.float32(0.00065247447), np.complex128(0.00023154763350894926+0j))\n",
      "bin size 200: (np.float32(0.00065247447), np.complex128(0.00023154768607146496+0j))\n",
      "jack bin size 500: (np.float32(0.00065247447), np.complex128(0.00023855717570008127+0j))\n",
      "bin size 500: (np.float32(0.00065247447), np.complex128(0.00023855681375219598+0j))\n",
      "jack bin size 1000: (np.float32(0.00065247447), np.complex128(0.0002403005622864113+0j))\n",
      "bin size 1000: (np.float32(0.00065247447), np.complex128(0.0002403006617379419+0j))\n",
      "jack bin size 2000: (np.float32(0.00065247447), np.complex128(0.0002626718305691611+0j))\n",
      "bin size 2000: (np.float32(0.00065247447), np.complex128(0.0002626718653898154+0j))\n",
      "jack bin size 5000: (np.float32(0.00065247447), np.complex128(0.0002656244408497292+0j))\n",
      "bin size 5000: (np.float32(0.00065247447), np.complex128(0.0002656245802295286+0j))\n",
      "jack bin size 10000: (np.float32(0.00065247447), np.complex128(0.00027614378632279113+0j))\n",
      "bin size 10000: (np.float32(0.00065247447), np.complex128(0.0002761437790468335+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_32x32_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(32,32), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e1885",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d32bc0bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00017851342272479087 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 4.838381300942274e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.9426132e-05), np.complex128(6.147896127182718e-05+0j)) <f>: (np.float32(0.00058373593), np.complex128(0.0005166570519141552+0j))\n",
      "Epoch 200: <Test loss>: 9.378227332490496e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0001635133), np.complex128(9.564701961837783e-05+0j)) <f>: (np.float32(0.00044964981), np.complex128(0.00044528328060355725+0j))\n",
      "Epoch 300: <Test loss>: 2.9184134291426744e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(5.9896905e-05), np.complex128(4.626440820660983e-05+0j)) <f>: (np.float32(0.00055326545), np.complex128(0.0004878000585323132+0j))\n",
      "Epoch 400: <Test loss>: 2.991587052747491e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.79223e-05), np.complex128(6.13440211779781e-05+0j)) <f>: (np.float32(0.00059523946), np.complex128(0.0005082671541559463+0j))\n",
      "Epoch 500: <Test loss>: 2.4189735086110886e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(3.2630178e-05), np.complex128(4.90984554358934e-05+0j)) <f>: (np.float32(0.0005805325), np.complex128(0.0005003721081009696+0j))\n",
      "Epoch 600: <Test loss>: 3.601775006245589e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(5.8041915e-05), np.complex128(5.5696264068298854e-05+0j)) <f>: (np.float32(0.0005551209), np.complex128(0.0004990138744893167+0j))\n",
      "Epoch 700: <Test loss>: 2.3629149836779106e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.7285707e-05), np.complex128(6.400729810693138e-05+0j)) <f>: (np.float32(0.0005858768), np.complex128(0.0005058601952177038+0j))\n",
      "Epoch 800: <Test loss>: 2.7885494091606233e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.5332965e-05), np.complex128(5.723115510759172e-05+0j)) <f>: (np.float32(0.0005878294), np.complex128(0.0004912912003504501+0j))\n",
      "Epoch 900: <Test loss>: 2.458382141412585e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.589314e-05), np.complex128(6.229335118072427e-05+0j)) <f>: (np.float32(0.0005672693), np.complex128(0.00048689284626730287+0j))\n",
      "Epoch 1000: <Test loss>: 2.053789785350091e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.3443252e-05), np.complex128(5.3862576540569396e-05+0j)) <f>: (np.float32(0.00058971916), np.complex128(0.0004979010528163685+0j))\n",
      "Epoch 1100: <Test loss>: 2.1446976461447775e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.8433902e-05), np.complex128(5.190159623790018e-05+0j)) <f>: (np.float32(0.0005947286), np.complex128(0.0005010919788924817+0j))\n",
      "Epoch 1200: <Test loss>: 2.155110450985376e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.3435924e-05), np.complex128(5.7014718935557815e-05+0j)) <f>: (np.float32(0.0005997264), np.complex128(0.0004950200473224501+0j))\n",
      "Epoch 1300: <Test loss>: 2.1410980934888357e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.5306596e-05), np.complex128(5.846271835439755e-05+0j)) <f>: (np.float32(0.000597856), np.complex128(0.0004964330530222579+0j))\n",
      "Epoch 1400: <Test loss>: 2.298473418704816e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.8886574e-05), np.complex128(5.42443755774788e-05+0j)) <f>: (np.float32(0.000632049), np.complex128(0.0005056165276130166+0j))\n",
      "Epoch 1500: <Test loss>: 2.187907057304983e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.5654132e-05), np.complex128(5.757689400308121e-05+0j)) <f>: (np.float32(0.0006288167), np.complex128(0.0005082901476498157+0j))\n",
      "Epoch 1600: <Test loss>: 2.1792222923977533e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.7207636e-05), np.complex128(5.505847608550478e-05+0j)) <f>: (np.float32(0.0005959549), np.complex128(0.0004980384746217973+0j))\n",
      "Epoch 1700: <Test loss>: 2.2269991859502625e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.8153806e-06), np.complex128(5.899651627896978e-05+0j)) <f>: (np.float32(0.00061597803), np.complex128(0.0004927191121773181+0j))\n",
      "Epoch 1800: <Test loss>: 2.3878876618255163e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.6348258e-05), np.complex128(5.671459437463051e-05+0j)) <f>: (np.float32(0.0006295114), np.complex128(0.0004929992205053926+0j))\n",
      "Epoch 1900: <Test loss>: 2.223968294856604e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.6551638e-06), np.complex128(6.51519499113066e-05+0j)) <f>: (np.float32(0.0006105076), np.complex128(0.000503954335869993+0j))\n",
      "Epoch 2000: <Test loss>: 2.404765837127343e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-3.5443342e-08), np.complex128(6.58797891377631e-05+0j)) <f>: (np.float32(0.00061319757), np.complex128(0.0004844304175072983+0j))\n",
      "Epoch 2100: <Test loss>: 2.1610644580505323e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(3.5683718e-07), np.complex128(6.329256120758491e-05+0j)) <f>: (np.float32(0.00061280554), np.complex128(0.000502965108190988+0j))\n",
      "Epoch 2200: <Test loss>: 2.193462250943412e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.2444882e-06), np.complex128(6.319854556842777e-05+0j)) <f>: (np.float32(0.00061191805), np.complex128(0.0005020087374218433+0j))\n",
      "Epoch 2300: <Test loss>: 2.3020604658086086e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(5.419211e-06), np.complex128(6.713220114047562e-05+0j)) <f>: (np.float32(0.00060774334), np.complex128(0.0004928785443065335+0j))\n",
      "Epoch 2400: <Test loss>: 2.309339606654248e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(5.0254234e-06), np.complex128(6.48578155519718e-05+0j)) <f>: (np.float32(0.0006081378), np.complex128(0.000497670388428904+0j))\n",
      "Epoch 2500: <Test loss>: 2.1824112081958447e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.3629257e-06), np.complex128(6.995531853138026e-05+0j)) <f>: (np.float32(0.0006087994), np.complex128(0.0004923408137016306+0j))\n",
      "Epoch 2600: <Test loss>: 2.4214443783421302e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.4937057e-05), np.complex128(7.13188644329961e-05+0j)) <f>: (np.float32(0.0006380999), np.complex128(0.0005032895543182755+0j))\n",
      "Epoch 2700: <Test loss>: 2.417042196611874e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-7.3704523e-06), np.complex128(7.541463999440478e-05+0j)) <f>: (np.float32(0.00062053266), np.complex128(0.0004913193317008806+0j))\n",
      "Epoch 2800: <Test loss>: 2.720274324019556e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(3.275174e-05), np.complex128(7.5369477600929e-05+0j)) <f>: (np.float32(0.00058041076), np.complex128(0.0004791330019562779+0j))\n",
      "Epoch 2900: <Test loss>: 2.408574346191017e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.470003e-06), np.complex128(7.271764588176897e-05+0j)) <f>: (np.float32(0.0006196326), np.complex128(0.0004960172355076084+0j))\n",
      "Epoch 3000: <Test loss>: 2.5299286789959297e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.935716e-06), np.complex128(7.423459803144825e-05+0j)) <f>: (np.float32(0.0006082271), np.complex128(0.0004920259772690223+0j))\n",
      "Epoch 3100: <Test loss>: 2.445374775561504e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.189567e-05), np.complex128(7.310933608543769e-05+0j)) <f>: (np.float32(0.00062505825), np.complex128(0.000496010765614161+0j))\n",
      "Epoch 3200: <Test loss>: 2.534459099479136e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.21286575e-05), np.complex128(7.52349339466294e-05+0j)) <f>: (np.float32(0.00062529126), np.complex128(0.0004949302299781218+0j))\n",
      "Epoch 3300: <Test loss>: 2.5530941911711125e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.2699731e-05), np.complex128(7.116768617520069e-05+0j)) <f>: (np.float32(0.0006004629), np.complex128(0.0005016665308027395+0j))\n",
      "Epoch 3400: <Test loss>: 2.6250938844896154e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.0768819e-07), np.complex128(7.497767639418643e-05+0j)) <f>: (np.float32(0.00061336986), np.complex128(0.0004957588837820088+0j))\n",
      "Epoch 3500: <Test loss>: 2.6175046059506712e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.212905e-06), np.complex128(7.01649002636038e-05+0j)) <f>: (np.float32(0.00060594996), np.complex128(0.000501159976204007+0j))\n",
      "Epoch 3600: <Test loss>: 2.6498184979573125e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-7.547199e-06), np.complex128(7.638069974613863e-05+0j)) <f>: (np.float32(0.00062071014), np.complex128(0.0004941287877849105+0j))\n",
      "Epoch 3700: <Test loss>: 2.898186039601569e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.056257e-05), np.complex128(7.779931902996704e-05+0j)) <f>: (np.float32(0.0006025996), np.complex128(0.0004950309890540155+0j))\n",
      "Epoch 3800: <Test loss>: 2.8146041586296633e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.4244687e-06), np.complex128(7.836898680497544e-05+0j)) <f>: (np.float32(0.000619587), np.complex128(0.0005037944914436462+0j))\n",
      "Epoch 3900: <Test loss>: 3.079641373915365e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.6074584e-05), np.complex128(7.616242805899067e-05+0j)) <f>: (np.float32(0.0005970885), np.complex128(0.0004928740090380876+0j))\n",
      "Epoch 4000: <Test loss>: 2.825583123922115e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.4021897e-05), np.complex128(7.57672571160776e-05+0j)) <f>: (np.float32(0.0005991408), np.complex128(0.0005002138177176566+0j))\n",
      "Epoch 4100: <Test loss>: 2.84922157334222e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.2736971e-05), np.complex128(7.542847573468133e-05+0j)) <f>: (np.float32(0.00063589955), np.complex128(0.000500050040610979+0j))\n",
      "Epoch 4200: <Test loss>: 3.0463115763268434e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-3.788438e-06), np.complex128(7.646555366730748e-05+0j)) <f>: (np.float32(0.0006169512), np.complex128(0.0004985495010586478+0j))\n",
      "Epoch 4300: <Test loss>: 3.071177616220666e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(5.3724134e-06), np.complex128(7.869152209363437e-05+0j)) <f>: (np.float32(0.00060779037), np.complex128(0.0004995173527171453+0j))\n",
      "Epoch 4400: <Test loss>: 3.1393869903695304e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.9769821e-05), np.complex128(7.958738032994783e-05+0j)) <f>: (np.float32(0.000632932), np.complex128(0.0005053743823361995+0j))\n",
      "Epoch 4500: <Test loss>: 3.19769333145814e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.8968873e-05), np.complex128(8.220129657059642e-05+0j)) <f>: (np.float32(0.0006321319), np.complex128(0.0005076749368993639+0j))\n",
      "Epoch 4600: <Test loss>: 3.3341441394441063e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.8676574e-06), np.complex128(8.122364492643603e-05+0j)) <f>: (np.float32(0.00061903056), np.complex128(0.0005036797459804472+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652d5d8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00017851342272479087 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 6.589034455828369e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.2990414e-05), np.complex128(5.66154567364942e-05+0j)) <f>: (np.float32(0.00063615316), np.complex128(0.0004754532500580845+0j))\n",
      "Epoch 400: <Test loss>: 4.734599770017667e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.714727e-05), np.complex128(5.6737552188940395e-05+0j)) <f>: (np.float32(0.00059601554), np.complex128(0.0004742546705817947+0j))\n",
      "Epoch 600: <Test loss>: 3.7138222523935838e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.0884802e-05), np.complex128(5.269991845185719e-05+0j)) <f>: (np.float32(0.00067404733), np.complex128(0.0005196051349798442+0j))\n",
      "Epoch 800: <Test loss>: 3.4820568544091657e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-7.2460296e-07), np.complex128(5.606471102118206e-05+0j)) <f>: (np.float32(0.0006138868), np.complex128(0.0004982797001588596+0j))\n",
      "Epoch 1000: <Test loss>: 3.1468146062252345e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.9010155e-05), np.complex128(4.115618946617915e-05+0j)) <f>: (np.float32(0.0005941526), np.complex128(0.0005004184756706758+0j))\n",
      "Epoch 1200: <Test loss>: 2.975517645609216e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.680582e-06), np.complex128(3.894082183340513e-05+0j)) <f>: (np.float32(0.0006084821), np.complex128(0.0005079597707868681+0j))\n",
      "Epoch 1400: <Test loss>: 3.3486276151961647e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(3.6881902e-05), np.complex128(4.699786101708684e-05+0j)) <f>: (np.float32(0.00057628035), np.complex128(0.0005067827893372336+0j))\n",
      "Epoch 1600: <Test loss>: 3.1214985938277096e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.2165041e-05), np.complex128(3.9587406809801976e-05+0j)) <f>: (np.float32(0.0006253275), np.complex128(0.000502301373239681+0j))\n",
      "Epoch 1800: <Test loss>: 3.027764705620939e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.650307e-06), np.complex128(3.840009414550814e-05+0j)) <f>: (np.float32(0.00060451275), np.complex128(0.0004979837659639702+0j))\n",
      "Epoch 2000: <Test loss>: 3.0002408948348602e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(3.139888e-05), np.complex128(3.662223323513525e-05+0j)) <f>: (np.float32(0.00058176374), np.complex128(0.0004952486185090932+0j))\n",
      "Epoch 2200: <Test loss>: 2.8915353595948545e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.411759e-05), np.complex128(4.106175756549507e-05+0j)) <f>: (np.float32(0.0005890458), np.complex128(0.0005063687478717656+0j))\n",
      "Epoch 2400: <Test loss>: 3.114075298071839e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.8427492e-05), np.complex128(4.025330632104906e-05+0j)) <f>: (np.float32(0.00058473443), np.complex128(0.0004946092090885419+0j))\n",
      "Epoch 2600: <Test loss>: 3.0997425710665993e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(1.9177116e-05), np.complex128(4.6576223769851054e-05+0j)) <f>: (np.float32(0.0005939861), np.complex128(0.0005102833822742299+0j))\n",
      "Epoch 2800: <Test loss>: 2.9737154818576528e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.295154e-05), np.complex128(4.9438790860323597e-05+0j)) <f>: (np.float32(0.0005702111), np.complex128(0.0005055079983219534+0j))\n",
      "Epoch 3000: <Test loss>: 3.2600478334643412e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.7018893e-05), np.complex128(4.599996320513494e-05+0j)) <f>: (np.float32(0.000566144), np.complex128(0.0004986641513763549+0j))\n",
      "Epoch 3200: <Test loss>: 3.118550239378237e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(3.205479e-05), np.complex128(5.109707767378493e-05+0j)) <f>: (np.float32(0.0005811079), np.complex128(0.0005101839235200585+0j))\n",
      "Epoch 3400: <Test loss>: 3.5955656585429097e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.0262235e-05), np.complex128(5.507295009345598e-05+0j)) <f>: (np.float32(0.00057289994), np.complex128(0.0005020864395735397+0j))\n",
      "Epoch 3600: <Test loss>: 3.2323284813173814e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.7692304e-05), np.complex128(5.422869639329721e-05+0j)) <f>: (np.float32(0.00058547006), np.complex128(0.0005084731758610156+0j))\n",
      "Epoch 3800: <Test loss>: 3.277551513747312e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(3.6885776e-05), np.complex128(5.178033330850787e-05+0j)) <f>: (np.float32(0.0005762768), np.complex128(0.000503249402920705+0j))\n",
      "Epoch 4000: <Test loss>: 3.3027918107109144e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(3.9991337e-05), np.complex128(5.568171870122874e-05+0j)) <f>: (np.float32(0.0005731713), np.complex128(0.0005114953138345459+0j))\n",
      "Epoch 4200: <Test loss>: 3.522973656799877e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.2471136e-05), np.complex128(5.669854253727241e-05+0j)) <f>: (np.float32(0.00057069113), np.complex128(0.0004944967154019838+0j))\n",
      "Epoch 4400: <Test loss>: 3.3066864943975816e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(2.6782822e-05), np.complex128(5.8378756422215e-05+0j)) <f>: (np.float32(0.00058637996), np.complex128(0.0005088262607813568+0j))\n",
      "Epoch 4600: <Test loss>: 3.4025586046482204e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.3834094e-05), np.complex128(5.8901652259176775e-05+0j)) <f>: (np.float32(0.0005693287), np.complex128(0.0005011553775052332+0j))\n",
      "Epoch 4800: <Test loss>: 3.3996279853454325e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(5.5772194e-05), np.complex128(5.609822602069437e-05+0j)) <f>: (np.float32(0.0005573904), np.complex128(0.0005040530651753934+0j))\n",
      "Epoch 5000: <Test loss>: 3.5681446206581313e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.402331e-05), np.complex128(5.707506099938286e-05+0j)) <f>: (np.float32(0.0005391388), np.complex128(0.0004975622714349723+0j))\n",
      "Epoch 5200: <Test loss>: 3.7641618746420136e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.140672e-05), np.complex128(5.566348644634856e-05+0j)) <f>: (np.float32(0.00053175585), np.complex128(0.0005037843425911797+0j))\n",
      "Epoch 5400: <Test loss>: 3.691204028655193e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(6.0457285e-05), np.complex128(6.0218656157684676e-05+0j)) <f>: (np.float32(0.000552705), np.complex128(0.0005080288464139681+0j))\n",
      "Epoch 5600: <Test loss>: 3.7249012621032307e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.562489e-05), np.complex128(5.9197939285265204e-05+0j)) <f>: (np.float32(0.0005375375), np.complex128(0.0004940067795491651+0j))\n",
      "Epoch 5800: <Test loss>: 4.149189408053644e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.1157374e-05), np.complex128(6.88396424936759e-05+0j)) <f>: (np.float32(0.0005420052), np.complex128(0.0004897106751546176+0j))\n",
      "Epoch 6000: <Test loss>: 4.128650743950857e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.284855e-05), np.complex128(6.514754150351647e-05+0j)) <f>: (np.float32(0.000530314), np.complex128(0.000499356334829732+0j))\n",
      "Epoch 6200: <Test loss>: 4.198851002001902e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.6108845e-05), np.complex128(6.67265641934565e-05+0j)) <f>: (np.float32(0.0005370538), np.complex128(0.0005068070514376612+0j))\n",
      "Epoch 6400: <Test loss>: 4.049636117997579e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.9235364e-05), np.complex128(6.444926477426406e-05+0j)) <f>: (np.float32(0.00052392663), np.complex128(0.0005037280481751548+0j))\n",
      "Epoch 6600: <Test loss>: 4.262246420694282e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.170445e-05), np.complex128(6.329900335026382e-05+0j)) <f>: (np.float32(0.00053145783), np.complex128(0.0004941892686025778+0j))\n",
      "Epoch 6800: <Test loss>: 4.485566932999063e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(9.1526585e-05), np.complex128(6.677190102033404e-05+0j)) <f>: (np.float32(0.0005216361), np.complex128(0.0004975375018919213+0j))\n",
      "Epoch 7000: <Test loss>: 4.451265795069048e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.918311e-05), np.complex128(6.732613936807703e-05+0j)) <f>: (np.float32(0.0005239797), np.complex128(0.0004996100244262298+0j))\n",
      "Epoch 7200: <Test loss>: 4.55665031040553e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010750057), np.complex128(6.534264922778882e-05+0j)) <f>: (np.float32(0.0005056619), np.complex128(0.0004975288436521608+0j))\n",
      "Epoch 7400: <Test loss>: 5.077521109342342e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.060809e-05), np.complex128(6.878243626668718e-05+0j)) <f>: (np.float32(0.0005325542), np.complex128(0.0005000252076376001+0j))\n",
      "Epoch 7600: <Test loss>: 4.9071418288804125e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.4271094e-05), np.complex128(6.839310091394233e-05+0j)) <f>: (np.float32(0.0005288916), np.complex128(0.000496414309360359+0j))\n",
      "Epoch 7800: <Test loss>: 4.852367055718787e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(9.273654e-05), np.complex128(6.814043413148224e-05+0j)) <f>: (np.float32(0.0005204256), np.complex128(0.0004975311905742937+0j))\n",
      "Epoch 8000: <Test loss>: 5.074599812360248e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.823565e-05), np.complex128(7.079567523076773e-05+0j)) <f>: (np.float32(0.0005349268), np.complex128(0.0004959323974440215+0j))\n",
      "Epoch 8200: <Test loss>: 5.0184576139145065e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.322875e-05), np.complex128(6.893273442868278e-05+0j)) <f>: (np.float32(0.0005399337), np.complex128(0.0004930937316939867+0j))\n",
      "Epoch 8400: <Test loss>: 5.061202045908431e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.420588e-05), np.complex128(6.60837057488247e-05+0j)) <f>: (np.float32(0.00052895676), np.complex128(0.0004953374843985027+0j))\n",
      "Epoch 8600: <Test loss>: 5.400361260399222e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.230372e-05), np.complex128(7.453850859007302e-05+0j)) <f>: (np.float32(0.0005408586), np.complex128(0.0004905769114277945+0j))\n",
      "Epoch 8800: <Test loss>: 5.254287771094823e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.588394e-05), np.complex128(7.00475382993783e-05+0j)) <f>: (np.float32(0.00052727875), np.complex128(0.0004934357480221066+0j))\n",
      "Epoch 9000: <Test loss>: 5.4681145229551475e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(6.440972e-05), np.complex128(6.73358283506661e-05+0j)) <f>: (np.float32(0.000548753), np.complex128(0.0005056516362995178+0j))\n",
      "Epoch 9200: <Test loss>: 5.427918949862942e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(7.914433e-05), np.complex128(7.079843445003204e-05+0j)) <f>: (np.float32(0.00053401844), np.complex128(0.0004950520162077195+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "659b3f60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00017851342272479087 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 1.0894580555032007e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(6.358668e-05), np.complex128(8.272419240755819e-05+0j)) <f>: (np.float32(0.000549576), np.complex128(0.0004784441803102804+0j))\n",
      "Epoch 800: <Test loss>: 7.296806415979518e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-7.499561e-05), np.complex128(7.504370736554638e-05+0j)) <f>: (np.float32(0.0006881583), np.complex128(0.0004763741949890885+0j))\n",
      "Epoch 1200: <Test loss>: 5.013740974391112e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.8734565e-05), np.complex128(6.0226937779873133e-05+0j)) <f>: (np.float32(0.00064189726), np.complex128(0.00048155388385149787+0j))\n",
      "Epoch 1600: <Test loss>: 4.786797035194468e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.4416702e-05), np.complex128(6.248363820007504e-05+0j)) <f>: (np.float32(0.00063757907), np.complex128(0.00047306107038624625+0j))\n",
      "Epoch 2000: <Test loss>: 4.169336989434669e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.6044081e-05), np.complex128(5.223979485315872e-05+0j)) <f>: (np.float32(0.0006392066), np.complex128(0.0004849300264851241+0j))\n",
      "Epoch 2400: <Test loss>: 4.436381459527183e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-7.9272926e-05), np.complex128(5.204946422545751e-05+0j)) <f>: (np.float32(0.00069243566), np.complex128(0.0005077757276904216+0j))\n",
      "Epoch 2800: <Test loss>: 5.532596787816146e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-1.5831318e-05), np.complex128(6.812877087993678e-05+0j)) <f>: (np.float32(0.0006289945), np.complex128(0.0004745677943955492+0j))\n",
      "Epoch 3200: <Test loss>: 5.004385911888676e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-7.426371e-05), np.complex128(4.683428213018396e-05+0j)) <f>: (np.float32(0.0006874261), np.complex128(0.0005136856582029573+0j))\n",
      "Epoch 3600: <Test loss>: 4.274330876796739e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-3.759644e-05), np.complex128(5.412379452411159e-05+0j)) <f>: (np.float32(0.00065075886), np.complex128(0.000494012900575809+0j))\n",
      "Epoch 4000: <Test loss>: 4.51900177722564e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-3.1357737e-05), np.complex128(5.7125999517094436e-05+0j)) <f>: (np.float32(0.00064452074), np.complex128(0.0005006505672405183+0j))\n",
      "Epoch 4400: <Test loss>: 4.624364009941928e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.021263e-05), np.complex128(5.3352117012274544e-05+0j)) <f>: (np.float32(0.0006633757), np.complex128(0.0005090494721052912+0j))\n",
      "Epoch 4800: <Test loss>: 4.62407751911087e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-3.5590012e-05), np.complex128(5.3940734597747685e-05+0j)) <f>: (np.float32(0.000648752), np.complex128(0.0004973184135393014+0j))\n",
      "Epoch 5200: <Test loss>: 4.896990503766574e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-3.3512097e-05), np.complex128(5.5651890589526516e-05+0j)) <f>: (np.float32(0.0006466751), np.complex128(0.0004997580390964204+0j))\n",
      "Epoch 5600: <Test loss>: 4.952513336320408e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.3156585e-05), np.complex128(5.8404132517776665e-05+0j)) <f>: (np.float32(0.0006763189), np.complex128(0.0005098287453988961+0j))\n",
      "Epoch 6000: <Test loss>: 5.469389634527033e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.3863386e-05), np.complex128(6.192888051652225e-05+0j)) <f>: (np.float32(0.0006370261), np.complex128(0.0005030789973447602+0j))\n",
      "Epoch 6400: <Test loss>: 5.27523661730811e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-4.2553434e-05), np.complex128(5.6048742436129356e-05+0j)) <f>: (np.float32(0.0006557159), np.complex128(0.0005037772383944531+0j))\n",
      "Epoch 6800: <Test loss>: 5.400099780672463e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.08555e-05), np.complex128(5.717148699100078e-05+0j)) <f>: (np.float32(0.00066401804), np.complex128(0.0005014246392472341+0j))\n",
      "Epoch 7200: <Test loss>: 6.051961463526823e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-3.2461554e-05), np.complex128(6.90169461177815e-05+0j)) <f>: (np.float32(0.00064562366), np.complex128(0.000490396579005531+0j))\n",
      "Epoch 7600: <Test loss>: 5.627077371173073e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-3.7569615e-05), np.complex128(6.193808187846549e-05+0j)) <f>: (np.float32(0.0006507317), np.complex128(0.0005037074650337462+0j))\n",
      "Epoch 8000: <Test loss>: 5.812189101561671e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.0340786e-05), np.complex128(6.125603141997293e-05+0j)) <f>: (np.float32(0.00067350356), np.complex128(0.0005052076557192738+0j))\n",
      "Epoch 8400: <Test loss>: 5.91374509895104e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.502308e-05), np.complex128(6.262956363383996e-05+0j)) <f>: (np.float32(0.0006781856), np.complex128(0.0005056704433917447+0j))\n",
      "Epoch 8800: <Test loss>: 6.122828835941618e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.161028e-05), np.complex128(6.247627235324586e-05+0j)) <f>: (np.float32(0.00067477295), np.complex128(0.0005041318773578283+0j))\n",
      "Epoch 9200: <Test loss>: 5.925424829911208e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.144598e-05), np.complex128(6.161754068074039e-05+0j)) <f>: (np.float32(0.0006746085), np.complex128(0.0005087574071604045+0j))\n",
      "Epoch 9600: <Test loss>: 6.198567007231759e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.0652466e-05), np.complex128(6.1849192202683e-05+0j)) <f>: (np.float32(0.0006738153), np.complex128(0.0005078708414671308+0j))\n",
      "Epoch 10000: <Test loss>: 6.286322332016425e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.3782263e-05), np.complex128(6.306964324891716e-05+0j)) <f>: (np.float32(0.00066694507), np.complex128(0.0005063983698349021+0j))\n",
      "Epoch 10400: <Test loss>: 6.471352662629215e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-4.857941e-05), np.complex128(6.247165383249451e-05+0j)) <f>: (np.float32(0.0006617416), np.complex128(0.0005051620175983387+0j))\n",
      "Epoch 10800: <Test loss>: 6.66266987536801e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.2541014e-05), np.complex128(6.458668261529733e-05+0j)) <f>: (np.float32(0.0006657028), np.complex128(0.0005076751906206756+0j))\n",
      "Epoch 11200: <Test loss>: 6.955034677957883e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.215705e-05), np.complex128(6.633000174772478e-05+0j)) <f>: (np.float32(0.0006753201), np.complex128(0.0005108393808135724+0j))\n",
      "Epoch 11600: <Test loss>: 6.852697879367042e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.1029012e-05), np.complex128(6.201773054834977e-05+0j)) <f>: (np.float32(0.0006641915), np.complex128(0.0005094708080584694+0j))\n",
      "Epoch 12000: <Test loss>: 6.813715572207002e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.452084e-05), np.complex128(6.365947393941168e-05+0j)) <f>: (np.float32(0.0006776832), np.complex128(0.0005122697347080652+0j))\n",
      "Epoch 12400: <Test loss>: 6.941606898180908e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.5028537e-05), np.complex128(6.250758711325861e-05+0j)) <f>: (np.float32(0.0006681908), np.complex128(0.0005074114156020394+0j))\n",
      "Epoch 12800: <Test loss>: 7.322213150473544e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-7.663845e-05), np.complex128(6.596547161759034e-05+0j)) <f>: (np.float32(0.00068980077), np.complex128(0.0005123017035933346+0j))\n",
      "Epoch 13200: <Test loss>: 7.121147973521147e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.24248e-05), np.complex128(6.572112610127366e-05+0j)) <f>: (np.float32(0.00067558774), np.complex128(0.0005073315885343578+0j))\n",
      "Epoch 13600: <Test loss>: 7.671032108191866e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-4.9424336e-05), np.complex128(6.681482749475081e-05+0j)) <f>: (np.float32(0.000662586), np.complex128(0.0005153109334951341+0j))\n",
      "Epoch 14000: <Test loss>: 7.397445187962148e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.6149754e-05), np.complex128(6.436589750140573e-05+0j)) <f>: (np.float32(0.0006793128), np.complex128(0.0005099352132043021+0j))\n",
      "Epoch 14400: <Test loss>: 7.526636636612238e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.200985e-05), np.complex128(6.529455318164695e-05+0j)) <f>: (np.float32(0.0006751728), np.complex128(0.000510083735317116+0j))\n",
      "Epoch 14800: <Test loss>: 7.5127150012122e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.6951478e-05), np.complex128(6.603639861738627e-05+0j)) <f>: (np.float32(0.0006701134), np.complex128(0.0005085857012627375+0j))\n",
      "Epoch 15200: <Test loss>: 8.198571777029429e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.381193e-05), np.complex128(7.034532783135922e-05+0j)) <f>: (np.float32(0.00067697413), np.complex128(0.000501504276023932+0j))\n",
      "Epoch 15600: <Test loss>: 7.757399544061627e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-8.815931e-05), np.complex128(6.755953125964182e-05+0j)) <f>: (np.float32(0.00070132164), np.complex128(0.0005169498463078127+0j))\n",
      "Epoch 16000: <Test loss>: 7.978953362908214e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.735295e-05), np.complex128(6.657238885266708e-05+0j)) <f>: (np.float32(0.0006805151), np.complex128(0.0005114296000148255+0j))\n",
      "Epoch 16400: <Test loss>: 7.609527074237121e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-5.8608115e-05), np.complex128(6.725668315900964e-05+0j)) <f>: (np.float32(0.0006717707), np.complex128(0.0005128079410404275+0j))\n",
      "Epoch 16800: <Test loss>: 8.276068001578096e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.0413906e-05), np.complex128(7.052546996263902e-05+0j)) <f>: (np.float32(0.00067357614), np.complex128(0.0005090834073307259+0j))\n",
      "Epoch 17200: <Test loss>: 7.886174898885656e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-2.5813444e-05), np.complex128(6.5307263033603e-05+0j)) <f>: (np.float32(0.00063897617), np.complex128(0.0005109551411620182+0j))\n",
      "Epoch 17600: <Test loss>: 7.945146535348613e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-6.595696e-05), np.complex128(6.994653343096398e-05+0j)) <f>: (np.float32(0.0006791189), np.complex128(0.0005052650284508734+0j))\n",
      "Epoch 18000: <Test loss>: 7.787700269545894e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-8.104406e-05), np.complex128(6.471466915944867e-05+0j)) <f>: (np.float32(0.0006942062), np.complex128(0.0005161247763174529+0j))\n",
      "Epoch 18400: <Test loss>: 8.309758413815871e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(-7.489594e-05), np.complex128(7.35759454351655e-05+0j)) <f>: (np.float32(0.0006880589), np.complex128(0.0005176473896238987+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db8f0124",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00017851342272479087 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 2.5508979888400063e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.000103940976), np.complex128(0.0001752945794719354+0j)) <f>: (np.float32(0.00050922146), np.complex128(0.00046121536150254937+0j))\n",
      "Epoch 1600: <Test loss>: 1.804502790037077e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010562594), np.complex128(0.0001376628089596758+0j)) <f>: (np.float32(0.0005075365), np.complex128(0.0004490761922067518+0j))\n",
      "Epoch 2400: <Test loss>: 1.3093153938825708e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00017832986), np.complex128(0.00011350352909357788+0j)) <f>: (np.float32(0.0004348326), np.complex128(0.0004463738065162432+0j))\n",
      "Epoch 3200: <Test loss>: 1.0819796443684027e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00018986555), np.complex128(9.419789034682211e-05+0j)) <f>: (np.float32(0.00042329726), np.complex128(0.00045080451006662724+0j))\n",
      "Epoch 4000: <Test loss>: 1.3041802958468907e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00015175682), np.complex128(9.635695563478021e-05+0j)) <f>: (np.float32(0.00046140558), np.complex128(0.0004878678655528548+0j))\n",
      "Epoch 4800: <Test loss>: 1.0759123142634053e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00020959332), np.complex128(9.523803672156117e-05+0j)) <f>: (np.float32(0.00040356952), np.complex128(0.00044705314532821733+0j))\n",
      "Epoch 5600: <Test loss>: 9.524453162157442e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00017625286), np.complex128(7.877675659677071e-05+0j)) <f>: (np.float32(0.00043690964), np.complex128(0.00045929316884540044+0j))\n",
      "Epoch 6400: <Test loss>: 8.126274224196095e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00019589232), np.complex128(7.03433932063578e-05+0j)) <f>: (np.float32(0.00041726994), np.complex128(0.0004640709631501377+0j))\n",
      "Epoch 7200: <Test loss>: 8.536074346920941e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00017782017), np.complex128(6.294236633155936e-05+0j)) <f>: (np.float32(0.0004353421), np.complex128(0.000470349423867862+0j))\n",
      "Epoch 8000: <Test loss>: 8.178877578757238e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00016850383), np.complex128(5.975372374725112e-05+0j)) <f>: (np.float32(0.00044465918), np.complex128(0.0004720475488916497+0j))\n",
      "Epoch 8800: <Test loss>: 8.240097486122977e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0001640002), np.complex128(6.273207100814677e-05+0j)) <f>: (np.float32(0.00044916273), np.complex128(0.00047986337046706005+0j))\n",
      "Epoch 9600: <Test loss>: 8.330209311679937e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00016844831), np.complex128(6.668517590449169e-05+0j)) <f>: (np.float32(0.00044471418), np.complex128(0.0004791842536612336+0j))\n",
      "Epoch 10400: <Test loss>: 8.29918008093955e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0001382565), np.complex128(6.714417361486965e-05+0j)) <f>: (np.float32(0.00047490612), np.complex128(0.00047760179384039964+0j))\n",
      "Epoch 11200: <Test loss>: 1.2693299140664749e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(8.26411e-05), np.complex128(9.131435556857803e-05+0j)) <f>: (np.float32(0.00053052127), np.complex128(0.0004873579174315788+0j))\n",
      "Epoch 12000: <Test loss>: 8.806251571513712e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00013905848), np.complex128(6.626738412088577e-05+0j)) <f>: (np.float32(0.00047410437), np.complex128(0.00048154709680641095+0j))\n",
      "Epoch 12800: <Test loss>: 8.297149179270491e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.000117942735), np.complex128(6.754505328729513e-05+0j)) <f>: (np.float32(0.0004952197), np.complex128(0.0004928422304438019+0j))\n",
      "Epoch 13600: <Test loss>: 8.92003663466312e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00012477799), np.complex128(6.910616880278552e-05+0j)) <f>: (np.float32(0.00048838457), np.complex128(0.0004884792070533036+0j))\n",
      "Epoch 14400: <Test loss>: 9.073211003851611e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00011479045), np.complex128(7.23087422728622e-05+0j)) <f>: (np.float32(0.00049837265), np.complex128(0.0004912924372418445+0j))\n",
      "Epoch 15200: <Test loss>: 8.89915281732101e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.000114543844), np.complex128(7.054227107074562e-05+0j)) <f>: (np.float32(0.00049861864), np.complex128(0.000494778568064076+0j))\n",
      "Epoch 16000: <Test loss>: 8.961330422607716e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00012559698), np.complex128(7.20334308633363e-05+0j)) <f>: (np.float32(0.00048756573), np.complex128(0.0004967848059057129+0j))\n",
      "Epoch 16800: <Test loss>: 1.046245415636804e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(4.9832084e-05), np.complex128(8.88020864284139e-05+0j)) <f>: (np.float32(0.00056333025), np.complex128(0.0005168643105106188+0j))\n",
      "Epoch 17600: <Test loss>: 1.1686567631841172e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0001406962), np.complex128(6.918094523060681e-05+0j)) <f>: (np.float32(0.00047246597), np.complex128(0.0004877343764277568+0j))\n",
      "Epoch 18400: <Test loss>: 9.552233677823097e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00011430035), np.complex128(7.548033795654318e-05+0j)) <f>: (np.float32(0.0004988627), np.complex128(0.0004925831809845954+0j))\n",
      "Epoch 19200: <Test loss>: 9.984623829950579e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.000110639296), np.complex128(7.718708950492739e-05+0j)) <f>: (np.float32(0.0005025234), np.complex128(0.0004947469480456102+0j))\n",
      "Epoch 20000: <Test loss>: 1.1385942343622446e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(9.005304e-05), np.complex128(9.746360870833859e-05+0j)) <f>: (np.float32(0.00052311), np.complex128(0.00048488600583755084+0j))\n",
      "Epoch 20800: <Test loss>: 9.989414138544817e-06 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010323133), np.complex128(8.34700934198886e-05+0j)) <f>: (np.float32(0.0005099312), np.complex128(0.0004984169633884686+0j))\n",
      "Epoch 21600: <Test loss>: 1.0261629540764261e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00011036953), np.complex128(8.318009788945029e-05+0j)) <f>: (np.float32(0.0005027931), np.complex128(0.0004986426167800276+0j))\n",
      "Epoch 22400: <Test loss>: 1.053373398463009e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.000105827225), np.complex128(8.316393901341384e-05+0j)) <f>: (np.float32(0.00050733605), np.complex128(0.0004959705825014266+0j))\n",
      "Epoch 23200: <Test loss>: 1.0553530046308879e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010115195), np.complex128(8.449579353744492e-05+0j)) <f>: (np.float32(0.0005120102), np.complex128(0.0004991572270304055+0j))\n",
      "Epoch 24000: <Test loss>: 1.1540081686689518e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(9.9114994e-05), np.complex128(9.035796101306028e-05+0j)) <f>: (np.float32(0.00051404774), np.complex128(0.000496531972618642+0j))\n",
      "Epoch 24800: <Test loss>: 1.1251917385379784e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.000109279674), np.complex128(8.411832758481124e-05+0j)) <f>: (np.float32(0.00050388323), np.complex128(0.0004963819281779582+0j))\n",
      "Epoch 25600: <Test loss>: 1.0926609320449643e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(9.339119e-05), np.complex128(8.627913720678701e-05+0j)) <f>: (np.float32(0.0005197717), np.complex128(0.0004951520141196781+0j))\n",
      "Epoch 26400: <Test loss>: 1.1258754966547713e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010484633), np.complex128(8.694140533176095e-05+0j)) <f>: (np.float32(0.00050831644), np.complex128(0.0004997586416845356+0j))\n",
      "Epoch 27200: <Test loss>: 1.1727643141057342e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0001022614), np.complex128(8.643872791182221e-05+0j)) <f>: (np.float32(0.0005109008), np.complex128(0.0004979564592078027+0j))\n",
      "Epoch 28000: <Test loss>: 1.1711946171999443e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(9.562655e-05), np.complex128(8.8697775254157e-05+0j)) <f>: (np.float32(0.00051753625), np.complex128(0.0005012937190504167+0j))\n",
      "Epoch 28800: <Test loss>: 1.3539670362661127e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00011088456), np.complex128(8.990134986877014e-05+0j)) <f>: (np.float32(0.0005022782), np.complex128(0.0004895693523840221+0j))\n",
      "Epoch 29600: <Test loss>: 1.1781567991420161e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0001021987), np.complex128(8.810956203702431e-05+0j)) <f>: (np.float32(0.00051096367), np.complex128(0.0005019298300939165+0j))\n",
      "Epoch 30400: <Test loss>: 1.1762354006350506e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010229733), np.complex128(8.860404901587096e-05+0j)) <f>: (np.float32(0.0005108654), np.complex128(0.000502634445891565+0j))\n",
      "Epoch 31200: <Test loss>: 1.1924356840609107e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010537054), np.complex128(9.768498055276341e-05+0j)) <f>: (np.float32(0.000507792), np.complex128(0.0004965689207846528+0j))\n",
      "Epoch 32000: <Test loss>: 1.2391962627589237e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010075325), np.complex128(8.782043074480385e-05+0j)) <f>: (np.float32(0.0005124095), np.complex128(0.0005026834458198797+0j))\n",
      "Epoch 32800: <Test loss>: 1.2086753486073576e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(9.690966e-05), np.complex128(9.245352461398405e-05+0j)) <f>: (np.float32(0.0005162529), np.complex128(0.0005069497379603069+0j))\n",
      "Epoch 33600: <Test loss>: 1.2153009265603032e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010227763), np.complex128(8.882376374297899e-05+0j)) <f>: (np.float32(0.0005108853), np.complex128(0.0005066146989682577+0j))\n",
      "Epoch 34400: <Test loss>: 1.2677208360400982e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010028481), np.complex128(9.254327852798438e-05+0j)) <f>: (np.float32(0.0005128781), np.complex128(0.0005059234669697993+0j))\n",
      "Epoch 35200: <Test loss>: 1.362687999062473e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.000110670684), np.complex128(9.266114000604221e-05+0j)) <f>: (np.float32(0.00050249195), np.complex128(0.0005033885056298235+0j))\n",
      "Epoch 36000: <Test loss>: 1.3041438251093496e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00010368866), np.complex128(9.311748157143873e-05+0j)) <f>: (np.float32(0.0005094745), np.complex128(0.0005052085437438646+0j))\n",
      "Epoch 36800: <Test loss>: 1.3003037565795239e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(9.93667e-05), np.complex128(9.31523841093742e-05+0j)) <f>: (np.float32(0.00051379646), np.complex128(0.0005105378013194986+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f87d6a38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00017851342272479087 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.00010302069131284952 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0008392912), np.complex128(0.0003082491613390152+0j)) <f>: (np.float32(-0.00022612904), np.complex128(0.00039584181377627474+0j))\n",
      "Epoch 3200: <Test loss>: 7.83834038884379e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0007876473), np.complex128(0.000303606283341754+0j)) <f>: (np.float32(-0.00017448522), np.complex128(0.00046354503058622696+0j))\n",
      "Epoch 4800: <Test loss>: 6.476648559328169e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0005990958), np.complex128(0.0002402494553188043+0j)) <f>: (np.float32(1.4067113e-05), np.complex128(0.0004401433942719148+0j))\n",
      "Epoch 6400: <Test loss>: 5.757057078881189e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0005039605), np.complex128(0.00020142037393681316+0j)) <f>: (np.float32(0.000109201814), np.complex128(0.00042401655055105845+0j))\n",
      "Epoch 8000: <Test loss>: 4.972265742253512e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00041571233), np.complex128(0.0001930392454214386+0j)) <f>: (np.float32(0.00019744999), np.complex128(0.00043343909404773036+0j))\n",
      "Epoch 9600: <Test loss>: 4.458106559468433e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00042304987), np.complex128(0.00017503250121457087+0j)) <f>: (np.float32(0.0001901128), np.complex128(0.0004448842086954774+0j))\n",
      "Epoch 11200: <Test loss>: 4.346997229731642e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00037233587), np.complex128(0.00019191221535503738+0j)) <f>: (np.float32(0.0002408264), np.complex128(0.00046498267896843+0j))\n",
      "Epoch 12800: <Test loss>: 3.912674947059713e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.000432668), np.complex128(0.00016323539581963918+0j)) <f>: (np.float32(0.00018049477), np.complex128(0.0004377594922578695+0j))\n",
      "Epoch 14400: <Test loss>: 3.9045888115651906e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00044506462), np.complex128(0.00015653743862824627+0j)) <f>: (np.float32(0.00016809787), np.complex128(0.0004375718019175678+0j))\n",
      "Epoch 16000: <Test loss>: 3.457116326899268e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00039285657), np.complex128(0.00014930005407067865+0j)) <f>: (np.float32(0.00022030582), np.complex128(0.000440284970763822+0j))\n",
      "Epoch 17600: <Test loss>: 3.5754281270783395e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0003912537), np.complex128(0.00016142691788286062+0j)) <f>: (np.float32(0.00022190908), np.complex128(0.00044001215692345783+0j))\n",
      "Epoch 19200: <Test loss>: 3.561707853805274e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0004658749), np.complex128(0.0001554481971797007+0j)) <f>: (np.float32(0.00014728797), np.complex128(0.00044385701796521507+0j))\n",
      "Epoch 20800: <Test loss>: 3.318685048725456e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00035163746), np.complex128(0.0001501046360651218+0j)) <f>: (np.float32(0.00026152536), np.complex128(0.00043585239601876447+0j))\n",
      "Epoch 22400: <Test loss>: 3.051935345865786e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0003625298), np.complex128(0.00013499329603660934+0j)) <f>: (np.float32(0.00025063287), np.complex128(0.0004482369138229389+0j))\n",
      "Epoch 24000: <Test loss>: 3.071699757128954e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00036331362), np.complex128(0.00013412326980133944+0j)) <f>: (np.float32(0.00024984937), np.complex128(0.0004460969648500563+0j))\n",
      "Epoch 25600: <Test loss>: 2.9904438633820973e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00035380624), np.complex128(0.00013140845176655937+0j)) <f>: (np.float32(0.00025935617), np.complex128(0.0004491484710654114+0j))\n",
      "Epoch 27200: <Test loss>: 2.9258651920827106e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00033402233), np.complex128(0.0001283663887412727+0j)) <f>: (np.float32(0.00027914063), np.complex128(0.0004473887869083818+0j))\n",
      "Epoch 28800: <Test loss>: 3.227551496820524e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0004275086), np.complex128(0.00011584026273002707+0j)) <f>: (np.float32(0.00018565377), np.complex128(0.0004527764003856979+0j))\n",
      "Epoch 30400: <Test loss>: 2.9106307920301333e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0003335056), np.complex128(0.00012099159823585827+0j)) <f>: (np.float32(0.00027965714), np.complex128(0.0004503151133715958+0j))\n",
      "Epoch 32000: <Test loss>: 2.8322536309133284e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0003314284), np.complex128(0.00012827917204038898+0j)) <f>: (np.float32(0.00028173445), np.complex128(0.00045881189123234215+0j))\n",
      "Epoch 33600: <Test loss>: 2.985882747452706e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00029338454), np.complex128(0.00013113513048357183+0j)) <f>: (np.float32(0.000319778), np.complex128(0.00045227352474598446+0j))\n",
      "Epoch 35200: <Test loss>: 2.8116350222262554e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00030638822), np.complex128(0.0001262115970716579+0j)) <f>: (np.float32(0.0003067742), np.complex128(0.0004553975318261469+0j))\n",
      "Epoch 36800: <Test loss>: 2.8382093660184182e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00033073538), np.complex128(0.00012249534894854015+0j)) <f>: (np.float32(0.0002824275), np.complex128(0.0004667463909512823+0j))\n",
      "Epoch 38400: <Test loss>: 3.392237704247236e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00030020738), np.complex128(0.0001713705732385217+0j)) <f>: (np.float32(0.00031295532), np.complex128(0.0004624937680465207+0j))\n",
      "Epoch 40000: <Test loss>: 2.779960777843371e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00030191644), np.complex128(0.00012874887361860268+0j)) <f>: (np.float32(0.00031124623), np.complex128(0.0004698601223183224+0j))\n",
      "Epoch 41600: <Test loss>: 2.7524543838808313e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0002897041), np.complex128(0.00012354355890371546+0j)) <f>: (np.float32(0.00032345846), np.complex128(0.000463982985285319+0j))\n",
      "Epoch 43200: <Test loss>: 2.8640752134379e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00033224066), np.complex128(0.00011574020931653149+0j)) <f>: (np.float32(0.00028092205), np.complex128(0.0004565174576958216+0j))\n",
      "Epoch 44800: <Test loss>: 2.758745540631935e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00028251522), np.complex128(0.00012471439554033326+0j)) <f>: (np.float32(0.00033064757), np.complex128(0.0004681186426654045+0j))\n",
      "Epoch 46400: <Test loss>: 2.7821213734569028e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00027385628), np.complex128(0.00012067869642825148+0j)) <f>: (np.float32(0.00033930654), np.complex128(0.0004623509863783831+0j))\n",
      "Epoch 48000: <Test loss>: 2.7908636184292845e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0002438711), np.complex128(0.00011830726640243256+0j)) <f>: (np.float32(0.00036929152), np.complex128(0.0004645915675665035+0j))\n",
      "Epoch 49600: <Test loss>: 2.8573194867931306e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00024699615), np.complex128(0.00012651679209442295+0j)) <f>: (np.float32(0.00036616623), np.complex128(0.00046224385255453395+0j))\n",
      "Epoch 51200: <Test loss>: 2.7992709874524735e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0002499411), np.complex128(0.00011725695531764505+0j)) <f>: (np.float32(0.0003632207), np.complex128(0.00046709925386547574+0j))\n",
      "Epoch 52800: <Test loss>: 2.7845835575135425e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00023120485), np.complex128(0.00012205840913469477+0j)) <f>: (np.float32(0.00038195756), np.complex128(0.0004659474225408596+0j))\n",
      "Epoch 54400: <Test loss>: 2.966933789139148e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00022464702), np.complex128(0.00013582895889173105+0j)) <f>: (np.float32(0.0003885152), np.complex128(0.00047494241391221766+0j))\n",
      "Epoch 56000: <Test loss>: 3.293238114565611e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00021144642), np.complex128(0.00013308261598397704+0j)) <f>: (np.float32(0.000401716), np.complex128(0.00044201607955812576+0j))\n",
      "Epoch 57600: <Test loss>: 2.8348913474474102e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00022796002), np.complex128(0.00012510521357699314+0j)) <f>: (np.float32(0.00038520247), np.complex128(0.0004626927489851914+0j))\n",
      "Epoch 59200: <Test loss>: 2.9612921935040504e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0002192793), np.complex128(0.00012909566307889826+0j)) <f>: (np.float32(0.00039388338), np.complex128(0.0004605344052172134+0j))\n",
      "Epoch 60800: <Test loss>: 2.8542021027533337e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0002192444), np.complex128(0.00012603717159868146+0j)) <f>: (np.float32(0.00039391842), np.complex128(0.0004622380486795297+0j))\n",
      "Epoch 62400: <Test loss>: 2.8436890715966e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00020807235), np.complex128(0.00012457530868879673+0j)) <f>: (np.float32(0.00040509013), np.complex128(0.00046183149199275584+0j))\n",
      "Epoch 64000: <Test loss>: 2.899569517467171e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00021103215), np.complex128(0.00012910713603945997+0j)) <f>: (np.float32(0.0004021304), np.complex128(0.0004612751128714457+0j))\n",
      "Epoch 65600: <Test loss>: 2.943172148661688e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0002047589), np.complex128(0.00012657365738339912+0j)) <f>: (np.float32(0.00040840358), np.complex128(0.00045206835935034204+0j))\n",
      "Epoch 67200: <Test loss>: 2.959289849968627e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.0002144066), np.complex128(0.00013268348857436025+0j)) <f>: (np.float32(0.0003987559), np.complex128(0.00047355627095628197+0j))\n",
      "Epoch 68800: <Test loss>: 2.885832691390533e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00020376888), np.complex128(0.000124683853837442+0j)) <f>: (np.float32(0.0004093937), np.complex128(0.00046637805103707744+0j))\n",
      "Epoch 70400: <Test loss>: 2.98917821055511e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00020447344), np.complex128(0.00012923104718504273+0j)) <f>: (np.float32(0.00040868894), np.complex128(0.00046313130627239865+0j))\n",
      "Epoch 72000: <Test loss>: 3.020504482265096e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00018369415), np.complex128(0.000132691916879182+0j)) <f>: (np.float32(0.00042946835), np.complex128(0.0004559465213142549+0j))\n",
      "Epoch 73600: <Test loss>: 2.9960336178191938e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00019614778), np.complex128(0.00012932607374505103+0j)) <f>: (np.float32(0.00041701467), np.complex128(0.00047196087134855326+0j))\n",
      "Epoch 75200: <Test loss>: 2.950742418761365e-05 <O>: (np.float32(0.00061316247), np.complex128(0.0005036270353779495+0j)) <O-f>: (np.float32(0.00017169373), np.complex128(0.0001284671557459573+0j)) <f>: (np.float32(0.00044146887), np.complex128(0.00046460498308085763+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764461c7",
   "metadata": {},
   "source": [
    "## sweep1 - pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe13cf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(-0.00026859494), np.complex128(4.165512866076346e-05+0j))\n",
      "bin size 1: (np.float32(-0.00026859494), np.complex128(4.165511880292293e-05+0j))\n",
      "jack bin size 2: (np.float32(-0.00026859494), np.complex128(5.771586174480367e-05+0j))\n",
      "bin size 2: (np.float32(-0.00026859494), np.complex128(5.771588137737261e-05+0j))\n",
      "jack bin size 4: (np.float32(-0.00026859494), np.complex128(7.934396848155021e-05+0j))\n",
      "bin size 4: (np.float32(-0.00026859494), np.complex128(7.934399817653766e-05+0j))\n",
      "jack bin size 5: (np.float32(-0.00026859494), np.complex128(8.761107903970432e-05+0j))\n",
      "bin size 5: (np.float32(-0.00026859494), np.complex128(8.761109916150745e-05+0j))\n",
      "jack bin size 10: (np.float32(-0.00026859494), np.complex128(0.00011723072219819025+0j))\n",
      "bin size 10: (np.float32(-0.00026859494), np.complex128(0.00011723072634455652+0j))\n",
      "jack bin size 20: (np.float32(-0.00026859494), np.complex128(0.00015163922532935542+0j))\n",
      "bin size 20: (np.float32(-0.00026859494), np.complex128(0.00015163922928166087+0j))\n",
      "jack bin size 50: (np.float32(-0.00026859494), np.complex128(0.0001948985297015352+0j))\n",
      "bin size 50: (np.float32(-0.00026859494), np.complex128(0.00019489852278525317+0j))\n",
      "jack bin size 100: (np.float32(-0.00026859494), np.complex128(0.0002172294583771019+0j))\n",
      "bin size 100: (np.float32(-0.00026859494), np.complex128(0.0002172294962164096+0j))\n",
      "jack bin size 200: (np.float32(-0.00026859494), np.complex128(0.00022576405381299875+0j))\n",
      "bin size 200: (np.float32(-0.00026859494), np.complex128(0.00022576412669208715+0j))\n",
      "jack bin size 500: (np.float32(-0.00026859494), np.complex128(0.0002490216586118482+0j))\n",
      "bin size 500: (np.float32(-0.00026859494), np.complex128(0.00024902164700682153+0j))\n",
      "jack bin size 1000: (np.float32(-0.00026859494), np.complex128(0.0002353523733190024+0j))\n",
      "bin size 1000: (np.float32(-0.00026859494), np.complex128(0.0002353523747815249+0j))\n",
      "jack bin size 2000: (np.float32(-0.00026859494), np.complex128(0.00021831642879988067+0j))\n",
      "bin size 2000: (np.float32(-0.00026859494), np.complex128(0.0002183163970974939+0j))\n",
      "jack bin size 5000: (np.float32(-0.00026859494), np.complex128(0.0002697519632902595+0j))\n",
      "bin size 5000: (np.float32(-0.00026859494), np.complex128(0.0002697519849901085+0j))\n",
      "jack bin size 10000: (np.float32(-0.00026859494), np.complex128(0.00024116023269016296+0j))\n",
      "bin size 10000: (np.float32(-0.00026859494), np.complex128(0.00024116021813824773+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_32x32_0.1_0.5_sweep1.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(32,32), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb3c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22f6d132",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 5.21244646733976e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.706873e-05), np.complex128(6.398452265481425e-05+0j)) <f>: (np.float32(0.00025053628), np.complex128(0.0003580384483694219+0j))\n",
      "Epoch 200: <Test loss>: 3.7355471249611583e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.4315666e-06), np.complex128(6.204893826968417e-05+0j)) <f>: (np.float32(0.00028517336), np.complex128(0.0003793555103871217+0j))\n",
      "Epoch 300: <Test loss>: 3.2295290566253243e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.3142647e-05), np.complex128(6.495649332023071e-05+0j)) <f>: (np.float32(0.00026446223), np.complex128(0.00034861774435225953+0j))\n",
      "Epoch 400: <Test loss>: 2.658916628206498e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.854104e-05), np.complex128(5.1547587613408716e-05+0j)) <f>: (np.float32(0.00024906412), np.complex128(0.0003608113685845725+0j))\n",
      "Epoch 500: <Test loss>: 3.313788511150051e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.000102603146), np.complex128(5.6750959774503515e-05+0j)) <f>: (np.float32(0.00018500169), np.complex128(0.0003488596993380929+0j))\n",
      "Epoch 600: <Test loss>: 3.6574890600604704e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.445616e-05), np.complex128(6.138800614599196e-05+0j)) <f>: (np.float32(0.00026314877), np.complex128(0.00035615459934549784+0j))\n",
      "Epoch 700: <Test loss>: 2.2917693058843724e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.765378e-05), np.complex128(4.9532354558394335e-05+0j)) <f>: (np.float32(0.00022995147), np.complex128(0.00035840907177543175+0j))\n",
      "Epoch 800: <Test loss>: 2.4510136427124962e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.9653892e-05), np.complex128(5.6953139980338425e-05+0j)) <f>: (np.float32(0.00023795123), np.complex128(0.00035956534322300194+0j))\n",
      "Epoch 900: <Test loss>: 2.145573489542585e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.9170535e-05), np.complex128(5.0404462101298965e-05+0j)) <f>: (np.float32(0.00025843453), np.complex128(0.00035925983104859735+0j))\n",
      "Epoch 1000: <Test loss>: 2.159371433663182e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.6194668e-05), np.complex128(5.105638315403169e-05+0j)) <f>: (np.float32(0.00026141017), np.complex128(0.00036609749353722396+0j))\n",
      "Epoch 1100: <Test loss>: 2.7238424991082866e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.7802491e-05), np.complex128(5.76836234585899e-05+0j)) <f>: (np.float32(0.0002698026), np.complex128(0.00035870884350516004+0j))\n",
      "Epoch 1200: <Test loss>: 2.135847353201825e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.7146876e-05), np.complex128(5.4209758476018966e-05+0j)) <f>: (np.float32(0.00026045833), np.complex128(0.00035916148232516444+0j))\n",
      "Epoch 1300: <Test loss>: 2.164651050406974e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.2513748e-05), np.complex128(5.570197279781124e-05+0j)) <f>: (np.float32(0.0002550913), np.complex128(0.0003541661854260052+0j))\n",
      "Epoch 1400: <Test loss>: 2.2327026272250805e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.3138756e-06), np.complex128(5.347414110560184e-05+0j)) <f>: (np.float32(0.00028229118), np.complex128(0.0003645581663393725+0j))\n",
      "Epoch 1500: <Test loss>: 2.210673528679763e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.5901583e-06), np.complex128(5.1548428065253595e-05+0j)) <f>: (np.float32(0.0002820149), np.complex128(0.00036806208936858437+0j))\n",
      "Epoch 1600: <Test loss>: 2.3009688447928056e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.9208448e-05), np.complex128(6.160683681290466e-05+0j)) <f>: (np.float32(0.00025839626), np.complex128(0.0003545782605513078+0j))\n",
      "Epoch 1700: <Test loss>: 3.49464789906051e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.1877247e-06), np.complex128(5.6550900520258276e-05+0j)) <f>: (np.float32(0.00028979275), np.complex128(0.00035452504250618673+0j))\n",
      "Epoch 1800: <Test loss>: 2.262438329125871e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.3300342e-05), np.complex128(5.624853211148096e-05+0j)) <f>: (np.float32(0.0002743046), np.complex128(0.0003660360295494739+0j))\n",
      "Epoch 1900: <Test loss>: 2.055921413557371e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.603403e-05), np.complex128(4.9604201297945036e-05+0j)) <f>: (np.float32(0.00027157107), np.complex128(0.00036831587411057393+0j))\n",
      "Epoch 2000: <Test loss>: 2.158125425921753e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.4346347e-06), np.complex128(5.286725558568906e-05+0j)) <f>: (np.float32(0.00028417024), np.complex128(0.0003625183738542684+0j))\n",
      "Epoch 2100: <Test loss>: 2.15177055906679e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.2452396e-05), np.complex128(5.0433485440715765e-05+0j)) <f>: (np.float32(0.00026515246), np.complex128(0.00037446607870525325+0j))\n",
      "Epoch 2200: <Test loss>: 2.184364802815253e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.649264e-06), np.complex128(5.241884281128198e-05+0j)) <f>: (np.float32(0.00028495584), np.complex128(0.00036813281418421003+0j))\n",
      "Epoch 2300: <Test loss>: 2.3807424440747127e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.6176791e-05), np.complex128(5.1131563950193445e-05+0j)) <f>: (np.float32(0.00030378182), np.complex128(0.0003626619484015049+0j))\n",
      "Epoch 2400: <Test loss>: 2.2703925424139015e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.2508816e-06), np.complex128(5.293958598149012e-05+0j)) <f>: (np.float32(0.00028535418), np.complex128(0.0003677748768437833+0j))\n",
      "Epoch 2500: <Test loss>: 2.9658460789505625e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.7496568e-05), np.complex128(5.6399139496325136e-05+0j)) <f>: (np.float32(0.00027010855), np.complex128(0.00034915553838749043+0j))\n",
      "Epoch 2600: <Test loss>: 2.240481308035669e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-8.7398475e-06), np.complex128(5.32735545867558e-05+0j)) <f>: (np.float32(0.00029634507), np.complex128(0.0003648477257863064+0j))\n",
      "Epoch 2700: <Test loss>: 2.990185066664708e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.27699e-06), np.complex128(6.175816571772889e-05+0j)) <f>: (np.float32(0.00028132796), np.complex128(0.0003711440421413389+0j))\n",
      "Epoch 2800: <Test loss>: 2.430206222925335e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.0385839e-05), np.complex128(5.439090756375442e-05+0j)) <f>: (np.float32(0.00029799095), np.complex128(0.00036326107956383006+0j))\n",
      "Epoch 2900: <Test loss>: 2.427717163300258e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.8322042e-05), np.complex128(5.4777202225154835e-05+0j)) <f>: (np.float32(0.00030592713), np.complex128(0.00036641952931205056+0j))\n",
      "Epoch 3000: <Test loss>: 2.522021077311365e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.621128e-06), np.complex128(6.047089478543134e-05+0j)) <f>: (np.float32(0.00028922627), np.complex128(0.0003820523459239377+0j))\n",
      "Epoch 3100: <Test loss>: 2.5176118469971698e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-6.8020877e-06), np.complex128(5.38265005415675e-05+0j)) <f>: (np.float32(0.0002944072), np.complex128(0.00036328778373188246+0j))\n",
      "Epoch 3200: <Test loss>: 2.8317315354797756e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.3774987e-05), np.complex128(5.516769121698865e-05+0j)) <f>: (np.float32(0.00034137996), np.complex128(0.00036651873434491027+0j))\n",
      "Epoch 3300: <Test loss>: 2.7327157567924587e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.63923e-05), np.complex128(5.923651681782426e-05+0j)) <f>: (np.float32(0.00031399727), np.complex128(0.0003714339504550763+0j))\n",
      "Epoch 3400: <Test loss>: 2.8019953788316343e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.2716195e-05), np.complex128(5.3542966975785566e-05+0j)) <f>: (np.float32(0.00031032125), np.complex128(0.00036455889578814355+0j))\n",
      "Epoch 3500: <Test loss>: 2.711572506086668e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.9708512e-05), np.complex128(5.9018284774631254e-05+0j)) <f>: (np.float32(0.0003173136), np.complex128(0.0003716283326949731+0j))\n",
      "Epoch 3600: <Test loss>: 2.7088444767287e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.60316e-05), np.complex128(5.989287402911557e-05+0j)) <f>: (np.float32(0.00032363643), np.complex128(0.00037310625933540244+0j))\n",
      "Epoch 3700: <Test loss>: 2.763199290711782e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.311292e-05), np.complex128(5.915560350577715e-05+0j)) <f>: (np.float32(0.00032071793), np.complex128(0.00037499283586342686+0j))\n",
      "Epoch 3800: <Test loss>: 2.6981169867212884e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.8299284e-05), np.complex128(5.970523522595527e-05+0j)) <f>: (np.float32(0.0003259044), np.complex128(0.00037075588024966044+0j))\n",
      "Epoch 3900: <Test loss>: 2.8019455839967122e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.6290205e-05), np.complex128(5.563434021067142e-05+0j)) <f>: (np.float32(0.0003138952), np.complex128(0.0003649163256859469+0j))\n",
      "Epoch 4000: <Test loss>: 3.1272813885152573e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.0541143e-05), np.complex128(6.739176597109652e-05+0j)) <f>: (np.float32(0.00030814603), np.complex128(0.0003750576933737204+0j))\n",
      "Epoch 4100: <Test loss>: 3.1793408652447397e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.9205872e-05), np.complex128(6.18318043640432e-05+0j)) <f>: (np.float32(0.00031681065), np.complex128(0.00036770738697488133+0j))\n",
      "Epoch 4200: <Test loss>: 3.0424441774812294e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.3759721e-05), np.complex128(6.0922415645906274e-05+0j)) <f>: (np.float32(0.0003113647), np.complex128(0.0003711655133073382+0j))\n",
      "Epoch 4300: <Test loss>: 3.2245782222162234e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.1733645e-05), np.complex128(6.708353422138251e-05+0j)) <f>: (np.float32(0.00032933883), np.complex128(0.00036437821449907643+0j))\n",
      "Epoch 4400: <Test loss>: 3.131643779852311e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.1869513e-05), np.complex128(6.283874099772304e-05+0j)) <f>: (np.float32(0.00032947448), np.complex128(0.0003682733123605427+0j))\n",
      "Epoch 4500: <Test loss>: 3.2225175345956814e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.667141e-05), np.complex128(6.36512755695286e-05+0j)) <f>: (np.float32(0.00033427658), np.complex128(0.00036728306979629113+0j))\n",
      "Epoch 4600: <Test loss>: 3.1799183943803655e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.5327794e-05), np.complex128(6.15903964647881e-05+0j)) <f>: (np.float32(0.00032293255), np.complex128(0.0003667660491934525+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1pkl_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca1408c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 8.054565114434808e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.61376e-07), np.complex128(0.0001076374051512588+0j)) <f>: (np.float32(0.0002869437), np.complex128(0.00035611425765694367+0j))\n",
      "Epoch 400: <Test loss>: 4.757207534566987e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.195534e-05), np.complex128(6.468798877776924e-05+0j)) <f>: (np.float32(0.00021564956), np.complex128(0.0003600007607089774+0j))\n",
      "Epoch 600: <Test loss>: 5.148387572262436e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.2121334e-05), np.complex128(7.909796777733442e-05+0j)) <f>: (np.float32(0.00025548367), np.complex128(0.00035722200490365856+0j))\n",
      "Epoch 800: <Test loss>: 3.57261455974367e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.9263424e-05), np.complex128(5.559447028518108e-05+0j)) <f>: (np.float32(0.00023834172), np.complex128(0.00037367643455303416+0j))\n",
      "Epoch 1000: <Test loss>: 3.4820229757315246e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.4265471e-05), np.complex128(5.904138927157445e-05+0j)) <f>: (np.float32(0.00026333946), np.complex128(0.0003673619454090539+0j))\n",
      "Epoch 1200: <Test loss>: 3.2493542221345706e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.2833984e-05), np.complex128(5.9884350578801935e-05+0j)) <f>: (np.float32(0.00023477098), np.complex128(0.00036512599463487137+0j))\n",
      "Epoch 1400: <Test loss>: 3.4989991490874672e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(8.0198486e-05), np.complex128(6.228211608389225e-05+0j)) <f>: (np.float32(0.00020740666), np.complex128(0.00035556520473850774+0j))\n",
      "Epoch 1600: <Test loss>: 3.1948961805028375e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.5398916e-05), np.complex128(5.8847272646175786e-05+0j)) <f>: (np.float32(0.00024220633), np.complex128(0.0003581465336481898+0j))\n",
      "Epoch 1800: <Test loss>: 3.1836709695198806e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.0571867e-05), np.complex128(5.873382354030355e-05+0j)) <f>: (np.float32(0.0002470332), np.complex128(0.00036385326510524844+0j))\n",
      "Epoch 2000: <Test loss>: 3.3684673326206394e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.3600038e-05), np.complex128(5.765028289247935e-05+0j)) <f>: (np.float32(0.00025400476), np.complex128(0.000355582267496717+0j))\n",
      "Epoch 2200: <Test loss>: 3.2754874155216385e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.5364293e-05), np.complex128(6.055156626935327e-05+0j)) <f>: (np.float32(0.00024224068), np.complex128(0.0003534031820115106+0j))\n",
      "Epoch 2400: <Test loss>: 3.1823947210796177e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.8801474e-05), np.complex128(5.524598009921826e-05+0j)) <f>: (np.float32(0.0002488036), np.complex128(0.0003577671251417637+0j))\n",
      "Epoch 2600: <Test loss>: 3.289434289399651e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.059143e-05), np.complex128(5.612024427327201e-05+0j)) <f>: (np.float32(0.00025701354), np.complex128(0.0003578609385967506+0j))\n",
      "Epoch 2800: <Test loss>: 3.3138992421299918e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.6111382e-05), np.complex128(5.592519997932759e-05+0j)) <f>: (np.float32(0.00026149367), np.complex128(0.0003552963235784743+0j))\n",
      "Epoch 3000: <Test loss>: 3.958517027058406e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.793865e-05), np.complex128(6.308192494615977e-05+0j)) <f>: (np.float32(0.00022966643), np.complex128(0.00034677794769107274+0j))\n",
      "Epoch 3200: <Test loss>: 3.640363729573437e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.348633e-05), np.complex128(6.491335276845725e-05+0j)) <f>: (np.float32(0.00022411862), np.complex128(0.00036178898851356895+0j))\n",
      "Epoch 3400: <Test loss>: 3.7295042147889035e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.908284e-05), np.complex128(5.913561105929731e-05+0j)) <f>: (np.float32(0.00023852228), np.complex128(0.00035666809956513715+0j))\n",
      "Epoch 3600: <Test loss>: 3.56676150659041e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.5379107e-05), np.complex128(5.93641307087991e-05+0j)) <f>: (np.float32(0.0002322259), np.complex128(0.0003544675111987674+0j))\n",
      "Epoch 3800: <Test loss>: 3.572164814613643e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.6383706e-05), np.complex128(6.116764918681378e-05+0j)) <f>: (np.float32(0.00023122143), np.complex128(0.00035406571178658717+0j))\n",
      "Epoch 4000: <Test loss>: 3.632368361650151e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.5721775e-05), np.complex128(5.811242436848463e-05+0j)) <f>: (np.float32(0.00023188307), np.complex128(0.00036932936389000656+0j))\n",
      "Epoch 4200: <Test loss>: 3.6214762531017186e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.15468e-05), np.complex128(5.749362187571475e-05+0j)) <f>: (np.float32(0.0002160582), np.complex128(0.00035659131715319554+0j))\n",
      "Epoch 4400: <Test loss>: 3.834211838693591e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.0794755e-05), np.complex128(5.801194676467111e-05+0j)) <f>: (np.float32(0.00022681014), np.complex128(0.0003523866158611741+0j))\n",
      "Epoch 4600: <Test loss>: 4.115724095754558e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.0161216e-05), np.complex128(5.899723779894982e-05+0j)) <f>: (np.float32(0.00022744374), np.complex128(0.00035971348475384845+0j))\n",
      "Epoch 4800: <Test loss>: 3.851779183605686e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.9983074e-05), np.complex128(5.895682078688121e-05+0j)) <f>: (np.float32(0.00020762194), np.complex128(0.00036065945294921507+0j))\n",
      "Epoch 5000: <Test loss>: 3.84499981009867e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.761973e-05), np.complex128(5.926292365621455e-05+0j)) <f>: (np.float32(0.00021998538), np.complex128(0.0003613071717427234+0j))\n",
      "Epoch 5200: <Test loss>: 4.0939257814898156e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(8.868794e-05), np.complex128(5.314368099034898e-05+0j)) <f>: (np.float32(0.00019891701), np.complex128(0.0003616252431220553+0j))\n",
      "Epoch 5400: <Test loss>: 4.013683792436495e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(8.523746e-05), np.complex128(5.9431477859463304e-05+0j)) <f>: (np.float32(0.00020236749), np.complex128(0.00035746177154317884+0j))\n",
      "Epoch 5600: <Test loss>: 4.301654371374752e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.893991e-05), np.complex128(6.388109950514816e-05+0j)) <f>: (np.float32(0.00020866511), np.complex128(0.00036191540515710435+0j))\n",
      "Epoch 5800: <Test loss>: 4.210916358715622e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.340754e-05), np.complex128(5.887503530782526e-05+0j)) <f>: (np.float32(0.00021419737), np.complex128(0.00035981963540761486+0j))\n",
      "Epoch 6000: <Test loss>: 4.13702446167008e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.365203e-05), np.complex128(5.66444761115155e-05+0j)) <f>: (np.float32(0.00021395294), np.complex128(0.00035883706791304106+0j))\n",
      "Epoch 6200: <Test loss>: 4.222610186843667e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(8.506341e-05), np.complex128(5.720996144927698e-05+0j)) <f>: (np.float32(0.00020254153), np.complex128(0.00036250200882966617+0j))\n",
      "Epoch 6400: <Test loss>: 5.56153645447921e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(8.847341e-05), np.complex128(6.306359754578771e-05+0j)) <f>: (np.float32(0.0001991317), np.complex128(0.0003583732653553235+0j))\n",
      "Epoch 6600: <Test loss>: 4.336221536505036e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.854313e-05), np.complex128(5.7630393520282375e-05+0j)) <f>: (np.float32(0.00021906204), np.complex128(0.0003581922986297808+0j))\n",
      "Epoch 6800: <Test loss>: 4.384339263197035e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(8.010665e-05), np.complex128(6.0644638382382666e-05+0j)) <f>: (np.float32(0.00020749829), np.complex128(0.0003596226842394375+0j))\n",
      "Epoch 7000: <Test loss>: 4.376097876956919e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.820633e-05), np.complex128(5.823477750663799e-05+0j)) <f>: (np.float32(0.00020939876), np.complex128(0.00035892983476761735+0j))\n",
      "Epoch 7200: <Test loss>: 4.805954176845262e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.6134475e-05), np.complex128(6.150633145832268e-05+0j)) <f>: (np.float32(0.00022147053), np.complex128(0.00036575709468246584+0j))\n",
      "Epoch 7400: <Test loss>: 4.6242939788498916e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(8.1271755e-05), np.complex128(5.849570608930906e-05+0j)) <f>: (np.float32(0.00020633325), np.complex128(0.00035858769157884156+0j))\n",
      "Epoch 7600: <Test loss>: 4.980553057976067e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.252237e-05), np.complex128(5.606136110698903e-05+0j)) <f>: (np.float32(0.0002150826), np.complex128(0.00035515049725459675+0j))\n",
      "Epoch 7800: <Test loss>: 5.1994697969348636e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.6679335e-05), np.complex128(6.392566723929972e-05+0j)) <f>: (np.float32(0.00022092578), np.complex128(0.00035033588164450454+0j))\n",
      "Epoch 8000: <Test loss>: 5.022853656555526e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.35502e-05), np.complex128(6.0905650217359136e-05+0j)) <f>: (np.float32(0.00021405493), np.complex128(0.00035548487022820285+0j))\n",
      "Epoch 8200: <Test loss>: 4.8659630920155905e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.7365065e-05), np.complex128(6.0311268400836684e-05+0j)) <f>: (np.float32(0.00022023998), np.complex128(0.0003609611910191087+0j))\n",
      "Epoch 8400: <Test loss>: 5.108941422804492e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.25486e-05), np.complex128(5.8701926014153086e-05+0j)) <f>: (np.float32(0.0002150563), np.complex128(0.0003607027124328534+0j))\n",
      "Epoch 8600: <Test loss>: 5.372629857447464e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.384407e-05), np.complex128(5.980058290199863e-05+0j)) <f>: (np.float32(0.00021376075), np.complex128(0.0003580279823653159+0j))\n",
      "Epoch 8800: <Test loss>: 5.36594870936824e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(7.413711e-05), np.complex128(5.9029218577405673e-05+0j)) <f>: (np.float32(0.00021346787), np.complex128(0.0003623849164443343+0j))\n",
      "Epoch 9000: <Test loss>: 5.427821633929852e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.694622e-05), np.complex128(6.11373374188612e-05+0j)) <f>: (np.float32(0.00022065888), np.complex128(0.000353506383155029+0j))\n",
      "Epoch 9200: <Test loss>: 5.20286403116188e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.556172e-05), np.complex128(6.0663734875480704e-05+0j)) <f>: (np.float32(0.00022204324), np.complex128(0.00036268817684209794+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1pkl_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6bcbcf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b60e38b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 1.2305552445468493e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.627052e-05), np.complex128(8.293905471458069e-05+0j)) <f>: (np.float32(0.00033387556), np.complex128(0.0003809908711014371+0j))\n",
      "Epoch 800: <Test loss>: 7.095457476680167e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-7.006801e-06), np.complex128(6.714590209130535e-05+0j)) <f>: (np.float32(0.00029461161), np.complex128(0.0003509541687658601+0j))\n",
      "Epoch 1200: <Test loss>: 5.479331775859464e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.5693008e-06), np.complex128(4.886342624898929e-05+0j)) <f>: (np.float32(0.0002840355), np.complex128(0.0003570465883318085+0j))\n",
      "Epoch 1600: <Test loss>: 5.164208232599776e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.4384686e-06), np.complex128(5.166957999157205e-05+0j)) <f>: (np.float32(0.00028416642), np.complex128(0.0003393449915749599+0j))\n",
      "Epoch 2000: <Test loss>: 6.065360594220692e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.0811073e-05), np.complex128(7.353221022406782e-05+0j)) <f>: (np.float32(0.00027679396), np.complex128(0.000332673992272931+0j))\n",
      "Epoch 2400: <Test loss>: 4.143275873502716e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.2478764e-05), np.complex128(4.859866409587482e-05+0j)) <f>: (np.float32(0.00022512613), np.complex128(0.00034934668568066356+0j))\n",
      "Epoch 2800: <Test loss>: 4.414277555042645e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.6764242e-05), np.complex128(5.7130114559617947e-05+0j)) <f>: (np.float32(0.00024084075), np.complex128(0.00034397721327713123+0j))\n",
      "Epoch 3200: <Test loss>: 4.432316018210258e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.367413e-05), np.complex128(5.953097229319413e-05+0j)) <f>: (np.float32(0.00026393068), np.complex128(0.00034181182874275457+0j))\n",
      "Epoch 3600: <Test loss>: 4.286475814296864e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.617092e-05), np.complex128(5.289648507367159e-05+0j)) <f>: (np.float32(0.0002514339), np.complex128(0.0003481297748396062+0j))\n",
      "Epoch 4000: <Test loss>: 4.9293994379695505e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.028347e-05), np.complex128(5.058851709093204e-05+0j)) <f>: (np.float32(0.00022732133), np.complex128(0.0003505747285442701+0j))\n",
      "Epoch 4400: <Test loss>: 4.472254659049213e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.6232729e-05), np.complex128(5.503846778144296e-05+0j)) <f>: (np.float32(0.00027137215), np.complex128(0.0003462909296333381+0j))\n",
      "Epoch 4800: <Test loss>: 4.657444605982164e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.4086052e-05), np.complex128(5.680360694667332e-05+0j)) <f>: (np.float32(0.00026351892), np.complex128(0.00034852637296489735+0j))\n",
      "Epoch 5200: <Test loss>: 5.0476401156629436e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.2387071e-05), np.complex128(5.995834998510627e-05+0j)) <f>: (np.float32(0.000275218), np.complex128(0.00034183412450301684+0j))\n",
      "Epoch 5600: <Test loss>: 4.760325282404665e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.3993016e-05), np.complex128(6.133313494794963e-05+0j)) <f>: (np.float32(0.0002536122), np.complex128(0.0003509637150302114+0j))\n",
      "Epoch 6000: <Test loss>: 4.714053829957265e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.343439e-05), np.complex128(5.823435728071554e-05+0j)) <f>: (np.float32(0.0002441708), np.complex128(0.00035375826498718115+0j))\n",
      "Epoch 6400: <Test loss>: 4.8490287554159295e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.6085315e-05), np.complex128(5.822819661011676e-05+0j)) <f>: (np.float32(0.00024151956), np.complex128(0.00035321114669374664+0j))\n",
      "Epoch 6800: <Test loss>: 5.042684279032983e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.835481e-05), np.complex128(5.861489960425313e-05+0j)) <f>: (np.float32(0.0002492503), np.complex128(0.00035351075984765514+0j))\n",
      "Epoch 7200: <Test loss>: 5.287217845761916e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.812675e-05), np.complex128(5.5687379857995194e-05+0j)) <f>: (np.float32(0.0002494784), np.complex128(0.0003563016308456058+0j))\n",
      "Epoch 7600: <Test loss>: 5.216291356191505e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.9463581e-05), np.complex128(5.908108476366301e-05+0j)) <f>: (np.float32(0.00025814158), np.complex128(0.00035282517314838135+0j))\n",
      "Epoch 8000: <Test loss>: 5.497054189618211e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.742565e-05), np.complex128(6.142181451077088e-05+0j)) <f>: (np.float32(0.0002501794), np.complex128(0.00035191745344074766+0j))\n",
      "Epoch 8400: <Test loss>: 5.590808086708421e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.5948022e-05), np.complex128(5.2370576296133836e-05+0j)) <f>: (np.float32(0.00025165686), np.complex128(0.00035983270205516545+0j))\n",
      "Epoch 8800: <Test loss>: 6.122262675489765e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.801478e-05), np.complex128(6.358729022624392e-05+0j)) <f>: (np.float32(0.00025959013), np.complex128(0.00034797614658539503+0j))\n",
      "Epoch 9200: <Test loss>: 5.595466063823551e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.2639913e-05), np.complex128(6.003511257507041e-05+0j)) <f>: (np.float32(0.00025496492), np.complex128(0.00035146300685639766+0j))\n",
      "Epoch 9600: <Test loss>: 5.730996690544998e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.7439885e-05), np.complex128(6.152876200803177e-05+0j)) <f>: (np.float32(0.00027016536), np.complex128(0.0003543267593011231+0j))\n",
      "Epoch 10000: <Test loss>: 5.957880603091326e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.9476952e-05), np.complex128(5.8141312918454614e-05+0j)) <f>: (np.float32(0.00024812814), np.complex128(0.00034668457824838124+0j))\n",
      "Epoch 10400: <Test loss>: 6.781741376471473e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.2979637e-05), np.complex128(6.891047831237545e-05+0j)) <f>: (np.float32(0.00027462572), np.complex128(0.00035051018818561613+0j))\n",
      "Epoch 10800: <Test loss>: 5.778634204034461e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.76122e-05), np.complex128(5.403725573485748e-05+0j)) <f>: (np.float32(0.00021999278), np.complex128(0.00036129829149681526+0j))\n",
      "Epoch 11200: <Test loss>: 6.661213774350472e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.6065113e-05), np.complex128(6.674057436713481e-05+0j)) <f>: (np.float32(0.00023153998), np.complex128(0.00035179839471525046+0j))\n",
      "Epoch 11600: <Test loss>: 6.2282360886456445e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.1930645e-05), np.complex128(5.928048989265163e-05+0j)) <f>: (np.float32(0.00023567442), np.complex128(0.000351956241086268+0j))\n",
      "Epoch 12000: <Test loss>: 6.397042852768209e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.8459537e-05), np.complex128(6.177977563757058e-05+0j)) <f>: (np.float32(0.00024914552), np.complex128(0.00035148454145272494+0j))\n",
      "Epoch 12400: <Test loss>: 6.54503082841984e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.77693e-05), np.complex128(6.839884135861867e-05+0j)) <f>: (np.float32(0.00026983558), np.complex128(0.00035556596590244273+0j))\n",
      "Epoch 12800: <Test loss>: 6.408982699213084e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.2595923e-05), np.complex128(6.33055406384346e-05+0j)) <f>: (np.float32(0.0002650089), np.complex128(0.0003514723628297652+0j))\n",
      "Epoch 13200: <Test loss>: 7.083524451445555e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.7354916e-05), np.complex128(6.912550712400873e-05+0j)) <f>: (np.float32(0.00025025033), np.complex128(0.00034175620034517277+0j))\n",
      "Epoch 13600: <Test loss>: 6.66469077259535e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.734814e-05), np.complex128(6.312914089650182e-05+0j)) <f>: (np.float32(0.00025025682), np.complex128(0.00035411728064318245+0j))\n",
      "Epoch 14000: <Test loss>: 6.773861514375312e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.645273e-05), np.complex128(6.433523686664961e-05+0j)) <f>: (np.float32(0.00024115227), np.complex128(0.0003515813995634518+0j))\n",
      "Epoch 14400: <Test loss>: 6.801648396503879e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.0509766e-05), np.complex128(6.486478099485601e-05+0j)) <f>: (np.float32(0.00025709503), np.complex128(0.0003517266232992141+0j))\n",
      "Epoch 14800: <Test loss>: 7.04577587384847e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.8435268e-05), np.complex128(6.856033497349134e-05+0j)) <f>: (np.float32(0.0002591697), np.complex128(0.00035021079703785536+0j))\n",
      "Epoch 15200: <Test loss>: 6.945575933059445e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.309655e-05), np.complex128(6.624971084577034e-05+0j)) <f>: (np.float32(0.00024450856), np.complex128(0.00034721365061352375+0j))\n",
      "Epoch 15600: <Test loss>: 7.0980026976030786e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.849296e-05), np.complex128(6.94528869039622e-05+0j)) <f>: (np.float32(0.000259112), np.complex128(0.0003474110457939966+0j))\n",
      "Epoch 16000: <Test loss>: 7.261541213665623e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.249478e-05), np.complex128(7.035535775196085e-05+0j)) <f>: (np.float32(0.00025511012), np.complex128(0.0003462577238566744+0j))\n",
      "Epoch 16400: <Test loss>: 7.364551947830478e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.3646694e-05), np.complex128(6.846327071419876e-05+0j)) <f>: (np.float32(0.0003012521), np.complex128(0.0003555840118307347+0j))\n",
      "Epoch 16800: <Test loss>: 7.2881516643974464e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.998035e-05), np.complex128(6.906282210244632e-05+0j)) <f>: (np.float32(0.00026762456), np.complex128(0.00034914938564568264+0j))\n",
      "Epoch 17200: <Test loss>: 7.321784778469009e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.6212688e-05), np.complex128(6.853692918249055e-05+0j)) <f>: (np.float32(0.00024139226), np.complex128(0.0003447910243842861+0j))\n",
      "Epoch 17600: <Test loss>: 7.545383141405182e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.3618675e-05), np.complex128(7.137623716459561e-05+0j)) <f>: (np.float32(0.00025398633), np.complex128(0.000345144331310775+0j))\n",
      "Epoch 18000: <Test loss>: 7.707761142228264e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.3359316e-05), np.complex128(7.032207268738723e-05+0j)) <f>: (np.float32(0.00023424573), np.complex128(0.0003411961739800075+0j))\n",
      "Epoch 18400: <Test loss>: 7.5736302278528456e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.5409914e-05), np.complex128(7.022649111200968e-05+0j)) <f>: (np.float32(0.00025219517), np.complex128(0.00034656637583231083+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1pkl_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ded69902",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "046104d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 2.6263362087775022e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.7667974e-05), np.complex128(0.00016113888076379668+0j)) <f>: (np.float32(0.00034527286), np.complex128(0.0003074257405371811+0j))\n",
      "Epoch 1600: <Test loss>: 1.7718686649459414e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.1125106e-06), np.complex128(0.00014154081234776884+0j)) <f>: (np.float32(0.00028549248), np.complex128(0.00031220918014110286+0j))\n",
      "Epoch 2400: <Test loss>: 1.3387407307163812e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(3.9413102e-05), np.complex128(0.00011860894897078929+0j)) <f>: (np.float32(0.00024819194), np.complex128(0.00033258788560278584+0j))\n",
      "Epoch 3200: <Test loss>: 1.1307039130770136e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(6.0838236e-05), np.complex128(0.00011338768152843135+0j)) <f>: (np.float32(0.00022676683), np.complex128(0.0003416634334905964+0j))\n",
      "Epoch 4000: <Test loss>: 1.0329809811082669e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.1170442e-05), np.complex128(0.00011417995011046794+0j)) <f>: (np.float32(0.00024643473), np.complex128(0.0003471878979003901+0j))\n",
      "Epoch 4800: <Test loss>: 1.116750809160294e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00011604184), np.complex128(0.0001073896145752572+0j)) <f>: (np.float32(0.00017156325), np.complex128(0.0003579540226029665+0j))\n",
      "Epoch 5600: <Test loss>: 8.715833246242255e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.4109027e-05), np.complex128(9.29652567132327e-05+0j)) <f>: (np.float32(0.00026349604), np.complex128(0.00035513549598204474+0j))\n",
      "Epoch 6400: <Test loss>: 9.154694453172851e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.9009438e-05), np.complex128(9.822714335182949e-05+0j)) <f>: (np.float32(0.00026859576), np.complex128(0.0003541565440161621+0j))\n",
      "Epoch 7200: <Test loss>: 8.3359354903223e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(4.6321962e-05), np.complex128(8.585179123003883e-05+0j)) <f>: (np.float32(0.000241283), np.complex128(0.0003472846608656251+0j))\n",
      "Epoch 8000: <Test loss>: 8.01671740191523e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.8451237e-05), np.complex128(8.050638270715942e-05+0j)) <f>: (np.float32(0.00026915382), np.complex128(0.00035207625126668394+0j))\n",
      "Epoch 8800: <Test loss>: 9.479484106122982e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-9.1470574e-05), np.complex128(9.379510776455918e-05+0j)) <f>: (np.float32(0.0003790754), np.complex128(0.0003770704962542965+0j))\n",
      "Epoch 9600: <Test loss>: 7.919883501017466e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.1190232e-05), np.complex128(8.400267823943945e-05+0j)) <f>: (np.float32(0.0002764147), np.complex128(0.0003616670754233155+0j))\n",
      "Epoch 10400: <Test loss>: 8.183375030057505e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.1522e-05), np.complex128(8.060909226563647e-05+0j)) <f>: (np.float32(0.00026608285), np.complex128(0.00035770302879540515+0j))\n",
      "Epoch 11200: <Test loss>: 7.99297140474664e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.0335336e-05), np.complex128(7.658970267661994e-05+0j)) <f>: (np.float32(0.00026726988), np.complex128(0.00035799163678742037+0j))\n",
      "Epoch 12000: <Test loss>: 1.0175939678447321e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.0381215e-06), np.complex128(9.081626891862219e-05+0j)) <f>: (np.float32(0.000291643), np.complex128(0.0003631897204449252+0j))\n",
      "Epoch 12800: <Test loss>: 7.913939043646678e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.1519675e-06), np.complex128(7.181988473561804e-05+0j)) <f>: (np.float32(0.00028545316), np.complex128(0.0003622497147003826+0j))\n",
      "Epoch 13600: <Test loss>: 7.872892638260964e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.309272e-06), np.complex128(7.508300245368999e-05+0j)) <f>: (np.float32(0.00029191433), np.complex128(0.0003695923460295439+0j))\n",
      "Epoch 14400: <Test loss>: 8.270563739642967e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.3143717e-05), np.complex128(7.299796828720018e-05+0j)) <f>: (np.float32(0.00026446127), np.complex128(0.00036445658266921597+0j))\n",
      "Epoch 15200: <Test loss>: 8.762038305576425e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-9.045419e-06), np.complex128(7.356482927019833e-05+0j)) <f>: (np.float32(0.00029665057), np.complex128(0.0003591637023866415+0j))\n",
      "Epoch 16000: <Test loss>: 8.793634151516017e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.4958322e-05), np.complex128(7.583396996346265e-05+0j)) <f>: (np.float32(0.00027264675), np.complex128(0.000365861215565739+0j))\n",
      "Epoch 16800: <Test loss>: 8.730729859962594e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(5.1931743e-06), np.complex128(7.330673919470147e-05+0j)) <f>: (np.float32(0.00028241184), np.complex128(0.00036761262206497567+0j))\n",
      "Epoch 17600: <Test loss>: 8.715638614376076e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.9395943e-06), np.complex128(7.479209511228789e-05+0j)) <f>: (np.float32(0.0002856655), np.complex128(0.000365946719647769+0j))\n",
      "Epoch 18400: <Test loss>: 8.780593816481996e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.334617e-06), np.complex128(7.516422498858568e-05+0j)) <f>: (np.float32(0.00028627043), np.complex128(0.0003641364815193908+0j))\n",
      "Epoch 19200: <Test loss>: 9.190853234031238e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(2.359256e-06), np.complex128(7.53246244303018e-05+0j)) <f>: (np.float32(0.00028524548), np.complex128(0.00036884012577079493+0j))\n",
      "Epoch 20000: <Test loss>: 9.154322469839826e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.1690935e-05), np.complex128(7.25279574861379e-05+0j)) <f>: (np.float32(0.00029929608), np.complex128(0.0003671627107490716+0j))\n",
      "Epoch 20800: <Test loss>: 9.576126103638671e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.434097e-05), np.complex128(7.758638341915497e-05+0j)) <f>: (np.float32(0.0002732642), np.complex128(0.0003670357232325849+0j))\n",
      "Epoch 21600: <Test loss>: 9.35824846237665e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.3804012e-06), np.complex128(7.944232309879626e-05+0j)) <f>: (np.float32(0.00028898526), np.complex128(0.00036849611138734564+0j))\n",
      "Epoch 22400: <Test loss>: 9.278949619329069e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.318562e-06), np.complex128(7.743488008092898e-05+0j)) <f>: (np.float32(0.00029092378), np.complex128(0.0003653292571206763+0j))\n",
      "Epoch 23200: <Test loss>: 1.000378506432753e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.0448061e-06), np.complex128(8.124418049509864e-05+0j)) <f>: (np.float32(0.00028656027), np.complex128(0.00036589568994896106+0j))\n",
      "Epoch 24000: <Test loss>: 9.745600436872337e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-8.687667e-06), np.complex128(7.655490321296735e-05+0j)) <f>: (np.float32(0.00029629303), np.complex128(0.00036628271009473697+0j))\n",
      "Epoch 24800: <Test loss>: 9.92613877315307e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.324761e-07), np.complex128(7.995850324978997e-05+0j)) <f>: (np.float32(0.00028793735), np.complex128(0.0003682320826473977+0j))\n",
      "Epoch 25600: <Test loss>: 9.86332815955393e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.381456e-06), np.complex128(7.791464329490827e-05+0j)) <f>: (np.float32(0.00028998672), np.complex128(0.00036864523608827476+0j))\n",
      "Epoch 26400: <Test loss>: 9.984074495150708e-06 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.2161344e-05), np.complex128(7.323760013727366e-05+0j)) <f>: (np.float32(0.00031976617), np.complex128(0.00036527524619645637+0j))\n",
      "Epoch 27200: <Test loss>: 1.040728238876909e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.7134698e-05), np.complex128(7.874045066283013e-05+0j)) <f>: (np.float32(0.00031473985), np.complex128(0.00036420184647230764+0j))\n",
      "Epoch 28000: <Test loss>: 1.0131106137123425e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.945501e-06), np.complex128(7.768600074914613e-05+0j)) <f>: (np.float32(0.0002935504), np.complex128(0.0003668795260500932+0j))\n",
      "Epoch 28800: <Test loss>: 1.0099947758135386e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.7329317e-05), np.complex128(7.606411105072177e-05+0j)) <f>: (np.float32(0.00027027563), np.complex128(0.0003660303525351255+0j))\n",
      "Epoch 29600: <Test loss>: 1.035544482874684e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.3186191e-05), np.complex128(8.012380268433757e-05+0j)) <f>: (np.float32(0.00030079085), np.complex128(0.0003664346891604223+0j))\n",
      "Epoch 30400: <Test loss>: 1.0687050234992057e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.98259e-07), np.complex128(7.902989117789918e-05+0j)) <f>: (np.float32(0.00028810318), np.complex128(0.0003559081408063825+0j))\n",
      "Epoch 31200: <Test loss>: 1.032392174238339e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-6.345283e-06), np.complex128(7.923099703255503e-05+0j)) <f>: (np.float32(0.00029395043), np.complex128(0.0003655496140798545+0j))\n",
      "Epoch 32000: <Test loss>: 1.1415832886996213e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.4808169e-05), np.complex128(8.515386733198545e-05+0j)) <f>: (np.float32(0.0003024131), np.complex128(0.0003715251632666187+0j))\n",
      "Epoch 32800: <Test loss>: 1.1019956218660809e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(1.7374042e-06), np.complex128(8.149961442561405e-05+0j)) <f>: (np.float32(0.00028586763), np.complex128(0.00036371111774038994+0j))\n",
      "Epoch 33600: <Test loss>: 1.0754934919532388e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-4.4837743e-05), np.complex128(7.867097859618076e-05+0j)) <f>: (np.float32(0.00033244284), np.complex128(0.0003617480759520635+0j))\n",
      "Epoch 34400: <Test loss>: 1.0929366908385418e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-9.502327e-06), np.complex128(8.221564768228729e-05+0j)) <f>: (np.float32(0.0002971076), np.complex128(0.00036731912993771105+0j))\n",
      "Epoch 35200: <Test loss>: 1.1192653801117558e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.4115127e-05), np.complex128(8.293903092820772e-05+0j)) <f>: (np.float32(0.00030172017), np.complex128(0.00036383981787573033+0j))\n",
      "Epoch 36000: <Test loss>: 1.1471399375295732e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-1.8462726e-05), np.complex128(8.261734402018467e-05+0j)) <f>: (np.float32(0.00030606773), np.complex128(0.00036681333650291344+0j))\n",
      "Epoch 36800: <Test loss>: 1.0998773177561816e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-2.615516e-05), np.complex128(7.437971869292775e-05+0j)) <f>: (np.float32(0.00031376033), np.complex128(0.0003625698158502078+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1pkl_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d6729b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6329ffcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00018035263929050416 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 8.634312689537182e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00010962964), np.complex128(0.00033992204898317043+0j)) <f>: (np.float32(0.0003972343), np.complex128(0.0002765369151763884+0j))\n",
      "Epoch 3200: <Test loss>: 8.601634908700362e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00028898154), np.complex128(0.0003287590724339912+0j)) <f>: (np.float32(0.0005765875), np.complex128(0.0002811476974277244+0j))\n",
      "Epoch 4800: <Test loss>: 5.787106420029886e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-7.867418e-05), np.complex128(0.0002612220906559233+0j)) <f>: (np.float32(0.0003662796), np.complex128(0.00030452218556168873+0j))\n",
      "Epoch 6400: <Test loss>: 4.654817166738212e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00012275495), np.complex128(0.00023888487867435983+0j)) <f>: (np.float32(0.0004103601), np.complex128(0.000306669143585806+0j))\n",
      "Epoch 8000: <Test loss>: 4.31966946052853e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00011767364), np.complex128(0.0002293922129501774+0j)) <f>: (np.float32(0.0004052786), np.complex128(0.0003058169571302623+0j))\n",
      "Epoch 9600: <Test loss>: 3.90987224818673e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-8.160053e-05), np.complex128(0.00022787416662734157+0j)) <f>: (np.float32(0.00036920529), np.complex128(0.0003025262551483381+0j))\n",
      "Epoch 11200: <Test loss>: 4.110737063456327e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.1231884e-05), np.complex128(0.00023406834429684823+0j)) <f>: (np.float32(0.00033883672), np.complex128(0.00029758363713651263+0j))\n",
      "Epoch 12800: <Test loss>: 4.001740308012813e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00013569121), np.complex128(0.00018258667268733012+0j)) <f>: (np.float32(0.00042329624), np.complex128(0.00034892005329510445+0j))\n",
      "Epoch 14400: <Test loss>: 3.471016316325404e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-3.622017e-05), np.complex128(0.00021370522683825048+0j)) <f>: (np.float32(0.00032382525), np.complex128(0.0002992576586356925+0j))\n",
      "Epoch 16000: <Test loss>: 3.1363069865619764e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00012603901), np.complex128(0.00021558668136729576+0j)) <f>: (np.float32(0.00041364427), np.complex128(0.00029746162890076735+0j))\n",
      "Epoch 17600: <Test loss>: 3.1510648113908246e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.471002e-05), np.complex128(0.00022233833233126907+0j)) <f>: (np.float32(0.00034231512), np.complex128(0.000310526183250523+0j))\n",
      "Epoch 19200: <Test loss>: 3.0270120987552218e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-8.0692844e-05), np.complex128(0.000221370179378714+0j)) <f>: (np.float32(0.00036829786), np.complex128(0.0002975924539520929+0j))\n",
      "Epoch 20800: <Test loss>: 3.114895662292838e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-5.989032e-05), np.complex128(0.00021553733257217755+0j)) <f>: (np.float32(0.00034749546), np.complex128(0.00029054153870101463+0j))\n",
      "Epoch 22400: <Test loss>: 2.83579920505872e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00010911688), np.complex128(0.00021329210511253734+0j)) <f>: (np.float32(0.00039672237), np.complex128(0.0003015779717460026+0j))\n",
      "Epoch 24000: <Test loss>: 2.8586830012500286e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00010816819), np.complex128(0.0002134271324230873+0j)) <f>: (np.float32(0.00039577307), np.complex128(0.0002980527044114472+0j))\n",
      "Epoch 25600: <Test loss>: 2.8352591471048072e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-9.335329e-05), np.complex128(0.000216478955790082+0j)) <f>: (np.float32(0.0003809584), np.complex128(0.00029983906102136525+0j))\n",
      "Epoch 27200: <Test loss>: 2.665437204996124e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.0001285477), np.complex128(0.00020461226732542708+0j)) <f>: (np.float32(0.00041615267), np.complex128(0.0002966283763981064+0j))\n",
      "Epoch 28800: <Test loss>: 2.6238923965138383e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00012124777), np.complex128(0.00020871754157844108+0j)) <f>: (np.float32(0.00040885294), np.complex128(0.00030142906905122115+0j))\n",
      "Epoch 30400: <Test loss>: 2.6389032427687198e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00015156344), np.complex128(0.0002111008251467892+0j)) <f>: (np.float32(0.00043916883), np.complex128(0.00030219127958661676+0j))\n",
      "Epoch 32000: <Test loss>: 3.24395477946382e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00013284848), np.complex128(0.00023227891131602626+0j)) <f>: (np.float32(0.00042045326), np.complex128(0.00027925408013330135+0j))\n",
      "Epoch 33600: <Test loss>: 2.9675205951207317e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00015831242), np.complex128(0.0002187609252671672+0j)) <f>: (np.float32(0.0004459171), np.complex128(0.00029556138313707714+0j))\n",
      "Epoch 35200: <Test loss>: 2.5510569685138762e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00010803157), np.complex128(0.00020782417298249831+0j)) <f>: (np.float32(0.00039563616), np.complex128(0.0003057056051896068+0j))\n",
      "Epoch 36800: <Test loss>: 2.5979676138376817e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00014061299), np.complex128(0.00020750007572201446+0j)) <f>: (np.float32(0.00042821825), np.complex128(0.0003050658151870879+0j))\n",
      "Epoch 38400: <Test loss>: 2.6559111574897543e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016136332), np.complex128(0.00020957890957395053+0j)) <f>: (np.float32(0.00044896797), np.complex128(0.00033464464570060734+0j))\n",
      "Epoch 40000: <Test loss>: 2.905643850681372e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.000114394854), np.complex128(0.00022861623803362407+0j)) <f>: (np.float32(0.00040199995), np.complex128(0.0003144180461652656+0j))\n",
      "Epoch 41600: <Test loss>: 3.253085014875978e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00027211584), np.complex128(0.00022110584934470847+0j)) <f>: (np.float32(0.00055972085), np.complex128(0.0003654031534526978+0j))\n",
      "Epoch 43200: <Test loss>: 2.497310379112605e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.0001158689), np.complex128(0.0002096203454356613+0j)) <f>: (np.float32(0.00040347356), np.complex128(0.0003126942318586721+0j))\n",
      "Epoch 44800: <Test loss>: 2.7184689315618016e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016248412), np.complex128(0.00021543735051780086+0j)) <f>: (np.float32(0.00045008905), np.complex128(0.00030887585297881953+0j))\n",
      "Epoch 46400: <Test loss>: 2.606628549983725e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00013134441), np.complex128(0.00021318386125794968+0j)) <f>: (np.float32(0.00041894923), np.complex128(0.0003086809632962994+0j))\n",
      "Epoch 48000: <Test loss>: 2.6175046514254063e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00014250656), np.complex128(0.00021289081314298044+0j)) <f>: (np.float32(0.00043011157), np.complex128(0.0003094118392497049+0j))\n",
      "Epoch 49600: <Test loss>: 3.2232430385192856e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00014054394), np.complex128(0.00022982422105602735+0j)) <f>: (np.float32(0.0004281493), np.complex128(0.00029164002511985386+0j))\n",
      "Epoch 51200: <Test loss>: 2.736069836828392e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00014065023), np.complex128(0.00021643858238636383+0j)) <f>: (np.float32(0.00042825518), np.complex128(0.00030560858850306016+0j))\n",
      "Epoch 52800: <Test loss>: 2.7141937607666478e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00015675856), np.complex128(0.00021288399438272953+0j)) <f>: (np.float32(0.00044436357), np.complex128(0.0003071767765001131+0j))\n",
      "Epoch 54400: <Test loss>: 2.5234612621716224e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00011739477), np.complex128(0.00020052012315029157+0j)) <f>: (np.float32(0.0004050001), np.complex128(0.000322177573324651+0j))\n",
      "Epoch 56000: <Test loss>: 2.6967873054672964e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016121617), np.complex128(0.00021496094118241013+0j)) <f>: (np.float32(0.00044882103), np.complex128(0.0003105901844513897+0j))\n",
      "Epoch 57600: <Test loss>: 2.662698352651205e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016377156), np.complex128(0.00021224579013840852+0j)) <f>: (np.float32(0.00045137663), np.complex128(0.00031406610299082686+0j))\n",
      "Epoch 59200: <Test loss>: 2.7356372811482288e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00015552835), np.complex128(0.0002153913635300622+0j)) <f>: (np.float32(0.00044313318), np.complex128(0.0003095281070407738+0j))\n",
      "Epoch 60800: <Test loss>: 2.729375773924403e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016096396), np.complex128(0.00021440543422810882+0j)) <f>: (np.float32(0.00044856913), np.complex128(0.000309032240452295+0j))\n",
      "Epoch 62400: <Test loss>: 2.810181467793882e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016208763), np.complex128(0.00021747792002442195+0j)) <f>: (np.float32(0.00044969236), np.complex128(0.0003056243509395471+0j))\n",
      "Epoch 64000: <Test loss>: 2.9735301723121665e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00012955471), np.complex128(0.0002272958564701547+0j)) <f>: (np.float32(0.00041715993), np.complex128(0.00029842672134000045+0j))\n",
      "Epoch 65600: <Test loss>: 2.900963772844989e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00017889927), np.complex128(0.00022016587531532982+0j)) <f>: (np.float32(0.00046650396), np.complex128(0.000300628958894896+0j))\n",
      "Epoch 67200: <Test loss>: 2.8239366656634957e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016246614), np.complex128(0.00021825002569097243+0j)) <f>: (np.float32(0.00045007106), np.complex128(0.0003084044387817521+0j))\n",
      "Epoch 68800: <Test loss>: 2.7892532671103254e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016170641), np.complex128(0.000217071426767976+0j)) <f>: (np.float32(0.0004493111), np.complex128(0.00030546038354188577+0j))\n",
      "Epoch 70400: <Test loss>: 2.7337215215084143e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00020233516), np.complex128(0.00021460944202026682+0j)) <f>: (np.float32(0.00048994017), np.complex128(0.00032215936882053924+0j))\n",
      "Epoch 72000: <Test loss>: 2.886606853280682e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00017162759), np.complex128(0.00021840411381506098+0j)) <f>: (np.float32(0.00045923225), np.complex128(0.0003122035348419184+0j))\n",
      "Epoch 73600: <Test loss>: 2.8431371902115643e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00016607343), np.complex128(0.00022019757462170557+0j)) <f>: (np.float32(0.00045367857), np.complex128(0.0003077777154267838+0j))\n",
      "Epoch 75200: <Test loss>: 3.052057945751585e-05 <O>: (np.float32(0.00028760495), np.complex128(0.00037218299919742953+0j)) <O-f>: (np.float32(-0.00015672679), np.complex128(0.00022669942109676598+0j)) <f>: (np.float32(0.00044433176), np.complex128(0.00029646079347175387+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1pkl_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201caf9",
   "metadata": {},
   "source": [
    "## m^2=0.01, lambda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b28572b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0026315458), np.complex128(0.00014273172330526426+0j))\n",
      "bin size 1: (np.float32(0.0026315458), np.complex128(0.00014273153766844907+0j))\n",
      "jack bin size 2: (np.float32(0.0026315458), np.complex128(0.0002009817562697372+0j))\n",
      "bin size 2: (np.float32(0.0026315458), np.complex128(0.0002009828085752801+0j))\n",
      "jack bin size 4: (np.float32(0.0026315458), np.complex128(0.0002824853088164997+0j))\n",
      "bin size 4: (np.float32(0.0026315458), np.complex128(0.0002824858900202818+0j))\n",
      "jack bin size 5: (np.float32(0.0026315458), np.complex128(0.00031500454263360945+0j))\n",
      "bin size 5: (np.float32(0.0026315458), np.complex128(0.00031500483356931857+0j))\n",
      "jack bin size 10: (np.float32(0.0026315458), np.complex128(0.00043987794528254736+0j))\n",
      "bin size 10: (np.float32(0.0026315458), np.complex128(0.0004398786801819173+0j))\n",
      "jack bin size 20: (np.float32(0.0026315458), np.complex128(0.0006075311796406414+0j))\n",
      "bin size 20: (np.float32(0.0026315458), np.complex128(0.0006075311335893693+0j))\n",
      "jack bin size 50: (np.float32(0.0026315458), np.complex128(0.0009034917664639983+0j))\n",
      "bin size 50: (np.float32(0.0026315458), np.complex128(0.0009034917877824204+0j))\n",
      "jack bin size 100: (np.float32(0.0026315458), np.complex128(0.001168088945640063+0j))\n",
      "bin size 100: (np.float32(0.0026315458), np.complex128(0.001168090030808103+0j))\n",
      "jack bin size 200: (np.float32(0.0026315458), np.complex128(0.0014455953358068145+0j))\n",
      "bin size 200: (np.float32(0.0026315458), np.complex128(0.0014455951677370509+0j))\n",
      "jack bin size 500: (np.float32(0.0026315458), np.complex128(0.0016686322408391977+0j))\n",
      "bin size 500: (np.float32(0.0026315458), np.complex128(0.001668630398476752+0j))\n",
      "jack bin size 1000: (np.float32(0.0026315458), np.complex128(0.0016684701008113714+0j))\n",
      "bin size 1000: (np.float32(0.0026315458), np.complex128(0.0016684703889283058+0j))\n",
      "jack bin size 2000: (np.float32(0.0026315458), np.complex128(0.0017495480133220553+0j))\n",
      "bin size 2000: (np.float32(0.0026315458), np.complex128(0.001749547996691295+0j))\n",
      "jack bin size 5000: (np.float32(0.0026315458), np.complex128(0.001766026048075269+0j))\n",
      "bin size 5000: (np.float32(0.0026315458), np.complex128(0.0017660262817659504+0j))\n",
      "jack bin size 10000: (np.float32(0.0026315458), np.complex128(0.0017377176554873586+0j))\n",
      "bin size 10000: (np.float32(0.0026315458), np.complex128(0.0017377175390720367+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_32x32_0.01_0.1_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(32,32), m2=0.01, lamda=0.1)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a110769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0cb1af3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.00018472775991540402 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00809773), np.complex128(0.00041512006647895304+0j)) <f>: (np.float32(0.00019001847), np.complex128(0.0021500927509228167+0j))\n",
      "Epoch 200: <Test loss>: 0.00012986133515369147 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008082474), np.complex128(0.00037356987160213625+0j)) <f>: (np.float32(0.00020527297), np.complex128(0.0022533233261133605+0j))\n",
      "Epoch 300: <Test loss>: 0.0001209946713061072 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007963734), np.complex128(0.0003222437311566668+0j)) <f>: (np.float32(0.00032401254), np.complex128(0.0022256633898799384+0j))\n",
      "Epoch 400: <Test loss>: 0.00011368985724402592 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0077890935), np.complex128(0.00029789948845436746+0j)) <f>: (np.float32(0.0004986559), np.complex128(0.0022522269963256704+0j))\n",
      "Epoch 500: <Test loss>: 0.00011406673729652539 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007813341), np.complex128(0.000299860246750889+0j)) <f>: (np.float32(0.00047440556), np.complex128(0.0021752631736462144+0j))\n",
      "Epoch 600: <Test loss>: 0.00011253687989665195 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007902395), np.complex128(0.0002907994464143187+0j)) <f>: (np.float32(0.00038535224), np.complex128(0.0022008925782224078+0j))\n",
      "Epoch 700: <Test loss>: 0.000108794127299916 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007947528), np.complex128(0.0002838789122022706+0j)) <f>: (np.float32(0.0003402188), np.complex128(0.0022114321615088342+0j))\n",
      "Epoch 800: <Test loss>: 0.0001110232769860886 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0078789415), np.complex128(0.0003012893003236595+0j)) <f>: (np.float32(0.00040881132), np.complex128(0.002181037616978323+0j))\n",
      "Epoch 900: <Test loss>: 0.00010776067210827023 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007993733), np.complex128(0.00031681710802768266+0j)) <f>: (np.float32(0.00029400867), np.complex128(0.0022129950847886706+0j))\n",
      "Epoch 1000: <Test loss>: 0.00011120452836621553 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007991271), np.complex128(0.0003211080428505051+0j)) <f>: (np.float32(0.00029647374), np.complex128(0.002192580921773683+0j))\n",
      "Epoch 1100: <Test loss>: 0.00010631282202666625 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008008302), np.complex128(0.0003099880086333246+0j)) <f>: (np.float32(0.00027944503), np.complex128(0.002218271473185987+0j))\n",
      "Epoch 1200: <Test loss>: 0.00011053735215682536 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007940608), np.complex128(0.00030629585610602374+0j)) <f>: (np.float32(0.00034713867), np.complex128(0.002242044906367375+0j))\n",
      "Epoch 1300: <Test loss>: 0.00010959018982248381 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008004749), np.complex128(0.0003174690132228334+0j)) <f>: (np.float32(0.00028300023), np.complex128(0.00223032678758828+0j))\n",
      "Epoch 1400: <Test loss>: 0.00011239636660320684 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008010373), np.complex128(0.00032051703077015317+0j)) <f>: (np.float32(0.00027737278), np.complex128(0.0022439780090409258+0j))\n",
      "Epoch 1500: <Test loss>: 0.00010953458695439622 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008077586), np.complex128(0.0003235193784813738+0j)) <f>: (np.float32(0.00021015835), np.complex128(0.0022510111638001876+0j))\n",
      "Epoch 1600: <Test loss>: 0.00011212785466341302 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008081089), np.complex128(0.0003216558905927106+0j)) <f>: (np.float32(0.00020665942), np.complex128(0.0022132452540019687+0j))\n",
      "Epoch 1700: <Test loss>: 0.00011685697973007336 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008074259), np.complex128(0.000346503262656035+0j)) <f>: (np.float32(0.00021348728), np.complex128(0.002212098179951946+0j))\n",
      "Epoch 1800: <Test loss>: 0.00011702749907271937 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008043725), np.complex128(0.0003230081934687034+0j)) <f>: (np.float32(0.00024402511), np.complex128(0.0022387074562337773+0j))\n",
      "Epoch 1900: <Test loss>: 0.00011840693332487717 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008105009), np.complex128(0.0003301686846112556+0j)) <f>: (np.float32(0.00018273869), np.complex128(0.0022453645960091567+0j))\n",
      "Epoch 2000: <Test loss>: 0.00012158812023699284 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00809456), np.complex128(0.0003439359201336583+0j)) <f>: (np.float32(0.00019318795), np.complex128(0.0022318298326385643+0j))\n",
      "Epoch 2100: <Test loss>: 0.00012212485307827592 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00806866), np.complex128(0.0003399282968704701+0j)) <f>: (np.float32(0.00021908748), np.complex128(0.0022076813993585397+0j))\n",
      "Epoch 2200: <Test loss>: 0.00012116213474655524 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0081123235), np.complex128(0.00034088977378101207+0j)) <f>: (np.float32(0.00017542571), np.complex128(0.002256185302508904+0j))\n",
      "Epoch 2300: <Test loss>: 0.00012496821000240743 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00810886), np.complex128(0.0003457112398665191+0j)) <f>: (np.float32(0.00017888732), np.complex128(0.0022686549438131415+0j))\n",
      "Epoch 2400: <Test loss>: 0.00012642735964618623 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0080843605), np.complex128(0.00035823819047202756+0j)) <f>: (np.float32(0.00020339449), np.complex128(0.0022303333843423836+0j))\n",
      "Epoch 2500: <Test loss>: 0.0001249054039362818 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008113851), np.complex128(0.00034630694579113676+0j)) <f>: (np.float32(0.00017389894), np.complex128(0.0022847246368085457+0j))\n",
      "Epoch 2600: <Test loss>: 0.0001313145039603114 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008074262), np.complex128(0.0003552992413735584+0j)) <f>: (np.float32(0.00021348355), np.complex128(0.0022843331448246518+0j))\n",
      "Epoch 2700: <Test loss>: 0.00013146335550118238 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008139971), np.complex128(0.0003522650516377242+0j)) <f>: (np.float32(0.00014777623), np.complex128(0.002273712624439805+0j))\n",
      "Epoch 2800: <Test loss>: 0.00013592798495665193 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008095329), np.complex128(0.0003422783270894085+0j)) <f>: (np.float32(0.0001924186), np.complex128(0.0023026558830676123+0j))\n",
      "Epoch 2900: <Test loss>: 0.0001384466013405472 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008174553), np.complex128(0.00035326303270198146+0j)) <f>: (np.float32(0.000113195565), np.complex128(0.002241820870449178+0j))\n",
      "Epoch 3000: <Test loss>: 0.00013650828623212874 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008136671), np.complex128(0.00035057371365902344+0j)) <f>: (np.float32(0.00015107238), np.complex128(0.002263929130662131+0j))\n",
      "Epoch 3100: <Test loss>: 0.00013770077202934772 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008151253), np.complex128(0.0003629469725799929+0j)) <f>: (np.float32(0.00013649267), np.complex128(0.0022927412153718085+0j))\n",
      "Epoch 3200: <Test loss>: 0.00014125405868981034 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008178853), np.complex128(0.000353759882460543+0j)) <f>: (np.float32(0.00010889382), np.complex128(0.002253653163818521+0j))\n",
      "Epoch 3300: <Test loss>: 0.0001403392234351486 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008187957), np.complex128(0.000366001491735924+0j)) <f>: (np.float32(9.979186e-05), np.complex128(0.002260352421331636+0j))\n",
      "Epoch 3400: <Test loss>: 0.00014376980834640563 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008168618), np.complex128(0.0003474308360563062+0j)) <f>: (np.float32(0.00011913135), np.complex128(0.0022662811272212346+0j))\n",
      "Epoch 3500: <Test loss>: 0.00014402125088963658 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0081586065), np.complex128(0.00035666150281103394+0j)) <f>: (np.float32(0.00012913746), np.complex128(0.0022453788044026098+0j))\n",
      "Epoch 3600: <Test loss>: 0.0001433916186215356 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008196076), np.complex128(0.00033838123117261296+0j)) <f>: (np.float32(9.166535e-05), np.complex128(0.002284950195054613+0j))\n",
      "Epoch 3700: <Test loss>: 0.00015021333820186555 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008217444), np.complex128(0.00035534637010719954+0j)) <f>: (np.float32(7.030035e-05), np.complex128(0.0022608065824795106+0j))\n",
      "Epoch 3800: <Test loss>: 0.0001528326829429716 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008200709), np.complex128(0.00034657877646141833+0j)) <f>: (np.float32(8.7043096e-05), np.complex128(0.002257928621641332+0j))\n",
      "Epoch 3900: <Test loss>: 0.0001555338967591524 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008150328), np.complex128(0.00034510268930049853+0j)) <f>: (np.float32(0.0001374204), np.complex128(0.0022864136595802777+0j))\n",
      "Epoch 4000: <Test loss>: 0.0001559068914502859 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008177492), np.complex128(0.00032649121620486715+0j)) <f>: (np.float32(0.000110256595), np.complex128(0.0022462188756655218+0j))\n",
      "Epoch 4100: <Test loss>: 0.00015612420975230634 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008203122), np.complex128(0.0003326770686438349+0j)) <f>: (np.float32(8.462934e-05), np.complex128(0.002283649112168412+0j))\n",
      "Epoch 4200: <Test loss>: 0.00015781764523126185 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008203015), np.complex128(0.0003246664525313963+0j)) <f>: (np.float32(8.472765e-05), np.complex128(0.002247051842731707+0j))\n",
      "Epoch 4300: <Test loss>: 0.00015754035848658532 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008222156), np.complex128(0.00032022528297690617+0j)) <f>: (np.float32(6.5588974e-05), np.complex128(0.002259446382527692+0j))\n",
      "Epoch 4400: <Test loss>: 0.00015619069745298475 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008235914), np.complex128(0.00031668828103168645+0j)) <f>: (np.float32(5.184008e-05), np.complex128(0.0022966190918992458+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3768357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e80003c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.0002153656241716817 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008476562), np.complex128(0.00038123130375940044+0j)) <f>: (np.float32(-0.00018880647), np.complex128(0.002335472704960558+0j))\n",
      "Epoch 400: <Test loss>: 0.00018196244491264224 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008325658), np.complex128(0.0003802493705681058+0j)) <f>: (np.float32(-3.7908816e-05), np.complex128(0.0022532834918674296+0j))\n",
      "Epoch 600: <Test loss>: 0.0001631522609386593 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008137307), np.complex128(0.000318120284114705+0j)) <f>: (np.float32(0.00015043955), np.complex128(0.002294515234782947+0j))\n",
      "Epoch 800: <Test loss>: 0.00015267194248735905 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0082252445), np.complex128(0.0003390513408718754+0j)) <f>: (np.float32(6.25014e-05), np.complex128(0.00229735260021126+0j))\n",
      "Epoch 1000: <Test loss>: 0.0001555604103486985 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00814384), np.complex128(0.00036771195395849125+0j)) <f>: (np.float32(0.00014390657), np.complex128(0.002217237305119654+0j))\n",
      "Epoch 1200: <Test loss>: 0.00013492941798176616 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0082119), np.complex128(0.0003469856502998318+0j)) <f>: (np.float32(7.585372e-05), np.complex128(0.002250731055472113+0j))\n",
      "Epoch 1400: <Test loss>: 0.00013427984958980232 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008238836), np.complex128(0.00033702972117571905+0j)) <f>: (np.float32(4.8914982e-05), np.complex128(0.002236297357494303+0j))\n",
      "Epoch 1600: <Test loss>: 0.00013425791985355318 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008152333), np.complex128(0.00033521285457807366+0j)) <f>: (np.float32(0.00013542101), np.complex128(0.0021991198274178276+0j))\n",
      "Epoch 1800: <Test loss>: 0.0001334369881078601 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008161475), np.complex128(0.0003312426869235195+0j)) <f>: (np.float32(0.00012627168), np.complex128(0.0021998109642707942+0j))\n",
      "Epoch 2000: <Test loss>: 0.00012775893264915794 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0081201205), np.complex128(0.00032169521739601817+0j)) <f>: (np.float32(0.00016762948), np.complex128(0.002222847590763117+0j))\n",
      "Epoch 2200: <Test loss>: 0.00013266131281852722 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008042623), np.complex128(0.00031855430613346626+0j)) <f>: (np.float32(0.00024511493), np.complex128(0.0022205440549745406+0j))\n",
      "Epoch 2400: <Test loss>: 0.00013315759133547544 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008052604), np.complex128(0.0003376519092622414+0j)) <f>: (np.float32(0.00023513503), np.complex128(0.0022156979779218025+0j))\n",
      "Epoch 2600: <Test loss>: 0.0001365373027510941 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008032306), np.complex128(0.0003246357839678492+0j)) <f>: (np.float32(0.0002554417), np.complex128(0.0022374416406098973+0j))\n",
      "Epoch 2800: <Test loss>: 0.0001368289376841858 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007992877), np.complex128(0.0002829582527077422+0j)) <f>: (np.float32(0.00029486654), np.complex128(0.0022152945610362604+0j))\n",
      "Epoch 3000: <Test loss>: 0.0001438279141439125 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007942203), np.complex128(0.00032867436123106014+0j)) <f>: (np.float32(0.00034554675), np.complex128(0.002215378035347797+0j))\n",
      "Epoch 3200: <Test loss>: 0.00014334183651953936 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008042864), np.complex128(0.00032886798230702196+0j)) <f>: (np.float32(0.0002448826), np.complex128(0.002195271636283855+0j))\n",
      "Epoch 3400: <Test loss>: 0.00015573941345792264 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008003572), np.complex128(0.00031527793940565096+0j)) <f>: (np.float32(0.0002841734), np.complex128(0.0022305246902113766+0j))\n",
      "Epoch 3600: <Test loss>: 0.0001562295074108988 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007973396), np.complex128(0.0003352998492728097+0j)) <f>: (np.float32(0.00031435117), np.complex128(0.0022175917537920454+0j))\n",
      "Epoch 3800: <Test loss>: 0.0001571608881931752 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007959966), np.complex128(0.00030655598388080487+0j)) <f>: (np.float32(0.00032778265), np.complex128(0.002216883110168574+0j))\n",
      "Epoch 4000: <Test loss>: 0.00016312001389451325 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007924955), np.complex128(0.0003269003735350856+0j)) <f>: (np.float32(0.00036278262), np.complex128(0.0022415613135473482+0j))\n",
      "Epoch 4200: <Test loss>: 0.00016290882194880396 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007914534), np.complex128(0.00031159393767548726+0j)) <f>: (np.float32(0.0003732174), np.complex128(0.0022217441567787+0j))\n",
      "Epoch 4400: <Test loss>: 0.00016260657866951078 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007997646), np.complex128(0.00032029255083966047+0j)) <f>: (np.float32(0.00029009843), np.complex128(0.0022175217267100264+0j))\n",
      "Epoch 4600: <Test loss>: 0.00017601960280444473 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0079241665), np.complex128(0.00033342992320586304+0j)) <f>: (np.float32(0.0003635886), np.complex128(0.0022391281261685127+0j))\n",
      "Epoch 4800: <Test loss>: 0.00017747496895026416 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007957714), np.complex128(0.0003106046148509904+0j)) <f>: (np.float32(0.00033003115), np.complex128(0.0022681312630258715+0j))\n",
      "Epoch 5000: <Test loss>: 0.0001786242937669158 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00794993), np.complex128(0.0003201661024809611+0j)) <f>: (np.float32(0.0003378197), np.complex128(0.0022448881073858563+0j))\n",
      "Epoch 5200: <Test loss>: 0.00017075496725738049 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007981945), np.complex128(0.0003193567314967603+0j)) <f>: (np.float32(0.0003058001), np.complex128(0.0022549897676883545+0j))\n",
      "Epoch 5400: <Test loss>: 0.00019245664589107037 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007977821), np.complex128(0.0003197642396384529+0j)) <f>: (np.float32(0.0003099236), np.complex128(0.0022599873163641547+0j))\n",
      "Epoch 5600: <Test loss>: 0.00018828062457032502 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00801619), np.complex128(0.00032128599663547183+0j)) <f>: (np.float32(0.00027155978), np.complex128(0.0022462762166819573+0j))\n",
      "Epoch 5800: <Test loss>: 0.00019508706463966519 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008033568), np.complex128(0.00032928402182781923+0j)) <f>: (np.float32(0.00025417554), np.complex128(0.0022341957838698094+0j))\n",
      "Epoch 6000: <Test loss>: 0.00020205657347105443 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008048541), np.complex128(0.0003320548171269846+0j)) <f>: (np.float32(0.00023920703), np.complex128(0.0022421991689248657+0j))\n",
      "Epoch 6200: <Test loss>: 0.00020006396516691893 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008030344), np.complex128(0.00032632794654081286+0j)) <f>: (np.float32(0.00025740216), np.complex128(0.002219700177891954+0j))\n",
      "Epoch 6400: <Test loss>: 0.00020404491806402802 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008006534), np.complex128(0.00032537458871224407+0j)) <f>: (np.float32(0.0002812113), np.complex128(0.0022489910347167374+0j))\n",
      "Epoch 6600: <Test loss>: 0.000208581070182845 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008021527), np.complex128(0.00033661726546844903+0j)) <f>: (np.float32(0.000266221), np.complex128(0.002255530955246129+0j))\n",
      "Epoch 6800: <Test loss>: 0.00020873808534815907 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0080269165), np.complex128(0.0003357362499288678+0j)) <f>: (np.float32(0.00026082952), np.complex128(0.0022262035625524664+0j))\n",
      "Epoch 7000: <Test loss>: 0.00021990899404045194 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0080565475), np.complex128(0.00033632957721618865+0j)) <f>: (np.float32(0.00023120694), np.complex128(0.002225048623141782+0j))\n",
      "Epoch 7200: <Test loss>: 0.0002136104740202427 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00799585), np.complex128(0.00033475672709003387+0j)) <f>: (np.float32(0.00029189233), np.complex128(0.002228881083554432+0j))\n",
      "Epoch 7400: <Test loss>: 0.00022373466345015913 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007986778), np.complex128(0.00033901689820381735+0j)) <f>: (np.float32(0.00030096585), np.complex128(0.002235623727411841+0j))\n",
      "Epoch 7600: <Test loss>: 0.00022634067863691598 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007994742), np.complex128(0.0003436854971990482+0j)) <f>: (np.float32(0.0002930101), np.complex128(0.002245279853091062+0j))\n",
      "Epoch 7800: <Test loss>: 0.0002222938055638224 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0080055), np.complex128(0.0003466983426295389+0j)) <f>: (np.float32(0.00028224368), np.complex128(0.0022332740143445424+0j))\n",
      "Epoch 8000: <Test loss>: 0.00022556139447260648 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007971362), np.complex128(0.0003503828835174899+0j)) <f>: (np.float32(0.00031638434), np.complex128(0.0022238756695179703+0j))\n",
      "Epoch 8200: <Test loss>: 0.00023072789190337062 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007975905), np.complex128(0.0003623847578685145+0j)) <f>: (np.float32(0.00031183715), np.complex128(0.0022208411626304966+0j))\n",
      "Epoch 8400: <Test loss>: 0.0002277753665111959 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007948274), np.complex128(0.0003664050989124498+0j)) <f>: (np.float32(0.00033947546), np.complex128(0.0022247492319940215+0j))\n",
      "Epoch 8600: <Test loss>: 0.00022668643214274198 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007966286), np.complex128(0.00037441920369292377+0j)) <f>: (np.float32(0.00032145978), np.complex128(0.0022049830732090176+0j))\n",
      "Epoch 8800: <Test loss>: 0.00023982554557733238 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007934689), np.complex128(0.00038411621021848573+0j)) <f>: (np.float32(0.00035305863), np.complex128(0.002205743475980068+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94452f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1181df18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.00025622069370001554 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008522707), np.complex128(0.0005048361442886732+0j)) <f>: (np.float32(-0.00023496107), np.complex128(0.0022508617219476187+0j))\n",
      "Epoch 800: <Test loss>: 0.00024068444326985627 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008348743), np.complex128(0.0004440245374612397+0j)) <f>: (np.float32(-6.099568e-05), np.complex128(0.0021992266440900373+0j))\n",
      "Epoch 1200: <Test loss>: 0.00022061598428990692 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008283083), np.complex128(0.00045774889408197155+0j)) <f>: (np.float32(4.6601594e-06), np.complex128(0.0021233439279195717+0j))\n",
      "Epoch 1600: <Test loss>: 0.00021032456425018609 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008299464), np.complex128(0.0004924650419988529+0j)) <f>: (np.float32(-1.1711644e-05), np.complex128(0.0021593721004542197+0j))\n",
      "Epoch 2000: <Test loss>: 0.0001999982341658324 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008263166), np.complex128(0.0004554987666294999+0j)) <f>: (np.float32(2.4583607e-05), np.complex128(0.0021537925150894675+0j))\n",
      "Epoch 2400: <Test loss>: 0.00020082508854102343 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008264479), np.complex128(0.0004420669823962779+0j)) <f>: (np.float32(2.3272403e-05), np.complex128(0.002151436458989377+0j))\n",
      "Epoch 2800: <Test loss>: 0.00020102845155633986 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008239379), np.complex128(0.0004490629035530535+0j)) <f>: (np.float32(4.8368067e-05), np.complex128(0.002125380041445657+0j))\n",
      "Epoch 3200: <Test loss>: 0.00021057027333881706 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008305327), np.complex128(0.0004787321222838524+0j)) <f>: (np.float32(-1.7582359e-05), np.complex128(0.0020870474450978417+0j))\n",
      "Epoch 3600: <Test loss>: 0.00021495077817235142 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008201998), np.complex128(0.00046711149591876345+0j)) <f>: (np.float32(8.57474e-05), np.complex128(0.0020750772541956106+0j))\n",
      "Epoch 4000: <Test loss>: 0.0002090782072627917 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008322113), np.complex128(0.0004767654015365069+0j)) <f>: (np.float32(-3.436687e-05), np.complex128(0.002077194304820116+0j))\n",
      "Epoch 4400: <Test loss>: 0.00021232808649074286 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008289099), np.complex128(0.000442804137952147+0j)) <f>: (np.float32(-1.3475344e-06), np.complex128(0.002093306369274568+0j))\n",
      "Epoch 4800: <Test loss>: 0.00022330092906486243 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008264082), np.complex128(0.0004636812155002614+0j)) <f>: (np.float32(2.3661834e-05), np.complex128(0.002081246361028009+0j))\n",
      "Epoch 5200: <Test loss>: 0.0002297701284987852 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0082654), np.complex128(0.00048515491871274825+0j)) <f>: (np.float32(2.2349672e-05), np.complex128(0.0020622516418911125+0j))\n",
      "Epoch 5600: <Test loss>: 0.00022739579435437918 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008241352), np.complex128(0.0004640881527690028+0j)) <f>: (np.float32(4.639974e-05), np.complex128(0.0020704290797659685+0j))\n",
      "Epoch 6000: <Test loss>: 0.00024238046898972243 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008124205), np.complex128(0.00048478451731288614+0j)) <f>: (np.float32(0.00016353794), np.complex128(0.0020599904775615834+0j))\n",
      "Epoch 6400: <Test loss>: 0.00023602820874657482 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008202305), np.complex128(0.00046468389040878443+0j)) <f>: (np.float32(8.544091e-05), np.complex128(0.0020796035155349994+0j))\n",
      "Epoch 6800: <Test loss>: 0.0002497028617653996 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008166629), np.complex128(0.0004863423978816529+0j)) <f>: (np.float32(0.0001211234), np.complex128(0.002051666135047275+0j))\n",
      "Epoch 7200: <Test loss>: 0.00025523873046040535 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008102043), np.complex128(0.0004821992874380373+0j)) <f>: (np.float32(0.00018570441), np.complex128(0.0020593658156922725+0j))\n",
      "Epoch 7600: <Test loss>: 0.00025567426928319037 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008133064), np.complex128(0.0004977408278080542+0j)) <f>: (np.float32(0.00015467897), np.complex128(0.002049173450020855+0j))\n",
      "Epoch 8000: <Test loss>: 0.0002562449953984469 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008156716), np.complex128(0.000483434719934846+0j)) <f>: (np.float32(0.00013103573), np.complex128(0.002065103976876813+0j))\n",
      "Epoch 8400: <Test loss>: 0.00026764514041133225 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008126635), np.complex128(0.0004960376917883611+0j)) <f>: (np.float32(0.00016111624), np.complex128(0.0020546557332625846+0j))\n",
      "Epoch 8800: <Test loss>: 0.0002674201678019017 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008164351), np.complex128(0.0005062225409659206+0j)) <f>: (np.float32(0.00012340046), np.complex128(0.002050706941628538+0j))\n",
      "Epoch 9200: <Test loss>: 0.00026624981546774507 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00813162), np.complex128(0.0005065562796362476+0j)) <f>: (np.float32(0.0001561296), np.complex128(0.0020764339020490656+0j))\n",
      "Epoch 9600: <Test loss>: 0.00027752615278586745 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008121092), np.complex128(0.0005245996072847406+0j)) <f>: (np.float32(0.00016665072), np.complex128(0.002048950175266593+0j))\n",
      "Epoch 10000: <Test loss>: 0.0002801600785460323 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008149441), np.complex128(0.0005155988754687061+0j)) <f>: (np.float32(0.000138304), np.complex128(0.002065117424106331+0j))\n",
      "Epoch 10400: <Test loss>: 0.0002859770320355892 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008106685), np.complex128(0.0005250611263506531+0j)) <f>: (np.float32(0.00018106247), np.complex128(0.0020606207212997513+0j))\n",
      "Epoch 10800: <Test loss>: 0.0002921129926107824 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008047843), np.complex128(0.000527998870272892+0j)) <f>: (np.float32(0.00023990468), np.complex128(0.0020407915127687993+0j))\n",
      "Epoch 11200: <Test loss>: 0.0002839118242263794 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00815298), np.complex128(0.0005126483503353924+0j)) <f>: (np.float32(0.00013476792), np.complex128(0.0020632944364820417+0j))\n",
      "Epoch 11600: <Test loss>: 0.00029578484827652574 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008103442), np.complex128(0.0005298320701799755+0j)) <f>: (np.float32(0.00018430692), np.complex128(0.0020620721340631116+0j))\n",
      "Epoch 12000: <Test loss>: 0.00030148611404001713 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008108702), np.complex128(0.0005354563104955801+0j)) <f>: (np.float32(0.00017904775), np.complex128(0.002057456562822018+0j))\n",
      "Epoch 12400: <Test loss>: 0.0003092511324211955 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008109674), np.complex128(0.0005592069087589189+0j)) <f>: (np.float32(0.00017807067), np.complex128(0.00203921285876764+0j))\n",
      "Epoch 12800: <Test loss>: 0.00031569518614560366 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008152468), np.complex128(0.0005636769073974467+0j)) <f>: (np.float32(0.00013528037), np.complex128(0.00201928381090055+0j))\n",
      "Epoch 13200: <Test loss>: 0.00031506267259828746 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008143154), np.complex128(0.0005528913412993642+0j)) <f>: (np.float32(0.00014459583), np.complex128(0.0020366609298149473+0j))\n",
      "Epoch 13600: <Test loss>: 0.00030892141512595117 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008254891), np.complex128(0.0005603956565343819+0j)) <f>: (np.float32(3.285812e-05), np.complex128(0.0020458386639610297+0j))\n",
      "Epoch 14000: <Test loss>: 0.00032657815609127283 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008132546), np.complex128(0.0005709374598822855+0j)) <f>: (np.float32(0.00015519877), np.complex128(0.0020365550011673286+0j))\n",
      "Epoch 14400: <Test loss>: 0.00033441424602642655 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008218753), np.complex128(0.0005656405834890523+0j)) <f>: (np.float32(6.899634e-05), np.complex128(0.0020445059927715267+0j))\n",
      "Epoch 14800: <Test loss>: 0.00033053639344871044 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00819462), np.complex128(0.0005796864050116596+0j)) <f>: (np.float32(9.3126386e-05), np.complex128(0.0020302821223184605+0j))\n",
      "Epoch 15200: <Test loss>: 0.0003258674987591803 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008207412), np.complex128(0.0005615953777565742+0j)) <f>: (np.float32(8.034148e-05), np.complex128(0.0020367013983641573+0j))\n",
      "Epoch 15600: <Test loss>: 0.0003809030749835074 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008009103), np.complex128(0.0006364068825492042+0j)) <f>: (np.float32(0.00027864924), np.complex128(0.0019622820198587003+0j))\n",
      "Epoch 16000: <Test loss>: 0.00033491358044557273 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008192079), np.complex128(0.0005709696190585385+0j)) <f>: (np.float32(9.566746e-05), np.complex128(0.002031931691426229+0j))\n",
      "Epoch 16400: <Test loss>: 0.0003378107794560492 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008132638), np.complex128(0.0005734820309169405+0j)) <f>: (np.float32(0.00015511004), np.complex128(0.0020435300537462197+0j))\n",
      "Epoch 16800: <Test loss>: 0.0003404260496608913 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008192622), np.complex128(0.0005808462919879209+0j)) <f>: (np.float32(9.512213e-05), np.complex128(0.002020492539229306+0j))\n",
      "Epoch 17200: <Test loss>: 0.0003446587943471968 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00820072), np.complex128(0.0005842126663510482+0j)) <f>: (np.float32(8.702594e-05), np.complex128(0.0020178156525306194+0j))\n",
      "Epoch 17600: <Test loss>: 0.0003465834306553006 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008165955), np.complex128(0.0005902791429128791+0j)) <f>: (np.float32(0.000121794255), np.complex128(0.002001023107518291+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "480a8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e26f251",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0004047975526191294 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0085852435), np.complex128(0.000637978242062653+0j)) <f>: (np.float32(-0.00029749135), np.complex128(0.002279666956181882+0j))\n",
      "Epoch 1600: <Test loss>: 0.00039470120100304484 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008609237), np.complex128(0.0006171290738588213+0j)) <f>: (np.float32(-0.00032148458), np.complex128(0.0020967204432442875+0j))\n",
      "Epoch 2400: <Test loss>: 0.00036334682954475284 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008563202), np.complex128(0.0006715037710093979+0j)) <f>: (np.float32(-0.00027546159), np.complex128(0.00209167138914222+0j))\n",
      "Epoch 3200: <Test loss>: 0.0003585890808608383 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008683318), np.complex128(0.000667896361400192+0j)) <f>: (np.float32(-0.00039557237), np.complex128(0.0020086644322616057+0j))\n",
      "Epoch 4000: <Test loss>: 0.00036097154952585697 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008850009), np.complex128(0.0006558010224609839+0j)) <f>: (np.float32(-0.0005622578), np.complex128(0.0020337529030013367+0j))\n",
      "Epoch 4800: <Test loss>: 0.0003704223781824112 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00885922), np.complex128(0.0006452563013179964+0j)) <f>: (np.float32(-0.00057147274), np.complex128(0.002005212426955793+0j))\n",
      "Epoch 5600: <Test loss>: 0.00036429386818781495 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008731044), np.complex128(0.0006215298700095932+0j)) <f>: (np.float32(-0.00044330294), np.complex128(0.0019958881687522257+0j))\n",
      "Epoch 6400: <Test loss>: 0.0003632994194049388 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0086222645), np.complex128(0.000619952167463353+0j)) <f>: (np.float32(-0.00033451605), np.complex128(0.0020009430584444613+0j))\n",
      "Epoch 7200: <Test loss>: 0.00035321706673130393 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008640577), np.complex128(0.000621704810853984+0j)) <f>: (np.float32(-0.0003528218), np.complex128(0.002016611618046129+0j))\n",
      "Epoch 8000: <Test loss>: 0.00036228675162419677 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008604243), np.complex128(0.0006354686211386793+0j)) <f>: (np.float32(-0.00031649938), np.complex128(0.001990076555247959+0j))\n",
      "Epoch 8800: <Test loss>: 0.00038099390803836286 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0086336145), np.complex128(0.0006582821631677236+0j)) <f>: (np.float32(-0.00034586276), np.complex128(0.0019579096404948346+0j))\n",
      "Epoch 9600: <Test loss>: 0.00037459307350218296 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00853596), np.complex128(0.0006582456272988443+0j)) <f>: (np.float32(-0.00024821004), np.complex128(0.001971570503357323+0j))\n",
      "Epoch 10400: <Test loss>: 0.00037533111753873527 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008606882), np.complex128(0.0006499314970673304+0j)) <f>: (np.float32(-0.0003191268), np.complex128(0.001964014555835382+0j))\n",
      "Epoch 11200: <Test loss>: 0.00038469579885713756 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008474898), np.complex128(0.0006912438599296283+0j)) <f>: (np.float32(-0.00018714681), np.complex128(0.0020057378837922443+0j))\n",
      "Epoch 12000: <Test loss>: 0.0004198120441287756 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008538074), np.complex128(0.000690229418695277+0j)) <f>: (np.float32(-0.00025032109), np.complex128(0.0018827740067265566+0j))\n",
      "Epoch 12800: <Test loss>: 0.0003902014868799597 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008600733), np.complex128(0.0006885556826325727+0j)) <f>: (np.float32(-0.00031298716), np.complex128(0.0019465978563963657+0j))\n",
      "Epoch 13600: <Test loss>: 0.00039156508864834905 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008520956), np.complex128(0.0006472930491473605+0j)) <f>: (np.float32(-0.00023320652), np.complex128(0.0019773336560910625+0j))\n",
      "Epoch 14400: <Test loss>: 0.00041282971506007016 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008502818), np.complex128(0.000705323109235047+0j)) <f>: (np.float32(-0.0002150765), np.complex128(0.0018929066210290777+0j))\n",
      "Epoch 15200: <Test loss>: 0.00040399713907390833 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008530941), np.complex128(0.000685773882171514+0j)) <f>: (np.float32(-0.00024319594), np.complex128(0.0019280707456368618+0j))\n",
      "Epoch 16000: <Test loss>: 0.00042417956865392625 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0084575), np.complex128(0.0007002460189280408+0j)) <f>: (np.float32(-0.00016975355), np.complex128(0.001894572174579481+0j))\n",
      "Epoch 16800: <Test loss>: 0.00042210184619762003 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008494773), np.complex128(0.0007211567145499504+0j)) <f>: (np.float32(-0.00020702236), np.complex128(0.0018752946830407377+0j))\n",
      "Epoch 17600: <Test loss>: 0.0004457630275283009 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008512359), np.complex128(0.0007626499807074631+0j)) <f>: (np.float32(-0.00022460721), np.complex128(0.0018470091969134137+0j))\n",
      "Epoch 18400: <Test loss>: 0.00042333913734182715 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008446653), np.complex128(0.0007053334483784972+0j)) <f>: (np.float32(-0.00015889917), np.complex128(0.0018899023069776917+0j))\n",
      "Epoch 19200: <Test loss>: 0.00044424005318433046 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008440835), np.complex128(0.0007654764995497022+0j)) <f>: (np.float32(-0.00015308284), np.complex128(0.0018492312881609467+0j))\n",
      "Epoch 20000: <Test loss>: 0.0004381393955554813 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008434168), np.complex128(0.0007652912829921891+0j)) <f>: (np.float32(-0.00014642047), np.complex128(0.0018505201924241882+0j))\n",
      "Epoch 20800: <Test loss>: 0.0004587862058542669 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008424852), np.complex128(0.0007907576716256402+0j)) <f>: (np.float32(-0.00013710778), np.complex128(0.0018202658289183694+0j))\n",
      "Epoch 21600: <Test loss>: 0.0004546106210909784 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0083763385), np.complex128(0.0007932473754266484+0j)) <f>: (np.float32(-8.8591914e-05), np.complex128(0.0018125105833061183+0j))\n",
      "Epoch 22400: <Test loss>: 0.0004651453346014023 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008449468), np.complex128(0.0007651490087666749+0j)) <f>: (np.float32(-0.00016172146), np.complex128(0.0018217223161079634+0j))\n",
      "Epoch 23200: <Test loss>: 0.0004751136002596468 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008388804), np.complex128(0.0008121565270511868+0j)) <f>: (np.float32(-0.00010105408), np.complex128(0.0017840011881212529+0j))\n",
      "Epoch 24000: <Test loss>: 0.00046599569031968713 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00838813), np.complex128(0.0008024213671833837+0j)) <f>: (np.float32(-0.00010038043), np.complex128(0.0018022990616756703+0j))\n",
      "Epoch 24800: <Test loss>: 0.0005106696044094861 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008412188), np.complex128(0.0008688940039166128+0j)) <f>: (np.float32(-0.00012443689), np.complex128(0.0018560614658708792+0j))\n",
      "Epoch 25600: <Test loss>: 0.0004698262782767415 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008540523), np.complex128(0.0007483805038486261+0j)) <f>: (np.float32(-0.00025277724), np.complex128(0.0018407025731300949+0j))\n",
      "Epoch 26400: <Test loss>: 0.00046260637464001775 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008397179), np.complex128(0.0007428718970208112+0j)) <f>: (np.float32(-0.00010942311), np.complex128(0.001860360519775675+0j))\n",
      "Epoch 27200: <Test loss>: 0.0004915212048217654 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008388478), np.complex128(0.0008423939546594522+0j)) <f>: (np.float32(-0.00010073296), np.complex128(0.001755799683748089+0j))\n",
      "Epoch 28000: <Test loss>: 0.0004995971685275435 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008367105), np.complex128(0.0008513273551824042+0j)) <f>: (np.float32(-7.9357116e-05), np.complex128(0.0017438603199852248+0j))\n",
      "Epoch 28800: <Test loss>: 0.0004704250895883888 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008365814), np.complex128(0.0008473261066671715+0j)) <f>: (np.float32(-7.806289e-05), np.complex128(0.0018270999393086328+0j))\n",
      "Epoch 29600: <Test loss>: 0.0004803427145816386 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008357588), np.complex128(0.0008085930746592263+0j)) <f>: (np.float32(-6.9843845e-05), np.complex128(0.0017932497105132888+0j))\n",
      "Epoch 30400: <Test loss>: 0.0005071480409242213 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008359848), np.complex128(0.0008578323890314425+0j)) <f>: (np.float32(-7.209965e-05), np.complex128(0.00173811315169412+0j))\n",
      "Epoch 31200: <Test loss>: 0.0005118249100632966 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008373528), np.complex128(0.0008719055490253812+0j)) <f>: (np.float32(-8.5781125e-05), np.complex128(0.0017395145813590834+0j))\n",
      "Epoch 32000: <Test loss>: 0.0005328901461325586 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008329024), np.complex128(0.0009053803971401217+0j)) <f>: (np.float32(-4.127229e-05), np.complex128(0.0017005843444616493+0j))\n",
      "Epoch 32800: <Test loss>: 0.0005301666678860784 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008366051), np.complex128(0.0009062008050013798+0j)) <f>: (np.float32(-7.830358e-05), np.complex128(0.001708168074467217+0j))\n",
      "Epoch 33600: <Test loss>: 0.0005533834919333458 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008342428), np.complex128(0.0009370551559789505+0j)) <f>: (np.float32(-5.467397e-05), np.complex128(0.0016665957106907939+0j))\n",
      "Epoch 34400: <Test loss>: 0.0005716719315387309 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008383416), np.complex128(0.0009649098867494008+0j)) <f>: (np.float32(-9.566566e-05), np.complex128(0.001669549661061815+0j))\n",
      "Epoch 35200: <Test loss>: 0.0005220927996560931 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00833081), np.complex128(0.0008998916440049444+0j)) <f>: (np.float32(-4.3061005e-05), np.complex128(0.001716311133104343+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b5a2ef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ce01ec68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0007346804486587644 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008720938), np.complex128(0.0009266796669508411+0j)) <f>: (np.float32(-0.00043318933), np.complex128(0.001748786699833104+0j))\n",
      "Epoch 3200: <Test loss>: 0.0007453602738678455 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00875156), np.complex128(0.0009365732123474491+0j)) <f>: (np.float32(-0.00046380138), np.complex128(0.0017513331737775966+0j))\n",
      "Epoch 4800: <Test loss>: 0.0007189736934378743 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008711458), np.complex128(0.0009726464838452448+0j)) <f>: (np.float32(-0.0004237135), np.complex128(0.001726768510685791+0j))\n",
      "Epoch 6400: <Test loss>: 0.0007256974349729717 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008548022), np.complex128(0.001020283801855629+0j)) <f>: (np.float32(-0.0002602756), np.complex128(0.0017173152351952442+0j))\n",
      "Epoch 8000: <Test loss>: 0.0006975071737542748 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0086461855), np.complex128(0.0010686655381944286+0j)) <f>: (np.float32(-0.0003584434), np.complex128(0.001645812002585371+0j))\n",
      "Epoch 9600: <Test loss>: 0.0006622551009058952 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008508611), np.complex128(0.0010493001956515223+0j)) <f>: (np.float32(-0.00022086628), np.complex128(0.0016417514467135371+0j))\n",
      "Epoch 11200: <Test loss>: 0.0007022813661023974 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008533198), np.complex128(0.001094670069729914+0j)) <f>: (np.float32(-0.00024545606), np.complex128(0.0015941781933343476+0j))\n",
      "Epoch 12800: <Test loss>: 0.0006900320877321064 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008740504), np.complex128(0.001125309582186835+0j)) <f>: (np.float32(-0.0004527567), np.complex128(0.0015427292200588698+0j))\n",
      "Epoch 14400: <Test loss>: 0.0006940514431335032 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008617856), np.complex128(0.0012011811346195878+0j)) <f>: (np.float32(-0.00033010374), np.complex128(0.0015011977054136243+0j))\n",
      "Epoch 16000: <Test loss>: 0.0006552389240823686 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008604473), np.complex128(0.0011555848777008658+0j)) <f>: (np.float32(-0.00031672497), np.complex128(0.0015090414997636452+0j))\n",
      "Epoch 17600: <Test loss>: 0.0007220974657684565 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0085687265), np.complex128(0.0011774219098316524+0j)) <f>: (np.float32(-0.00028097007), np.complex128(0.0014859873665008178+0j))\n",
      "Epoch 19200: <Test loss>: 0.0006318461382761598 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008559213), np.complex128(0.001111499150611122+0j)) <f>: (np.float32(-0.00027146764), np.complex128(0.0015396961086386104+0j))\n",
      "Epoch 20800: <Test loss>: 0.000512978935148567 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008628448), np.complex128(0.0008781630777348934+0j)) <f>: (np.float32(-0.00034070088), np.complex128(0.001825338098520454+0j))\n",
      "Epoch 22400: <Test loss>: 0.0006710976595059037 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008490791), np.complex128(0.0011874177683471878+0j)) <f>: (np.float32(-0.00020304859), np.complex128(0.0014901046290858079+0j))\n",
      "Epoch 24000: <Test loss>: 0.0006562907947227359 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008514143), np.complex128(0.0011152398907696062+0j)) <f>: (np.float32(-0.00022639653), np.complex128(0.001535798822430831+0j))\n",
      "Epoch 25600: <Test loss>: 0.000614957301877439 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008517692), np.complex128(0.0010807767981459433+0j)) <f>: (np.float32(-0.00022995188), np.complex128(0.0015584977458573321+0j))\n",
      "Epoch 27200: <Test loss>: 0.000648754124995321 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008596941), np.complex128(0.0011256774780887446+0j)) <f>: (np.float32(-0.00030919287), np.complex128(0.0015161969481951281+0j))\n",
      "Epoch 28800: <Test loss>: 0.0005804345128126442 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008610458), np.complex128(0.001036422316756822+0j)) <f>: (np.float32(-0.00032271343), np.complex128(0.0015966023736066193+0j))\n",
      "Epoch 30400: <Test loss>: 0.000639157893601805 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008498396), np.complex128(0.0011167265707952877+0j)) <f>: (np.float32(-0.00021064533), np.complex128(0.0015120741037412816+0j))\n",
      "Epoch 32000: <Test loss>: 0.0006783301942050457 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008674703), np.complex128(0.0011650134153635258+0j)) <f>: (np.float32(-0.0003869559), np.complex128(0.0014871069117885249+0j))\n",
      "Epoch 33600: <Test loss>: 0.000672198657412082 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008685423), np.complex128(0.001131903291634299+0j)) <f>: (np.float32(-0.00039768312), np.complex128(0.0015062598261632424+0j))\n",
      "Epoch 35200: <Test loss>: 0.0006006068433634937 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008665404), np.complex128(0.001078773922111686+0j)) <f>: (np.float32(-0.0003776513), np.complex128(0.001559451864849836+0j))\n",
      "Epoch 36800: <Test loss>: 0.0008073787903413177 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008654387), np.complex128(0.0012768011223718277+0j)) <f>: (np.float32(-0.00036663772), np.complex128(0.0014265188968669933+0j))\n",
      "Epoch 38400: <Test loss>: 0.0006232302985154092 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008699771), np.complex128(0.0011168563492462028+0j)) <f>: (np.float32(-0.00041203012), np.complex128(0.001537375192940185+0j))\n",
      "Epoch 40000: <Test loss>: 0.0006079482263885438 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008839838), np.complex128(0.0010520361994158262+0j)) <f>: (np.float32(-0.00055208796), np.complex128(0.0016286831499744345+0j))\n",
      "Epoch 41600: <Test loss>: 0.0006594857550226152 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008596129), np.complex128(0.0011097669952164076+0j)) <f>: (np.float32(-0.00030837898), np.complex128(0.0015108577637731756+0j))\n",
      "Epoch 43200: <Test loss>: 0.0006588430260308087 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.009020994), np.complex128(0.0010590425865767078+0j)) <f>: (np.float32(-0.0007332452), np.complex128(0.0017381879994810604+0j))\n",
      "Epoch 44800: <Test loss>: 0.0006045697955414653 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00870849), np.complex128(0.0010542445897125294+0j)) <f>: (np.float32(-0.0004207399), np.complex128(0.001577798072314125+0j))\n",
      "Epoch 46400: <Test loss>: 0.0007405212963931262 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008738997), np.complex128(0.0012443648824463742+0j)) <f>: (np.float32(-0.00045124555), np.complex128(0.001473633168393387+0j))\n",
      "Epoch 48000: <Test loss>: 0.0006656874320469797 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008718697), np.complex128(0.0011457398565051134+0j)) <f>: (np.float32(-0.0004309517), np.complex128(0.001502759994390181+0j))\n",
      "Epoch 49600: <Test loss>: 0.0006500290473923087 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008699728), np.complex128(0.001138134052745431+0j)) <f>: (np.float32(-0.0004119806), np.complex128(0.0014962103055902905+0j))\n",
      "Epoch 51200: <Test loss>: 0.0006314223282970488 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008713098), np.complex128(0.0011046911735159597+0j)) <f>: (np.float32(-0.00042535382), np.complex128(0.001534478076142976+0j))\n",
      "Epoch 52800: <Test loss>: 0.0006486857309937477 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008858945), np.complex128(0.001108941893510884+0j)) <f>: (np.float32(-0.0005712043), np.complex128(0.0015067864247455963+0j))\n",
      "Epoch 54400: <Test loss>: 0.000673415488563478 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008776318), np.complex128(0.001153530749961653+0j)) <f>: (np.float32(-0.0004885637), np.complex128(0.0014967128006480365+0j))\n",
      "Epoch 56000: <Test loss>: 0.0006517555448226631 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008805701), np.complex128(0.0011568114933820943+0j)) <f>: (np.float32(-0.00051794277), np.complex128(0.0014924816171941102+0j))\n",
      "Epoch 57600: <Test loss>: 0.0006419427809305489 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008754166), np.complex128(0.0011192423444610692+0j)) <f>: (np.float32(-0.00046642634), np.complex128(0.001506985342253939+0j))\n",
      "Epoch 64000: <Test loss>: 0.000718706171028316 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008773849), np.complex128(0.0012255147844571252+0j)) <f>: (np.float32(-0.00048610358), np.complex128(0.0014495396657773372+0j))\n",
      "Epoch 65600: <Test loss>: 0.000679893244523555 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00879668), np.complex128(0.0011732780065089377+0j)) <f>: (np.float32(-0.00050893036), np.complex128(0.0014893701058885471+0j))\n",
      "Epoch 67200: <Test loss>: 0.0006144042126834393 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008943877), np.complex128(0.0010512523908537753+0j)) <f>: (np.float32(-0.0006561345), np.complex128(0.0015907907601023525+0j))\n",
      "Epoch 68800: <Test loss>: 0.0006772252381779253 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008840972), np.complex128(0.0011661478033479654+0j)) <f>: (np.float32(-0.0005532322), np.complex128(0.0014991128773957+0j))\n",
      "Epoch 70400: <Test loss>: 0.0007108651334419847 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00877941), np.complex128(0.0012108593340529228+0j)) <f>: (np.float32(-0.00049166393), np.complex128(0.0014613347889745217+0j))\n",
      "Epoch 72000: <Test loss>: 0.0005682510090991855 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008936402), np.complex128(0.0009868092074622001+0j)) <f>: (np.float32(-0.00064865453), np.complex128(0.0016685425143151739+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1002f8",
   "metadata": {},
   "source": [
    "### l4, w64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79df5166",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4ae6e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.00019824346236418933 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007783222), np.complex128(0.0004463675269137795+0j)) <f>: (np.float32(0.0005045228), np.complex128(0.002143435864868749+0j))\n",
      "Epoch 200: <Test loss>: 0.00015587535744998604 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007991228), np.complex128(0.0004344052648025381+0j)) <f>: (np.float32(0.0002965154), np.complex128(0.0021116575243044344+0j))\n",
      "Epoch 300: <Test loss>: 0.00014729845861438662 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007935573), np.complex128(0.000429854582786902+0j)) <f>: (np.float32(0.00035217297), np.complex128(0.002091805734576745+0j))\n",
      "Epoch 400: <Test loss>: 0.00013285396562423557 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008035657), np.complex128(0.00038204660547926133+0j)) <f>: (np.float32(0.00025209415), np.complex128(0.0021346167657967006+0j))\n",
      "Epoch 500: <Test loss>: 0.00012801312550436705 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008045802), np.complex128(0.00037549051021625127+0j)) <f>: (np.float32(0.00024194604), np.complex128(0.002137999378323774+0j))\n",
      "Epoch 600: <Test loss>: 0.00013197299267631024 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008056487), np.complex128(0.00037300921093369184+0j)) <f>: (np.float32(0.00023126448), np.complex128(0.002118559632006223+0j))\n",
      "Epoch 700: <Test loss>: 0.00013560369552578777 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008050091), np.complex128(0.00039740787685734255+0j)) <f>: (np.float32(0.00023765476), np.complex128(0.002085474499826195+0j))\n",
      "Epoch 800: <Test loss>: 0.00013423409836832434 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008020621), np.complex128(0.00037407782166808297+0j)) <f>: (np.float32(0.00026712788), np.complex128(0.0021058844764395403+0j))\n",
      "Epoch 900: <Test loss>: 0.00013585458509624004 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008018844), np.complex128(0.00038142188017962234+0j)) <f>: (np.float32(0.00026890528), np.complex128(0.0021059239301035038+0j))\n",
      "Epoch 1000: <Test loss>: 0.00014269465464167297 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008003879), np.complex128(0.0004026697079944024+0j)) <f>: (np.float32(0.00028385964), np.complex128(0.0020815286259872326+0j))\n",
      "Epoch 1100: <Test loss>: 0.0001479458878748119 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007989045), np.complex128(0.00040798922901470134+0j)) <f>: (np.float32(0.0002987003), np.complex128(0.00206658837341069+0j))\n",
      "Epoch 1200: <Test loss>: 0.00014656067651230842 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008020437), np.complex128(0.00039909636047128714+0j)) <f>: (np.float32(0.00026730794), np.complex128(0.0020929105640283756+0j))\n",
      "Epoch 1300: <Test loss>: 0.00015455855464097112 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00801053), np.complex128(0.0004215094985553653+0j)) <f>: (np.float32(0.0002772155), np.complex128(0.0020675809311819105+0j))\n",
      "Epoch 1400: <Test loss>: 0.0001590330502949655 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0079816645), np.complex128(0.00042405343528674125+0j)) <f>: (np.float32(0.00030608484), np.complex128(0.002068826956543481+0j))\n",
      "Epoch 1500: <Test loss>: 0.00015991300460882485 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008012124), np.complex128(0.0004273823857563984+0j)) <f>: (np.float32(0.00027562064), np.complex128(0.0020649409609340705+0j))\n",
      "Epoch 1600: <Test loss>: 0.00016532200970686972 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008010576), np.complex128(0.00043980746725527474+0j)) <f>: (np.float32(0.00027717673), np.complex128(0.0020520243895393413+0j))\n",
      "Epoch 1700: <Test loss>: 0.0001689355995040387 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007987167), np.complex128(0.000446223952366543+0j)) <f>: (np.float32(0.00030058698), np.complex128(0.002045755062788837+0j))\n",
      "Epoch 1800: <Test loss>: 0.00016999388753902167 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00799236), np.complex128(0.00044348068582969284+0j)) <f>: (np.float32(0.0002953859), np.complex128(0.0020577066051746607+0j))\n",
      "Epoch 1900: <Test loss>: 0.00017390245920978487 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008001631), np.complex128(0.00044972996859657605+0j)) <f>: (np.float32(0.0002861172), np.complex128(0.002053460198442035+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b40259e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc4a690d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.00025555980391800404 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00740129), np.complex128(0.0005659094646490858+0j)) <f>: (np.float32(0.0008864575), np.complex128(0.0022256357342569674+0j))\n",
      "Epoch 400: <Test loss>: 0.00019390425586607307 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007582245), np.complex128(0.0004957225382041133+0j)) <f>: (np.float32(0.0007055018), np.complex128(0.0021383312457994275+0j))\n",
      "Epoch 600: <Test loss>: 0.00018161813204642385 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007480251), np.complex128(0.00047662401533558337+0j)) <f>: (np.float32(0.0008074958), np.complex128(0.002050486457808704+0j))\n",
      "Epoch 800: <Test loss>: 0.00017692569235805422 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007573671), np.complex128(0.0004375394524503309+0j)) <f>: (np.float32(0.0007140769), np.complex128(0.002114041997191431+0j))\n",
      "Epoch 1000: <Test loss>: 0.00016140977095346898 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00767565), np.complex128(0.000412958709770399+0j)) <f>: (np.float32(0.00061209855), np.complex128(0.002140695420981491+0j))\n",
      "Epoch 1200: <Test loss>: 0.00016402255278080702 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007611691), np.complex128(0.0004400787270526051+0j)) <f>: (np.float32(0.0006760557), np.complex128(0.002105172026996394+0j))\n",
      "Epoch 1400: <Test loss>: 0.0001716688129818067 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0075950716), np.complex128(0.000440653247247699+0j)) <f>: (np.float32(0.00069268054), np.complex128(0.002072533444324891+0j))\n",
      "Epoch 1600: <Test loss>: 0.0001715781691018492 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0076184548), np.complex128(0.0004512485857923631+0j)) <f>: (np.float32(0.00066928874), np.complex128(0.002059606977799007+0j))\n",
      "Epoch 1800: <Test loss>: 0.00017904210835695267 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0076606395), np.complex128(0.00045656268351962527+0j)) <f>: (np.float32(0.00062711106), np.complex128(0.0020454854838951966+0j))\n",
      "Epoch 2000: <Test loss>: 0.0001793233532225713 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0076540792), np.complex128(0.0004661255031864822+0j)) <f>: (np.float32(0.00063366414), np.complex128(0.002053452079360062+0j))\n",
      "Epoch 2200: <Test loss>: 0.00018737895879894495 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00770468), np.complex128(0.00046694784567274163+0j)) <f>: (np.float32(0.00058307307), np.complex128(0.002038028107102836+0j))\n",
      "Epoch 2400: <Test loss>: 0.00019330126815475523 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0076971804), np.complex128(0.00048112230390036137+0j)) <f>: (np.float32(0.00059056777), np.complex128(0.0020170464963743173+0j))\n",
      "Epoch 2600: <Test loss>: 0.0001954617619048804 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0076921997), np.complex128(0.000482262527474969+0j)) <f>: (np.float32(0.00059555226), np.complex128(0.0020297797541213706+0j))\n",
      "Epoch 2800: <Test loss>: 0.00019489077385514975 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0076875133), np.complex128(0.00048494667694620194+0j)) <f>: (np.float32(0.0006002368), np.complex128(0.002040508359784985+0j))\n",
      "Epoch 3000: <Test loss>: 0.00020308353123255074 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0077145235), np.complex128(0.0004914372486804754+0j)) <f>: (np.float32(0.00057322637), np.complex128(0.002022081722664899+0j))\n",
      "Epoch 3200: <Test loss>: 0.0002112786314683035 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0077326284), np.complex128(0.0005108902836517246+0j)) <f>: (np.float32(0.00055511954), np.complex128(0.001995563532333955+0j))\n",
      "Epoch 3400: <Test loss>: 0.0002130058128386736 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0077329003), np.complex128(0.0005185855558889317+0j)) <f>: (np.float32(0.0005548494), np.complex128(0.00198634114323702+0j))\n",
      "Epoch 3600: <Test loss>: 0.0002162189339287579 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0077361884), np.complex128(0.0005155082969604429+0j)) <f>: (np.float32(0.0005515498), np.complex128(0.001991643791790093+0j))\n",
      "Epoch 3800: <Test loss>: 0.00021924328757449985 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0077444254), np.complex128(0.0005192125963955396+0j)) <f>: (np.float32(0.0005433155), np.complex128(0.001989069535361974+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94e9a29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4d0119ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.0003215968899894506 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008198481), np.complex128(0.0006673927245965435+0j)) <f>: (np.float32(8.926642e-05), np.complex128(0.0019940328316606997+0j))\n",
      "Epoch 800: <Test loss>: 0.0002596214471850544 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008007853), np.complex128(0.0005993366131572339+0j)) <f>: (np.float32(0.0002798965), np.complex128(0.0020133377251011025+0j))\n",
      "Epoch 1200: <Test loss>: 0.0002531628997530788 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008204578), np.complex128(0.0006136256899873968+0j)) <f>: (np.float32(8.317441e-05), np.complex128(0.0019436470775417403+0j))\n",
      "Epoch 1600: <Test loss>: 0.00023620747379027307 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008260936), np.complex128(0.0005801380289464173+0j)) <f>: (np.float32(2.681613e-05), np.complex128(0.0019864096479911683+0j))\n",
      "Epoch 2000: <Test loss>: 0.00023639319988433272 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008200752), np.complex128(0.0005758968869310085+0j)) <f>: (np.float32(8.699438e-05), np.complex128(0.0019810100778970404+0j))\n",
      "Epoch 2400: <Test loss>: 0.00023468471772503108 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008403888), np.complex128(0.0005989859068741895+0j)) <f>: (np.float32(-0.000116141986), np.complex128(0.0019357102943309951+0j))\n",
      "Epoch 2800: <Test loss>: 0.00023167225299403071 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008316552), np.complex128(0.0005989447405913724+0j)) <f>: (np.float32(-2.88033e-05), np.complex128(0.0019307515650158792+0j))\n",
      "Epoch 3200: <Test loss>: 0.00023574735678266734 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008280996), np.complex128(0.0006062650444451075+0j)) <f>: (np.float32(6.746357e-06), np.complex128(0.0019126361170845464+0j))\n",
      "Epoch 3600: <Test loss>: 0.0002380598452873528 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008279135), np.complex128(0.0005962270681918361+0j)) <f>: (np.float32(8.61405e-06), np.complex128(0.001912565963141872+0j))\n",
      "Epoch 4000: <Test loss>: 0.00024676506291143596 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008248503), np.complex128(0.0006064119490845596+0j)) <f>: (np.float32(3.924268e-05), np.complex128(0.0019003574010673348+0j))\n",
      "Epoch 4400: <Test loss>: 0.00024819927057251334 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0082240505), np.complex128(0.0006023917349012801+0j)) <f>: (np.float32(6.369224e-05), np.complex128(0.0018950321713175236+0j))\n",
      "Epoch 4800: <Test loss>: 0.0002554002858232707 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008236451), np.complex128(0.0006218099149073399+0j)) <f>: (np.float32(5.1297633e-05), np.complex128(0.0018748202241879303+0j))\n",
      "Epoch 5200: <Test loss>: 0.00026339275063946843 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008211745), np.complex128(0.000629156415486504+0j)) <f>: (np.float32(7.600292e-05), np.complex128(0.0018641303110243442+0j))\n",
      "Epoch 5600: <Test loss>: 0.00026862381491810083 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008226221), np.complex128(0.0006279575188585747+0j)) <f>: (np.float32(6.152639e-05), np.complex128(0.0018565262833138436+0j))\n",
      "Epoch 6000: <Test loss>: 0.0002719078038353473 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008213084), np.complex128(0.0006246392881143352+0j)) <f>: (np.float32(7.466447e-05), np.complex128(0.001863795272032295+0j))\n",
      "Epoch 6400: <Test loss>: 0.0002795809705276042 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008247229), np.complex128(0.0006356622104994772+0j)) <f>: (np.float32(4.051681e-05), np.complex128(0.0018453288006662783+0j))\n",
      "Epoch 6800: <Test loss>: 0.00028413566178642213 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008216059), np.complex128(0.0006438978139850318+0j)) <f>: (np.float32(7.1690876e-05), np.complex128(0.0018331569013212764+0j))\n",
      "Epoch 7200: <Test loss>: 0.0002936453965958208 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00819342), np.complex128(0.0006502114151044211+0j)) <f>: (np.float32(9.43282e-05), np.complex128(0.001832897598140758+0j))\n",
      "Epoch 7600: <Test loss>: 0.00028751103673130274 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008208689), np.complex128(0.0006409752616263287+0j)) <f>: (np.float32(7.906064e-05), np.complex128(0.001838218768349583+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f609a58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7556803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0004624943248927593 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.0075229877), np.complex128(0.000663434798995277+0j)) <f>: (np.float32(0.00076475856), np.complex128(0.0019714582316769126+0j))\n",
      "Epoch 1600: <Test loss>: 0.00040065974462777376 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007771261), np.complex128(0.0006808784563236576+0j)) <f>: (np.float32(0.00051648443), np.complex128(0.0019434354739678145+0j))\n",
      "Epoch 2400: <Test loss>: 0.00039411018951795995 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007799995), np.complex128(0.0006659603409315575+0j)) <f>: (np.float32(0.00048775005), np.complex128(0.0019521433162446994+0j))\n",
      "Epoch 3200: <Test loss>: 0.00039346993435174227 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00807015), np.complex128(0.0007363986417660588+0j)) <f>: (np.float32(0.00021760011), np.complex128(0.0018526081919585078+0j))\n",
      "Epoch 4000: <Test loss>: 0.0003852873924188316 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00804469), np.complex128(0.000697855710450876+0j)) <f>: (np.float32(0.00024305566), np.complex128(0.0018807882568808365+0j))\n",
      "Epoch 4800: <Test loss>: 0.0003868583298753947 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008030555), np.complex128(0.0007177503790802365+0j)) <f>: (np.float32(0.00025719122), np.complex128(0.0018496415555219036+0j))\n",
      "Epoch 5600: <Test loss>: 0.0003912467509508133 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008039122), np.complex128(0.0007518859809208719+0j)) <f>: (np.float32(0.00024863033), np.complex128(0.0018419555758277361+0j))\n",
      "Epoch 6400: <Test loss>: 0.0004105228581465781 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008140818), np.complex128(0.0007663078174273617+0j)) <f>: (np.float32(0.00014693452), np.complex128(0.0017838059495719292+0j))\n",
      "Epoch 7200: <Test loss>: 0.00040494915447197855 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00804766), np.complex128(0.0007533174765612671+0j)) <f>: (np.float32(0.00024008797), np.complex128(0.001806337924375357+0j))\n",
      "Epoch 8000: <Test loss>: 0.00041145828436128795 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008054109), np.complex128(0.0007682763776542168+0j)) <f>: (np.float32(0.00023363448), np.complex128(0.0017805888902007148+0j))\n",
      "Epoch 8800: <Test loss>: 0.0004368899099063128 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007957949), np.complex128(0.0007869362480900477+0j)) <f>: (np.float32(0.00032979637), np.complex128(0.0017406809382287923+0j))\n",
      "Epoch 9600: <Test loss>: 0.0004564146511256695 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007960539), np.complex128(0.0008213442831890798+0j)) <f>: (np.float32(0.0003272015), np.complex128(0.001716651627104593+0j))\n",
      "Epoch 10400: <Test loss>: 0.00045564770698547363 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008032043), np.complex128(0.0008214167206235592+0j)) <f>: (np.float32(0.00025570247), np.complex128(0.0017210406252143727+0j))\n",
      "Epoch 11200: <Test loss>: 0.0004528793797362596 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00797203), np.complex128(0.0008110734542020311+0j)) <f>: (np.float32(0.000315716), np.complex128(0.0017336895206252986+0j))\n",
      "Epoch 12000: <Test loss>: 0.00046823802404105663 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007969421), np.complex128(0.0008393523435752518+0j)) <f>: (np.float32(0.00031832923), np.complex128(0.0016899165050521777+0j))\n",
      "Epoch 12800: <Test loss>: 0.0004822481132578105 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007914208), np.complex128(0.000844274917603456+0j)) <f>: (np.float32(0.0003735422), np.complex128(0.0016855368946309295+0j))\n",
      "Epoch 13600: <Test loss>: 0.0004898044280707836 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008006941), np.complex128(0.0008683420332030929+0j)) <f>: (np.float32(0.0002808114), np.complex128(0.0016672409239863494+0j))\n",
      "Epoch 14400: <Test loss>: 0.0005156301776878536 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007983569), np.complex128(0.0008983194598972328+0j)) <f>: (np.float32(0.00030418282), np.complex128(0.0016365391229474153+0j))\n",
      "Epoch 15200: <Test loss>: 0.0004977908101864159 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008015318), np.complex128(0.0008809509040771041+0j)) <f>: (np.float32(0.0002724337), np.complex128(0.001650718084728317+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96b5e308",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc4e9392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.002505482407286763 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0008983048028312624 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008126524), np.complex128(0.0011361598472193913+0j)) <f>: (np.float32(0.00016122495), np.complex128(0.001523542180167734+0j))\n",
      "Epoch 3200: <Test loss>: 0.0008421388338319957 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007872938), np.complex128(0.0011666206130122468+0j)) <f>: (np.float32(0.00041480502), np.complex128(0.0014426526544935926+0j))\n",
      "Epoch 4800: <Test loss>: 0.0008625198970548809 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008072459), np.complex128(0.0012484821450313643+0j)) <f>: (np.float32(0.00021528822), np.complex128(0.001376357939830166+0j))\n",
      "Epoch 6400: <Test loss>: 0.0007962426170706749 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008093664), np.complex128(0.0011942304392866157+0j)) <f>: (np.float32(0.00019408423), np.complex128(0.001390775907086653+0j))\n",
      "Epoch 8000: <Test loss>: 0.0008113480289466679 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00814062), np.complex128(0.0012032778875391598+0j)) <f>: (np.float32(0.00014713196), np.complex128(0.0013608671120073177+0j))\n",
      "Epoch 9600: <Test loss>: 0.0008139024139381945 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008042586), np.complex128(0.0011905835760134464+0j)) <f>: (np.float32(0.00024515687), np.complex128(0.0013688235586197166+0j))\n",
      "Epoch 12800: <Test loss>: 0.0008365768590010703 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008117447), np.complex128(0.0011971907326904284+0j)) <f>: (np.float32(0.00017030344), np.complex128(0.001367812986635368+0j))\n",
      "Epoch 14400: <Test loss>: 0.0008148222113959491 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008178139), np.complex128(0.001185513589903167+0j)) <f>: (np.float32(0.00010960542), np.complex128(0.0014009113123236027+0j))\n",
      "Epoch 16000: <Test loss>: 0.0008302401984110475 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008103801), np.complex128(0.0011985513132242143+0j)) <f>: (np.float32(0.00018394546), np.complex128(0.0013744717755992731+0j))\n",
      "Epoch 17600: <Test loss>: 0.0008418512879870832 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008131854), np.complex128(0.0011689667739811825+0j)) <f>: (np.float32(0.00015589173), np.complex128(0.0013988682214614468+0j))\n",
      "Epoch 19200: <Test loss>: 0.0008704665233381093 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008173477), np.complex128(0.0012031176625308456+0j)) <f>: (np.float32(0.00011427544), np.complex128(0.0013670071677495306+0j))\n",
      "Epoch 20800: <Test loss>: 0.0008583327871747315 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008200132), np.complex128(0.0012087185604857773+0j)) <f>: (np.float32(8.7621076e-05), np.complex128(0.001367694498782822+0j))\n",
      "Epoch 22400: <Test loss>: 0.0009203157969750464 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.00809871), np.complex128(0.00125396759978949+0j)) <f>: (np.float32(0.00018904028), np.complex128(0.0013329129859130239+0j))\n",
      "Epoch 24000: <Test loss>: 0.0008721514022909105 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.007987231), np.complex128(0.0012365308563668522+0j)) <f>: (np.float32(0.00030051943), np.complex128(0.0013406933499351295+0j))\n",
      "Epoch 25600: <Test loss>: 0.0008800706709735096 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008130666), np.complex128(0.0012267070209006233+0j)) <f>: (np.float32(0.00015708512), np.complex128(0.0013393635965407106+0j))\n",
      "Epoch 27200: <Test loss>: 0.0008898109663277864 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008199545), np.complex128(0.0012205242131173954+0j)) <f>: (np.float32(8.8199224e-05), np.complex128(0.0013789741871353656+0j))\n",
      "Epoch 28800: <Test loss>: 0.0009022380691021681 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008153602), np.complex128(0.001254620044142428+0j)) <f>: (np.float32(0.00013414685), np.complex128(0.0013202914924037574+0j))\n",
      "Epoch 30400: <Test loss>: 0.0009020253783091903 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008125265), np.complex128(0.001252949035583824+0j)) <f>: (np.float32(0.00016248338), np.complex128(0.0013137388858087827+0j))\n",
      "Epoch 32000: <Test loss>: 0.000914883625227958 <O>: (np.float32(0.008287755), np.complex128(0.0023403281697018277+0j)) <O-f>: (np.float32(0.008164401), np.complex128(0.0012776132842904568+0j)) <f>: (np.float32(0.00012334745), np.complex128(0.001306461016844423+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_32x32_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_30min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19320ed",
   "metadata": {},
   "source": [
    "# 64x64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec13821",
   "metadata": {},
   "source": [
    "## Sweep 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce09bf53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(3.405124e-05), np.complex128(1.0658076532853887e-05+0j))\n",
      "bin size 1: (np.float32(3.405124e-05), np.complex128(1.065807480584099e-05+0j))\n",
      "jack bin size 2: (np.float32(3.405124e-05), np.complex128(1.3135326250680487e-05+0j))\n",
      "bin size 2: (np.float32(3.405124e-05), np.complex128(1.3135326786436573e-05+0j))\n",
      "jack bin size 4: (np.float32(3.405124e-05), np.complex128(1.5655767322892354e-05+0j))\n",
      "bin size 4: (np.float32(3.405124e-05), np.complex128(1.5655769771065156e-05+0j))\n",
      "jack bin size 5: (np.float32(3.405124e-05), np.complex128(1.63665270248193e-05+0j))\n",
      "bin size 5: (np.float32(3.405124e-05), np.complex128(1.636652599978707e-05+0j))\n",
      "jack bin size 10: (np.float32(3.405124e-05), np.complex128(1.805728205996368e-05+0j))\n",
      "bin size 10: (np.float32(3.405124e-05), np.complex128(1.8057281353648882e-05+0j))\n",
      "jack bin size 20: (np.float32(3.405124e-05), np.complex128(1.8911975856213364e-05+0j))\n",
      "bin size 20: (np.float32(3.405124e-05), np.complex128(1.8911982022243948e-05+0j))\n",
      "jack bin size 50: (np.float32(3.405124e-05), np.complex128(1.946232620715423e-05+0j))\n",
      "bin size 50: (np.float32(3.405124e-05), np.complex128(1.9462327754418052e-05+0j))\n",
      "jack bin size 100: (np.float32(3.405124e-05), np.complex128(1.9338735359686657e-05+0j))\n",
      "bin size 100: (np.float32(3.405124e-05), np.complex128(1.9338736820024956e-05+0j))\n",
      "jack bin size 200: (np.float32(3.405124e-05), np.complex128(1.917474828565085e-05+0j))\n",
      "bin size 200: (np.float32(3.405124e-05), np.complex128(1.917474579697248e-05+0j))\n",
      "jack bin size 500: (np.float32(3.405124e-05), np.complex128(1.7383168743526425e-05+0j))\n",
      "bin size 500: (np.float32(3.405124e-05), np.complex128(1.738317037951281e-05+0j))\n",
      "jack bin size 1000: (np.float32(3.405124e-05), np.complex128(1.6946773493264427e-05+0j))\n",
      "bin size 1000: (np.float32(3.405124e-05), np.complex128(1.6946770431107925e-05+0j))\n",
      "jack bin size 2000: (np.float32(3.405124e-05), np.complex128(1.692948490017443e-05+0j))\n",
      "bin size 2000: (np.float32(3.405124e-05), np.complex128(1.692947885853105e-05+0j))\n",
      "jack bin size 5000: (np.float32(3.405124e-05), np.complex128(1.8850534072753613e-05+0j))\n",
      "bin size 5000: (np.float32(3.405124e-05), np.complex128(1.8850535324667977e-05+0j))\n",
      "jack bin size 10000: (np.float32(3.405124e-05), np.complex128(1.6037014574976638e-05+0j))\n",
      "bin size 10000: (np.float32(3.405124e-05), np.complex128(1.6037014574976638e-05+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### OLD\n",
    "with open(\"1+1scalar/config/c_64x64_0.1_0.5_sweep10.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model = eval(\"scalar.Model(geom=(64,), nbeta=64, nt=0, m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 16))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "053765b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(3.405124e-05), np.complex128(1.0658076532853887e-05+0j))\n",
      "bin size 1: (np.float32(3.405124e-05), np.complex128(1.065807480584099e-05+0j))\n",
      "jack bin size 2: (np.float32(3.405124e-05), np.complex128(1.3135326250680487e-05+0j))\n",
      "bin size 2: (np.float32(3.405124e-05), np.complex128(1.3135326786436573e-05+0j))\n",
      "jack bin size 4: (np.float32(3.405124e-05), np.complex128(1.5655767322892354e-05+0j))\n",
      "bin size 4: (np.float32(3.405124e-05), np.complex128(1.5655769771065156e-05+0j))\n",
      "jack bin size 5: (np.float32(3.405124e-05), np.complex128(1.63665270248193e-05+0j))\n",
      "bin size 5: (np.float32(3.405124e-05), np.complex128(1.636652599978707e-05+0j))\n",
      "jack bin size 10: (np.float32(3.405124e-05), np.complex128(1.805728205996368e-05+0j))\n",
      "bin size 10: (np.float32(3.405124e-05), np.complex128(1.8057281353648882e-05+0j))\n",
      "jack bin size 20: (np.float32(3.405124e-05), np.complex128(1.8911975856213364e-05+0j))\n",
      "bin size 20: (np.float32(3.405124e-05), np.complex128(1.8911982022243948e-05+0j))\n",
      "jack bin size 50: (np.float32(3.405124e-05), np.complex128(1.946232620715423e-05+0j))\n",
      "bin size 50: (np.float32(3.405124e-05), np.complex128(1.9462327754418052e-05+0j))\n",
      "jack bin size 100: (np.float32(3.405124e-05), np.complex128(1.9338735359686657e-05+0j))\n",
      "bin size 100: (np.float32(3.405124e-05), np.complex128(1.9338736820024956e-05+0j))\n",
      "jack bin size 200: (np.float32(3.405124e-05), np.complex128(1.917474828565085e-05+0j))\n",
      "bin size 200: (np.float32(3.405124e-05), np.complex128(1.917474579697248e-05+0j))\n",
      "jack bin size 500: (np.float32(3.405124e-05), np.complex128(1.7383168743526425e-05+0j))\n",
      "bin size 500: (np.float32(3.405124e-05), np.complex128(1.738317037951281e-05+0j))\n",
      "jack bin size 1000: (np.float32(3.405124e-05), np.complex128(1.6946773493264427e-05+0j))\n",
      "bin size 1000: (np.float32(3.405124e-05), np.complex128(1.6946770431107925e-05+0j))\n",
      "jack bin size 2000: (np.float32(3.405124e-05), np.complex128(1.692948490017443e-05+0j))\n",
      "bin size 2000: (np.float32(3.405124e-05), np.complex128(1.692947885853105e-05+0j))\n",
      "jack bin size 5000: (np.float32(3.405124e-05), np.complex128(1.8850534072753613e-05+0j))\n",
      "bin size 5000: (np.float32(3.405124e-05), np.complex128(1.8850535324667977e-05+0j))\n",
      "jack bin size 10000: (np.float32(3.405124e-05), np.complex128(1.6037014574976638e-05+0j))\n",
      "bin size 10000: (np.float32(3.405124e-05), np.complex128(1.6037014574976638e-05+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_64x64_0.1_0.5_sweep10.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model = eval(\"scalar.Model(geom=(64,64), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 16))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5aa67ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.00016814486), np.complex128(8.521981245694444e-05+0j)),\n",
       " (np.float32(0.00016147953), np.complex128(9.315137365994952e-05+0j)),\n",
       " (np.float32(0.0001011854), np.complex128(0.00010297631899207494+0j)),\n",
       " (np.float32(0.00011900659), np.complex128(0.0001384937932016328+0j)))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td= 32\n",
    "bs=25\n",
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:2]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:4]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:8]), Bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5c2facee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.00016814486), np.complex128(8.845423308216592e-05+0j)),\n",
       " (np.float32(0.00016147953), np.complex128(8.514809730518216e-05+0j)),\n",
       " (np.float32(0.0001011854), np.complex128(9.03889140317915e-05+0j)),\n",
       " (np.float32(0.00011900659), np.complex128(0.00013565501888460808+0j)))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td= 32\n",
    "bs=50\n",
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:2]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:4]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:8]), Bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7cdcb670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.00016814486), np.complex128(8.228795720818279e-05+0j)),\n",
       " (np.float32(0.00016147953), np.complex128(6.908529667271068e-05+0j)),\n",
       " (np.float32(0.0001011854), np.complex128(8.285808789440219e-05+0j)),\n",
       " (np.float32(8.325212e-05), np.complex128(0.00014569042908347925+0j)))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td= 32\n",
    "bs= 100\n",
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:2]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:4]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:8]), Bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "915f7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j) for i, j in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2862bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.1918727725278586e-05 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 5.108173809276195e-06 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-7.255829e-05), np.complex128(6.282167031072281e-05+0j)) <f>: (np.float32(-6.5397646e-05), np.complex128(0.00011857500581656355+0j))\n",
      "Epoch 200: <Test loss>: 8.661161814416118e-07 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-2.6876056e-05), np.complex128(2.7611850098268984e-05+0j)) <f>: (np.float32(-0.000111079775), np.complex128(0.00011392708510823312+0j))\n",
      "Epoch 300: <Test loss>: 7.554125431852299e-07 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-5.3034182e-05), np.complex128(2.690814412938656e-05+0j)) <f>: (np.float32(-8.492174e-05), np.complex128(0.00010735762983240586+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "91aa0ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.1918727725278586e-05 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 9.337839401268866e-06 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-0.00015378016), np.complex128(0.00011106965190992623+0j)) <f>: (np.float32(1.5824267e-05), np.complex128(0.00017470508971945345+0j))\n",
      "Epoch 400: <Test loss>: 2.816008645822876e-06 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(8.8360075e-06), np.complex128(6.0875905357962295e-05+0j)) <f>: (np.float32(-0.00014679194), np.complex128(0.0001261645555547176+0j))\n",
      "Epoch 600: <Test loss>: 1.41491909744218e-06 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-4.5214696e-05), np.complex128(4.4765387017753134e-05+0j)) <f>: (np.float32(-9.2741204e-05), np.complex128(0.00011608559538082195+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "30f5b809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.1918727725278586e-05 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.0008733688737265766 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(0.00061379425), np.complex128(0.0008139577580730011+0j)) <f>: (np.float32(-0.0007517505), np.complex128(0.0008240275763508859+0j))\n",
      "Epoch 800: <Test loss>: 3.497249053907581e-05 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(-5.1782963e-06), np.complex128(0.0001758163414917857+0j)) <f>: (np.float32(-0.00013277747), np.complex128(0.00019088470747313547+0j))\n",
      "Epoch 1200: <Test loss>: 3.781350824283436e-05 <O>: (np.float32(-0.00013795587), np.complex128(0.00011188290800171183+0j)) <O-f>: (np.float32(0.00019811057), np.complex128(0.0001593502248044917+0j)) <f>: (np.float32(-0.00033606632), np.complex128(0.00016432782464216255+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc1ea3d",
   "metadata": {},
   "source": [
    "## sweep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c603bbde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(-0.00011681768), np.complex128(1.5184077730095093e-05+0j))\n",
      "bin size 1: (np.float32(-0.00011681768), np.complex128(1.5183940980571554e-05+0j))\n",
      "jack bin size 2: (np.float32(-0.00011681768), np.complex128(2.1067158836151646e-05+0j))\n",
      "bin size 2: (np.float32(-0.00011681768), np.complex128(2.1067123963546364e-05+0j))\n",
      "jack bin size 4: (np.float32(-0.00011681768), np.complex128(2.9006948315715688e-05+0j))\n",
      "bin size 4: (np.float32(-0.00011681768), np.complex128(2.9006846226500887e-05+0j))\n",
      "jack bin size 5: (np.float32(-0.00011681768), np.complex128(3.205067928375791e-05+0j))\n",
      "bin size 5: (np.float32(-0.00011681768), np.complex128(3.20506346561429e-05+0j))\n",
      "jack bin size 10: (np.float32(-0.00011681768), np.complex128(4.3061662024710435e-05+0j))\n",
      "bin size 10: (np.float32(-0.00011681768), np.complex128(4.3061698707887046e-05+0j))\n",
      "jack bin size 20: (np.float32(-0.00011681768), np.complex128(5.578131639187397e-05+0j))\n",
      "bin size 20: (np.float32(-0.00011681768), np.complex128(5.5781262341122845e-05+0j))\n",
      "jack bin size 50: (np.float32(-0.00011681768), np.complex128(7.241630306331313e-05+0j))\n",
      "bin size 50: (np.float32(-0.00011681768), np.complex128(7.241628605739616e-05+0j))\n",
      "jack bin size 100: (np.float32(-0.00011681768), np.complex128(8.206552902277502e-05+0j))\n",
      "bin size 100: (np.float32(-0.00011681768), np.complex128(8.206551763501379e-05+0j))\n",
      "jack bin size 200: (np.float32(-0.00011681768), np.complex128(8.762375266449178e-05+0j))\n",
      "bin size 200: (np.float32(-0.00011681768), np.complex128(8.762374712730964e-05+0j))\n",
      "jack bin size 500: (np.float32(-0.00011681768), np.complex128(9.000512367954067e-05+0j))\n",
      "bin size 500: (np.float32(-0.00011681768), np.complex128(9.00051037898145e-05+0j))\n",
      "jack bin size 1000: (np.float32(-0.00011681768), np.complex128(9.312725110100553e-05+0j))\n",
      "bin size 1000: (np.float32(-0.00011681768), np.complex128(9.31272615214784e-05+0j))\n",
      "jack bin size 2000: (np.float32(-0.00011681768), np.complex128(9.547814352117712e-05+0j))\n",
      "bin size 2000: (np.float32(-0.00011681768), np.complex128(9.547812598092214e-05+0j))\n",
      "jack bin size 5000: (np.float32(-0.00011681768), np.complex128(0.00010710252098232102+0j))\n",
      "bin size 5000: (np.float32(-0.00011681768), np.complex128(0.00010710249594403366+0j))\n",
      "jack bin size 10000: (np.float32(-0.00011681768), np.complex128(0.00012341887486400083+0j))\n",
      "bin size 10000: (np.float32(-0.00011681768), np.complex128(0.00012341885788676638+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_64x64_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(64,64), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14cc8110",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9457290e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.5106241082539782e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 5.05856496602064e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(0.00011667244), np.complex128(5.3624098329584885e-05+0j)) <f>: (np.float32(-0.00018045455), np.complex128(0.0001812264727355118+0j))\n",
      "Epoch 200: <Test loss>: 1.0848041256394936e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(6.672085e-05), np.complex128(3.636521354642196e-05+0j)) <f>: (np.float32(-0.00013050313), np.complex128(0.00018949550400387783+0j))\n",
      "Epoch 300: <Test loss>: 8.757203318054962e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(3.0016503e-07), np.complex128(3.362667256900154e-05+0j)) <f>: (np.float32(-6.408235e-05), np.complex128(0.0001894479312579413+0j))\n",
      "Epoch 400: <Test loss>: 7.778968438287848e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.2186731e-06), np.complex128(2.7596295792545474e-05+0j)) <f>: (np.float32(-6.256341e-05), np.complex128(0.0001905775143950411+0j))\n",
      "Epoch 500: <Test loss>: 9.062494541467458e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-7.953216e-06), np.complex128(3.3715788215327214e-05+0j)) <f>: (np.float32(-5.582898e-05), np.complex128(0.00018223910620551755+0j))\n",
      "Epoch 600: <Test loss>: 8.717980790606816e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-9.9684485e-06), np.complex128(3.12531612355479e-05+0j)) <f>: (np.float32(-5.3813863e-05), np.complex128(0.00018663864960731318+0j))\n",
      "Epoch 700: <Test loss>: 9.657212558522588e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.1303921e-05), np.complex128(3.011926880054533e-05+0j)) <f>: (np.float32(-5.247822e-05), np.complex128(0.000183917916694455+0j))\n",
      "Epoch 800: <Test loss>: 9.221922709912178e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.9264975e-05), np.complex128(3.3025180609159325e-05+0j)) <f>: (np.float32(-4.4516975e-05), np.complex128(0.00018522447044643886+0j))\n",
      "Epoch 900: <Test loss>: 8.744802926230477e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-2.1671183e-05), np.complex128(3.107749292458388e-05+0j)) <f>: (np.float32(-4.2110787e-05), np.complex128(0.00019043376541440276+0j))\n",
      "Epoch 1000: <Test loss>: 8.906155812837824e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.7952738e-05), np.complex128(3.172690253277071e-05+0j)) <f>: (np.float32(-4.5829456e-05), np.complex128(0.00018968014968843964+0j))\n",
      "Epoch 1100: <Test loss>: 8.909921120903164e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.761571e-05), np.complex128(2.899609212656498e-05+0j)) <f>: (np.float32(-4.616644e-05), np.complex128(0.00018982263006251965+0j))\n",
      "Epoch 1200: <Test loss>: 9.263739570997132e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.7124721e-05), np.complex128(2.9284458290454973e-05+0j)) <f>: (np.float32(-4.665737e-05), np.complex128(0.00018826795272531267+0j))\n",
      "Epoch 1300: <Test loss>: 9.314596240983519e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.3938716e-05), np.complex128(2.914872333530921e-05+0j)) <f>: (np.float32(-4.98433e-05), np.complex128(0.00019034895906597983+0j))\n",
      "Epoch 1400: <Test loss>: 9.335752793049323e-07 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.4627624e-05), np.complex128(2.831230129845025e-05+0j)) <f>: (np.float32(-4.9154576e-05), np.complex128(0.00019254567818234673+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38f07b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.5106241082539782e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 7.569273748231353e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-4.474047e-05), np.complex128(0.00011160772345284176+0j)) <f>: (np.float32(-1.9041667e-05), np.complex128(0.00013782720451205058+0j))\n",
      "Epoch 400: <Test loss>: 1.0624934475345071e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(2.4617977e-05), np.complex128(8.811629358057434e-05+0j)) <f>: (np.float32(-8.84001e-05), np.complex128(0.0001817840094603064+0j))\n",
      "Epoch 800: <Test loss>: 1.483207825003774e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(3.612586e-05), np.complex128(3.891447446094726e-05+0j)) <f>: (np.float32(-9.990801e-05), np.complex128(0.00017197365293875955+0j))\n",
      "Epoch 1000: <Test loss>: 1.4897016171744326e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(3.4686767e-05), np.complex128(3.579999382315863e-05+0j)) <f>: (np.float32(-9.846901e-05), np.complex128(0.0001811772825162134+0j))\n",
      "Epoch 1200: <Test loss>: 1.555904987071699e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.6795513e-05), np.complex128(3.544350348708747e-05+0j)) <f>: (np.float32(-8.057752e-05), np.complex128(0.00018088333051907135+0j))\n",
      "Epoch 1400: <Test loss>: 1.56906503434584e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.6800177e-05), np.complex128(3.850086907898377e-05+0j)) <f>: (np.float32(-8.0582366e-05), np.complex128(0.0001837118474166398+0j))\n",
      "Epoch 1600: <Test loss>: 1.5988877066774876e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(2.3057446e-05), np.complex128(3.836882299384584e-05+0j)) <f>: (np.float32(-8.683962e-05), np.complex128(0.00018037650634144517+0j))\n",
      "Epoch 1800: <Test loss>: 1.6174960819625994e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.7236101e-05), np.complex128(3.790570627654886e-05+0j)) <f>: (np.float32(-8.1018254e-05), np.complex128(0.00018159390876754388+0j))\n",
      "Epoch 2000: <Test loss>: 1.6365448800570448e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.9200113e-05), np.complex128(3.750798622733268e-05+0j)) <f>: (np.float32(-8.298235e-05), np.complex128(0.00018520293585011158+0j))\n",
      "Epoch 2200: <Test loss>: 1.9820147372229258e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(8.7615535e-06), np.complex128(4.186124927612302e-05+0j)) <f>: (np.float32(-7.2543684e-05), np.complex128(0.00018885869522677096+0j))\n",
      "Epoch 2400: <Test loss>: 1.73107241607795e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.0183801e-05), np.complex128(3.960548048886237e-05+0j)) <f>: (np.float32(-7.3966e-05), np.complex128(0.00019401312296108802+0j))\n",
      "Epoch 2600: <Test loss>: 1.7683239548205165e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.612869e-05), np.complex128(3.8661062372134165e-05+0j)) <f>: (np.float32(-7.991082e-05), np.complex128(0.0001812275351935044+0j))\n",
      "Epoch 2800: <Test loss>: 1.7678723907010863e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.6216467e-05), np.complex128(3.9042453076307615e-05+0j)) <f>: (np.float32(-7.9998536e-05), np.complex128(0.00018559127217519176+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04e0258f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc4f8fc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.5106241082539782e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 1.5262208762578666e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-3.5496345e-05), np.complex128(0.00014264556251148975+0j)) <f>: (np.float32(-2.8285836e-05), np.complex128(0.00018111415348235558+0j))\n",
      "Epoch 800: <Test loss>: 9.508198672847357e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-4.343863e-05), np.complex128(0.0001057502022487462+0j)) <f>: (np.float32(-2.0343417e-05), np.complex128(0.0001473273232997631+0j))\n",
      "Epoch 1200: <Test loss>: 5.763189165008953e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-2.8716066e-05), np.complex128(7.235745676470124e-05+0j)) <f>: (np.float32(-3.5066023e-05), np.complex128(0.00015748453271193343+0j))\n",
      "Epoch 1600: <Test loss>: 5.041218173573725e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(7.2352634e-07), np.complex128(7.345348525833379e-05+0j)) <f>: (np.float32(-6.4505686e-05), np.complex128(0.00015557711932118876+0j))\n",
      "Epoch 2000: <Test loss>: 4.385562078823568e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-2.0227624e-05), np.complex128(6.780124045295453e-05+0j)) <f>: (np.float32(-4.3554555e-05), np.complex128(0.0001642468558285785+0j))\n",
      "Epoch 2400: <Test loss>: 5.780178526038071e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.9819447e-05), np.complex128(8.926864820539579e-05+0j)) <f>: (np.float32(-8.360158e-05), np.complex128(0.00016561500042896823+0j))\n",
      "Epoch 2800: <Test loss>: 4.8862102630664594e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-2.8479599e-05), np.complex128(7.395394261679548e-05+0j)) <f>: (np.float32(-3.5302437e-05), np.complex128(0.00016244641768586323+0j))\n",
      "Epoch 3200: <Test loss>: 4.392674327391433e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.4977794e-05), np.complex128(6.920609535562528e-05+0j)) <f>: (np.float32(-4.8804217e-05), np.complex128(0.00016522978804753787+0j))\n",
      "Epoch 3600: <Test loss>: 4.505575361690717e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(8.970551e-06), np.complex128(6.89987891864157e-05+0j)) <f>: (np.float32(-7.275276e-05), np.complex128(0.00017246133701493728+0j))\n",
      "Epoch 4000: <Test loss>: 4.417298441694584e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.0040523e-05), np.complex128(7.080455547667588e-05+0j)) <f>: (np.float32(-7.3822644e-05), np.complex128(0.0001671433700376723+0j))\n",
      "Epoch 4400: <Test loss>: 4.337125574238598e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.5909071e-06), np.complex128(6.589856451482606e-05+0j)) <f>: (np.float32(-6.2191175e-05), np.complex128(0.00016074212406268573+0j))\n",
      "Epoch 4800: <Test loss>: 4.469517989491578e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(9.849311e-06), np.complex128(7.159479428821917e-05+0j)) <f>: (np.float32(-7.363156e-05), np.complex128(0.00016497400525021893+0j))\n",
      "Epoch 5200: <Test loss>: 4.643476586352335e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(8.043094e-06), np.complex128(7.155342978562733e-05+0j)) <f>: (np.float32(-7.182522e-05), np.complex128(0.00016487451478088358+0j))\n",
      "Epoch 5600: <Test loss>: 4.7018652367114555e-06 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(8.6046175e-06), np.complex128(7.517947205365835e-05+0j)) <f>: (np.float32(-7.2386625e-05), np.complex128(0.00016477583304822917+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "efb7747e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9aa78a5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.5106241082539782e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 3.487565481918864e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-0.0002162993), np.complex128(0.00023542667305399302+0j)) <f>: (np.float32(0.00015251743), np.complex128(0.0001398093388290796+0j))\n",
      "Epoch 1600: <Test loss>: 6.188396218931302e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.8753291e-05), np.complex128(0.00024578628864254146+0j)) <f>: (np.float32(-8.2535385e-05), np.complex128(0.0001872261571620478+0j))\n",
      "Epoch 2400: <Test loss>: 0.00014460146485362202 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-8.947731e-05), np.complex128(0.00045270415324220225+0j)) <f>: (np.float32(2.5695232e-05), np.complex128(0.00033167096849760444+0j))\n",
      "Epoch 3200: <Test loss>: 1.789656380424276e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-0.00016344071), np.complex128(0.00017844935026113684+0j)) <f>: (np.float32(9.965876e-05), np.complex128(0.0001213886007294899+0j))\n",
      "Epoch 4000: <Test loss>: 2.0556917661451735e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-0.00014960192), np.complex128(0.00017106441090325593+0j)) <f>: (np.float32(8.581983e-05), np.complex128(0.00012083770833154446+0j))\n",
      "Epoch 4800: <Test loss>: 1.3273923286760692e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-7.491487e-05), np.complex128(0.00014230543323562529+0j)) <f>: (np.float32(1.1132585e-05), np.complex128(0.00011499231817766272+0j))\n",
      "Epoch 5600: <Test loss>: 1.2874019375885837e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-0.00010300992), np.complex128(0.00013718146791628962+0j)) <f>: (np.float32(3.9227874e-05), np.complex128(0.00013147731331630255+0j))\n",
      "Epoch 6400: <Test loss>: 1.3783817848889157e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-0.0001059146), np.complex128(0.00014357370678471217+0j)) <f>: (np.float32(4.213246e-05), np.complex128(0.0001310033936212825+0j))\n",
      "Epoch 7200: <Test loss>: 1.3144405784260016e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-9.99839e-05), np.complex128(0.00013962202907074535+0j)) <f>: (np.float32(3.620181e-05), np.complex128(0.00013211742047046103+0j))\n",
      "Epoch 8000: <Test loss>: 1.344666179647902e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-8.680044e-05), np.complex128(0.00014280921275751157+0j)) <f>: (np.float32(2.3018305e-05), np.complex128(0.0001383666477358073+0j))\n",
      "Epoch 8800: <Test loss>: 1.340042035735678e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-8.3790794e-05), np.complex128(0.00014310306960916173+0j)) <f>: (np.float32(2.0008632e-05), np.complex128(0.00013513092399060383+0j))\n",
      "Epoch 9600: <Test loss>: 1.508822515461361e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-7.867171e-05), np.complex128(0.0001607678767758194+0j)) <f>: (np.float32(1.4889663e-05), np.complex128(0.00014492491548754848+0j))\n",
      "Epoch 10400: <Test loss>: 1.381762012897525e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-8.091788e-05), np.complex128(0.0001427445931109477+0j)) <f>: (np.float32(1.7135666e-05), np.complex128(0.0001360504734543937+0j))\n",
      "Epoch 11200: <Test loss>: 1.3040367775829509e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-8.668747e-05), np.complex128(0.00013876571964388716+0j)) <f>: (np.float32(2.290523e-05), np.complex128(0.00013768401054678153+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d1af068",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14dcd9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.5106241082539782e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 3.919073787983507e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(6.983551e-06), np.complex128(0.00023882967843149143+0j)) <f>: (np.float32(-7.076571e-05), np.complex128(0.0001629941227098309+0j))\n",
      "Epoch 3200: <Test loss>: 4.1655755921965465e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(0.00012961958), np.complex128(0.0002426936478595332+0j)) <f>: (np.float32(-0.00019340163), np.complex128(0.00011726694559429172+0j))\n",
      "Epoch 4800: <Test loss>: 0.00014452997129410505 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-0.0004428516), np.complex128(0.0003705830643212549+0j)) <f>: (np.float32(0.00037906945), np.complex128(0.00032970605551460445+0j))\n",
      "Epoch 6400: <Test loss>: 4.393779090605676e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(0.00013182354), np.complex128(0.00023215565033130463+0j)) <f>: (np.float32(-0.00019560581), np.complex128(0.000150455469208822+0j))\n",
      "Epoch 8000: <Test loss>: 2.4164495698641986e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-2.2241366e-05), np.complex128(0.00016985201947306255+0j)) <f>: (np.float32(-4.1540876e-05), np.complex128(8.912447107004403e-05+0j))\n",
      "Epoch 9600: <Test loss>: 2.451973887218628e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(3.5780795e-05), np.complex128(0.0001643669928696503+0j)) <f>: (np.float32(-9.956269e-05), np.complex128(9.024763981523339e-05+0j))\n",
      "Epoch 11200: <Test loss>: 2.10634771065088e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.7509088e-05), np.complex128(0.0001660774392346356+0j)) <f>: (np.float32(-4.627316e-05), np.complex128(6.808333890756737e-05+0j))\n",
      "Epoch 12800: <Test loss>: 2.1009129341109656e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.17166455e-05), np.complex128(0.00016342327645092468+0j)) <f>: (np.float32(-7.549894e-05), np.complex128(6.393899157255746e-05+0j))\n",
      "Epoch 14400: <Test loss>: 2.007085640798323e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-1.0939649e-05), np.complex128(0.0001615761060141177+0j)) <f>: (np.float32(-5.2842435e-05), np.complex128(6.440110529779428e-05+0j))\n",
      "Epoch 16000: <Test loss>: 2.0166282411082648e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.7351165e-05), np.complex128(0.00015739720500797586+0j)) <f>: (np.float32(-8.113337e-05), np.complex128(6.970290547023184e-05+0j))\n",
      "Epoch 17600: <Test loss>: 2.0669715013355017e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(4.038289e-05), np.complex128(0.0001629592360294774+0j)) <f>: (np.float32(-0.000104165236), np.complex128(7.502767535016576e-05+0j))\n",
      "Epoch 19200: <Test loss>: 2.1695946998079307e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(6.7670895e-05), np.complex128(0.00015939868557501909+0j)) <f>: (np.float32(-0.00013145301), np.complex128(7.414191046478185e-05+0j))\n",
      "Epoch 20800: <Test loss>: 2.0246425265213475e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(3.802672e-06), np.complex128(0.00015835612884781923+0j)) <f>: (np.float32(-6.7584675e-05), np.complex128(7.351386300171829e-05+0j))\n",
      "Epoch 22400: <Test loss>: 1.958715438377112e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(-2.599598e-06), np.complex128(0.00016224946651768584+0j)) <f>: (np.float32(-6.118246e-05), np.complex128(7.520627929599361e-05+0j))\n",
      "Epoch 24000: <Test loss>: 1.995345337491017e-05 <O>: (np.float32(-6.378219e-05), np.complex128(0.0001931476161366821+0j)) <O-f>: (np.float32(1.8944312e-06), np.complex128(0.00016837020772504841+0j)) <f>: (np.float32(-6.567668e-05), np.complex128(7.083057776870319e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a9894",
   "metadata": {},
   "source": [
    "## m^2=0.01, lambda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8150714c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(5.373278e-05), np.complex128(5.118669839799836e-05+0j))\n",
      "bin size 1: (np.float32(5.373278e-05), np.complex128(5.1186445507927655e-05+0j))\n",
      "jack bin size 2: (np.float32(5.373278e-05), np.complex128(7.207437158027468e-05+0j))\n",
      "bin size 2: (np.float32(5.373278e-05), np.complex128(7.207435004186308e-05+0j))\n",
      "jack bin size 4: (np.float32(5.373278e-05), np.complex128(0.000101308309987129+0j))\n",
      "bin size 4: (np.float32(5.373278e-05), np.complex128(0.00010130831427076855+0j))\n",
      "jack bin size 5: (np.float32(5.373278e-05), np.complex128(0.00011296436050799229+0j))\n",
      "bin size 5: (np.float32(5.373278e-05), np.complex128(0.00011296424117195282+0j))\n",
      "jack bin size 10: (np.float32(5.373278e-05), np.complex128(0.00015773930730523085+0j))\n",
      "bin size 10: (np.float32(5.373278e-05), np.complex128(0.0001577392861015754+0j))\n",
      "jack bin size 20: (np.float32(5.373278e-05), np.complex128(0.0002179955464750625+0j))\n",
      "bin size 20: (np.float32(5.373278e-05), np.complex128(0.00021799555468518113+0j))\n",
      "jack bin size 50: (np.float32(5.373278e-05), np.complex128(0.00032370950759011576+0j))\n",
      "bin size 50: (np.float32(5.373278e-05), np.complex128(0.0003237094628478737+0j))\n",
      "jack bin size 100: (np.float32(5.373278e-05), np.complex128(0.0004199393296214506+0j))\n",
      "bin size 100: (np.float32(5.373278e-05), np.complex128(0.0004199393648709958+0j))\n",
      "jack bin size 200: (np.float32(5.373278e-05), np.complex128(0.0005036596611058869+0j))\n",
      "bin size 200: (np.float32(5.373278e-05), np.complex128(0.000503659668841656+0j))\n",
      "jack bin size 500: (np.float32(5.373278e-05), np.complex128(0.0005784239446133714+0j))\n",
      "bin size 500: (np.float32(5.373278e-05), np.complex128(0.0005784239376503554+0j))\n",
      "jack bin size 1000: (np.float32(5.373278e-05), np.complex128(0.0006140068329544131+0j))\n",
      "bin size 1000: (np.float32(5.373278e-05), np.complex128(0.0006140068234480168+0j))\n",
      "jack bin size 2000: (np.float32(5.373278e-05), np.complex128(0.0006239889989956282+0j))\n",
      "bin size 2000: (np.float32(5.373278e-05), np.complex128(0.0006239890520061765+0j))\n",
      "jack bin size 5000: (np.float32(5.373278e-05), np.complex128(0.00048806871768619897+0j))\n",
      "bin size 5000: (np.float32(5.373278e-05), np.complex128(0.0004880687844549651+0j))\n",
      "jack bin size 10000: (np.float32(5.373278e-05), np.complex128(0.0004042980872327462+0j))\n",
      "bin size 10000: (np.float32(5.373278e-05), np.complex128(0.00040429803387572366+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"1+1scalar/config/c_64x64_0.01_0.1_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(64,64), m2=0.01, lamda=0.1)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "122f3bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3c380e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 4.576784704113379e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.000633756), np.complex128(0.00020171402463989763+0j)) <f>: (np.float32(-0.00023897823), np.complex128(0.0008797154081499675+0j))\n",
      "Epoch 200: <Test loss>: 4.100240039406344e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00030929167), np.complex128(0.00015881950325082335+0j)) <f>: (np.float32(8.548694e-05), np.complex128(0.0007503953682148595+0j))\n",
      "Epoch 300: <Test loss>: 2.737485738180112e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477303), np.complex128(0.00013510915153054686+0j)) <f>: (np.float32(5.2098184e-09), np.complex128(0.0008131582187896272+0j))\n",
      "Epoch 400: <Test loss>: 3.0550945666618645e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004531053), np.complex128(0.0001478109636925362+0j)) <f>: (np.float32(-5.832658e-05), np.complex128(0.0007653922640742305+0j))\n",
      "Epoch 500: <Test loss>: 2.9288108635228127e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00028856815), np.complex128(0.00016354192287929047+0j)) <f>: (np.float32(0.00010621115), np.complex128(0.0007464303384168664+0j))\n",
      "Epoch 600: <Test loss>: 2.909021895902697e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00030061722), np.complex128(0.0001521527696383468+0j)) <f>: (np.float32(9.4161456e-05), np.complex128(0.0007541117511274241+0j))\n",
      "Epoch 700: <Test loss>: 3.190829011145979e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00029461598), np.complex128(0.00017163893109834987+0j)) <f>: (np.float32(0.00010016303), np.complex128(0.0007261505208364037+0j))\n",
      "Epoch 800: <Test loss>: 3.3794032788136974e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00030944296), np.complex128(0.00018824148642098996+0j)) <f>: (np.float32(8.533533e-05), np.complex128(0.0007125612708141315+0j))\n",
      "Epoch 900: <Test loss>: 3.53192699549254e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003190723), np.complex128(0.00019862918578722295+0j)) <f>: (np.float32(7.57065e-05), np.complex128(0.0006976394130326844+0j))\n",
      "Epoch 1000: <Test loss>: 3.5618097172118723e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00031768176), np.complex128(0.00019196221431101672+0j)) <f>: (np.float32(7.70968e-05), np.complex128(0.0007060698110552674+0j))\n",
      "Epoch 1100: <Test loss>: 4.1222381696570665e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.000342891), np.complex128(0.0002315832233370319+0j)) <f>: (np.float32(5.1887717e-05), np.complex128(0.0006640517857949104+0j))\n",
      "Epoch 1200: <Test loss>: 4.032008655485697e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00032695525), np.complex128(0.00022419481116869763+0j)) <f>: (np.float32(6.782375e-05), np.complex128(0.000670169450631369+0j))\n",
      "Epoch 1300: <Test loss>: 3.989849574281834e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00033857132), np.complex128(0.00021545019515920374+0j)) <f>: (np.float32(5.620774e-05), np.complex128(0.0006799178040073785+0j))\n",
      "Epoch 1400: <Test loss>: 4.07141815230716e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00032444307), np.complex128(0.00022226665606072464+0j)) <f>: (np.float32(7.033562e-05), np.complex128(0.0006752983635162821+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dd4e2eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77e301c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.00010507788101676852 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00014799382), np.complex128(0.0004125275738315579+0j)) <f>: (np.float32(0.00024678494), np.complex128(0.0006174436882852818+0j))\n",
      "Epoch 400: <Test loss>: 5.974874511593953e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00022990559), np.complex128(0.0002546848183427221+0j)) <f>: (np.float32(0.00016487349), np.complex128(0.0006716998341529844+0j))\n",
      "Epoch 600: <Test loss>: 5.504344153450802e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00018489553), np.complex128(0.0002268789447823486+0j)) <f>: (np.float32(0.00020988352), np.complex128(0.0007340496898626948+0j))\n",
      "Epoch 800: <Test loss>: 4.3700576497940347e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00029139538), np.complex128(0.00019527800298763145+0j)) <f>: (np.float32(0.00010338353), np.complex128(0.0007368325052090003+0j))\n",
      "Epoch 1000: <Test loss>: 4.9571681302040815e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00043332425), np.complex128(0.00021507237241097553+0j)) <f>: (np.float32(-3.8544822e-05), np.complex128(0.0007083347177741433+0j))\n",
      "Epoch 1200: <Test loss>: 4.58066861028783e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0002589886), np.complex128(0.00020542937680963355+0j)) <f>: (np.float32(0.00013579021), np.complex128(0.0007389006510506826+0j))\n",
      "Epoch 1400: <Test loss>: 4.943723979522474e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003091026), np.complex128(0.00022411861548728924+0j)) <f>: (np.float32(8.567672e-05), np.complex128(0.0007117419412684481+0j))\n",
      "Epoch 1600: <Test loss>: 5.2897914429195225e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0002718016), np.complex128(0.0002240427528151024+0j)) <f>: (np.float32(0.0001229772), np.complex128(0.0007171845805552309+0j))\n",
      "Epoch 1800: <Test loss>: 5.85197085456457e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00032963973), np.complex128(0.00023694884234814334+0j)) <f>: (np.float32(6.513877e-05), np.complex128(0.0006782693132151847+0j))\n",
      "Epoch 2000: <Test loss>: 5.886451981496066e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00029628308), np.complex128(0.00024758452258136167+0j)) <f>: (np.float32(9.849611e-05), np.complex128(0.000696717326355778+0j))\n",
      "Epoch 2200: <Test loss>: 5.4525698942597955e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00034927818), np.complex128(0.0002330451179620806+0j)) <f>: (np.float32(4.550041e-05), np.complex128(0.0007016628621626876+0j))\n",
      "Epoch 2400: <Test loss>: 5.7615110563347116e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00035277216), np.complex128(0.00024205718794922986+0j)) <f>: (np.float32(4.200601e-05), np.complex128(0.0006833564889443293+0j))\n",
      "Epoch 2600: <Test loss>: 5.844030965818092e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00037589023), np.complex128(0.0002445998243588835+0j)) <f>: (np.float32(1.8889144e-05), np.complex128(0.0006732601567893759+0j))\n",
      "Epoch 2800: <Test loss>: 6.204176315804943e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003763624), np.complex128(0.0002571977376437474+0j)) <f>: (np.float32(1.8416757e-05), np.complex128(0.0006588499914632225+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "465fcad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d94d923a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.0002248995442641899 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0006408646), np.complex128(0.00040486243100011063+0j)) <f>: (np.float32(-0.0002460856), np.complex128(0.00073187694741028+0j))\n",
      "Epoch 800: <Test loss>: 0.00012711537419818342 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00033741788), np.complex128(0.0003083228990952169+0j)) <f>: (np.float32(5.73614e-05), np.complex128(0.0006061823947278337+0j))\n",
      "Epoch 1200: <Test loss>: 0.00024323511752299964 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0009658083), np.complex128(0.0005581646216106128+0j)) <f>: (np.float32(-0.0005710301), np.complex128(0.0006263879988246355+0j))\n",
      "Epoch 1600: <Test loss>: 8.31397992442362e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00026018097), np.complex128(0.0002603895676020334+0j)) <f>: (np.float32(0.00013459858), np.complex128(0.000677952542157575+0j))\n",
      "Epoch 2000: <Test loss>: 7.714099047007039e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00035239963), np.complex128(0.00021698891976894+0j)) <f>: (np.float32(4.2379506e-05), np.complex128(0.0007019429070604341+0j))\n",
      "Epoch 2400: <Test loss>: 7.621387339895591e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0002809155), np.complex128(0.0002502659921212402+0j)) <f>: (np.float32(0.00011386314), np.complex128(0.000705746062661587+0j))\n",
      "Epoch 2800: <Test loss>: 7.756008562864736e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003481426), np.complex128(0.0002528324149038621+0j)) <f>: (np.float32(4.6636134e-05), np.complex128(0.0006629532359457432+0j))\n",
      "Epoch 3200: <Test loss>: 7.826813089195639e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00032324862), np.complex128(0.00027356653635043714+0j)) <f>: (np.float32(7.1529634e-05), np.complex128(0.0006632160277942968+0j))\n",
      "Epoch 3600: <Test loss>: 8.319657354149967e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00034702744), np.complex128(0.00029644902714592554+0j)) <f>: (np.float32(4.7751328e-05), np.complex128(0.0006211582317183368+0j))\n",
      "Epoch 4000: <Test loss>: 9.143796341959387e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00031387855), np.complex128(0.00031315530691228933+0j)) <f>: (np.float32(8.090029e-05), np.complex128(0.0005934526890791435+0j))\n",
      "Epoch 4400: <Test loss>: 8.510675252182409e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00024176728), np.complex128(0.0003065535735283441+0j)) <f>: (np.float32(0.00015301116), np.complex128(0.0006275533408090975+0j))\n",
      "Epoch 4800: <Test loss>: 9.403667354490608e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00030841722), np.complex128(0.00032702780506386765+0j)) <f>: (np.float32(8.636166e-05), np.complex128(0.000576812186562828+0j))\n",
      "Epoch 5200: <Test loss>: 9.772768680704758e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004252412), np.complex128(0.00034196098515884766+0j)) <f>: (np.float32(-3.0462019e-05), np.complex128(0.0005540197034026517+0j))\n",
      "Epoch 5600: <Test loss>: 9.850271453615278e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039246195), np.complex128(0.0003435745892706881+0j)) <f>: (np.float32(2.3170403e-06), np.complex128(0.0005539413669476761+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dd1b93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f84478d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.00024070516519714147 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00058130955), np.complex128(0.0007089548760901723+0j)) <f>: (np.float32(-0.00018653115), np.complex128(0.00034880426123149485+0j))\n",
      "Epoch 1600: <Test loss>: 0.00023891107412055135 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00026603634), np.complex128(0.0006967453625607166+0j)) <f>: (np.float32(0.00012874314), np.complex128(0.00030949331550591224+0j))\n",
      "Epoch 2400: <Test loss>: 0.00017767635290510952 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(-1.1696726e-05), np.complex128(0.00039636806349182505+0j)) <f>: (np.float32(0.0004064757), np.complex128(0.0006076253078272537+0j))\n",
      "Epoch 3200: <Test loss>: 0.00019925863307435066 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00028853124), np.complex128(0.0005932577676814594+0j)) <f>: (np.float32(0.00010624743), np.complex128(0.00038042393083052904+0j))\n",
      "Epoch 4000: <Test loss>: 0.00017114056390710175 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00015383206), np.complex128(0.0005431760351241986+0j)) <f>: (np.float32(0.00024094713), np.complex128(0.00042537995373443646+0j))\n",
      "Epoch 4800: <Test loss>: 0.00013426033547148108 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(-0.00010863036), np.complex128(0.00037131384512916843+0j)) <f>: (np.float32(0.0005034095), np.complex128(0.0006320078624476138+0j))\n",
      "Epoch 5600: <Test loss>: 0.00015734472253825516 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0002873041), np.complex128(0.00047402486250375714+0j)) <f>: (np.float32(0.000107474734), np.complex128(0.00048001820389750155+0j))\n",
      "Epoch 6400: <Test loss>: 0.00015143801283556968 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003272767), np.complex128(0.0004651892398312866+0j)) <f>: (np.float32(6.7501715e-05), np.complex128(0.00046582322595880123+0j))\n",
      "Epoch 7200: <Test loss>: 0.0002005349815590307 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004960076), np.complex128(0.0005625568546671161+0j)) <f>: (np.float32(-0.00010122873), np.complex128(0.00039823161481081617+0j))\n",
      "Epoch 8000: <Test loss>: 0.00023280321329366416 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005406193), np.complex128(0.0006319921317262908+0j)) <f>: (np.float32(-0.00014584039), np.complex128(0.0003622296390015974+0j))\n",
      "Epoch 8800: <Test loss>: 0.0002070854970952496 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005149376), np.complex128(0.0005723338785513434+0j)) <f>: (np.float32(-0.000120159006), np.complex128(0.00039504347966913127+0j))\n",
      "Epoch 9600: <Test loss>: 0.00021190001280047 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00051713997), np.complex128(0.0005885200296288008+0j)) <f>: (np.float32(-0.00012236112), np.complex128(0.00038441686997280484+0j))\n",
      "Epoch 10400: <Test loss>: 0.0002494906366337091 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00056424143), np.complex128(0.0006681118976544486+0j)) <f>: (np.float32(-0.00016946257), np.complex128(0.00035189103470917093+0j))\n",
      "Epoch 11200: <Test loss>: 0.0002285188384121284 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005650145), np.complex128(0.0006117522752524147+0j)) <f>: (np.float32(-0.00017023583), np.complex128(0.00037531886774891214+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bf06261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9d73b94b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0006040514563210309 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0011557353), np.complex128(0.0009823168179179183+0j)) <f>: (np.float32(-0.0007609556), np.complex128(0.0005229537488511551+0j))\n",
      "Epoch 3200: <Test loss>: 0.00030090779182501137 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004978873), np.complex128(0.0006819197285867171+0j)) <f>: (np.float32(-0.00010310868), np.complex128(0.00027069495026021414+0j))\n",
      "Epoch 4800: <Test loss>: 0.0003842975711449981 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00017260031), np.complex128(0.0008582163328063145+0j)) <f>: (np.float32(0.00022217851), np.complex128(0.00030457813111091017+0j))\n",
      "Epoch 6400: <Test loss>: 0.0002663851482793689 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(4.940828e-05), np.complex128(0.00048509199582745614+0j)) <f>: (np.float32(0.00034537038), np.complex128(0.00043615067713578674+0j))\n",
      "Epoch 8000: <Test loss>: 0.0003069161612074822 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003147631), np.complex128(0.0007585378559790342+0j)) <f>: (np.float32(8.001567e-05), np.complex128(0.00021013733432333594+0j))\n",
      "Epoch 9600: <Test loss>: 0.00024519211729057133 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00019850771), np.complex128(0.0006276042753624137+0j)) <f>: (np.float32(0.000196271), np.complex128(0.00026493883829287337+0j))\n",
      "Epoch 11200: <Test loss>: 0.00024940771982073784 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00020716971), np.complex128(0.0006596209870990344+0j)) <f>: (np.float32(0.0001876092), np.complex128(0.00022927209176668757+0j))\n",
      "Epoch 12800: <Test loss>: 0.00023074913769960403 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0002711084), np.complex128(0.0006219603716451553+0j)) <f>: (np.float32(0.00012367059), np.complex128(0.00024855654478391957+0j))\n",
      "Epoch 14400: <Test loss>: 0.00031842858879826963 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003228447), np.complex128(0.0008334986757635772+0j)) <f>: (np.float32(7.193433e-05), np.complex128(0.0001650845643117755+0j))\n",
      "Epoch 16000: <Test loss>: 0.00027331282035447657 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00030189793), np.complex128(0.0007390123518581416+0j)) <f>: (np.float32(9.2880684e-05), np.complex128(0.00014757976014728448+0j))\n",
      "Epoch 17600: <Test loss>: 0.00026536767836660147 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00030715406), np.complex128(0.0007331248122513601+0j)) <f>: (np.float32(8.7624976e-05), np.complex128(0.00014812911435977799+0j))\n",
      "Epoch 19200: <Test loss>: 0.0002865036076400429 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00032224716), np.complex128(0.0007752881563929713+0j)) <f>: (np.float32(7.253173e-05), np.complex128(0.00012969173372790146+0j))\n",
      "Epoch 20800: <Test loss>: 0.00027769579901359975 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0002609436), np.complex128(0.0007442741512800375+0j)) <f>: (np.float32(0.00013383536), np.complex128(0.0001454792014080374+0j))\n",
      "Epoch 22400: <Test loss>: 0.00029006932163611054 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0002747418), np.complex128(0.0007825252396564814+0j)) <f>: (np.float32(0.0001200371), np.complex128(0.00013908880201912437+0j))\n",
      "Epoch 24000: <Test loss>: 0.0002844578411895782 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0002791467), np.complex128(0.0007738937040640786+0j)) <f>: (np.float32(0.00011563232), np.complex128(0.00013066046548219874+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882807e9",
   "metadata": {},
   "source": [
    "### l4, w64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67155f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "420c9f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 7.625293801538646e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005897171), np.complex128(0.0002779838243864671+0j)) <f>: (np.float32(-0.00019493829), np.complex128(0.000714960840119172+0j))\n",
      "Epoch 200: <Test loss>: 5.3130181186133996e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00041044716), np.complex128(0.00022701210089822504+0j)) <f>: (np.float32(-1.5668042e-05), np.complex128(0.0007219166733087051+0j))\n",
      "Epoch 300: <Test loss>: 5.0858296162914485e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039467704), np.complex128(0.00020481951006430873+0j)) <f>: (np.float32(1.0151044e-07), np.complex128(0.0007141090976759238+0j))\n",
      "Epoch 400: <Test loss>: 5.112753569846973e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00045389862), np.complex128(0.00023716656694871305+0j)) <f>: (np.float32(-5.9120284e-05), np.complex128(0.0006761214354513126+0j))\n",
      "Epoch 500: <Test loss>: 5.436694118543528e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00041389858), np.complex128(0.00024177424111398115+0j)) <f>: (np.float32(-1.9120618e-05), np.complex128(0.0006567357317731456+0j))\n",
      "Epoch 600: <Test loss>: 5.655913264490664e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00042063813), np.complex128(0.0002583914964151156+0j)) <f>: (np.float32(-2.58598e-05), np.complex128(0.0006357626841388952+0j))\n",
      "Epoch 700: <Test loss>: 5.913430140935816e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004027364), np.complex128(0.00026923310320791136+0j)) <f>: (np.float32(-7.9571955e-06), np.complex128(0.0006161854208704236+0j))\n",
      "Epoch 800: <Test loss>: 6.259234214667231e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00041366118), np.complex128(0.00028399276964087875+0j)) <f>: (np.float32(-1.8883004e-05), np.complex128(0.0005984701548779093+0j))\n",
      "Epoch 900: <Test loss>: 6.048961222404614e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004220651), np.complex128(0.0002809265158742833+0j)) <f>: (np.float32(-2.7285272e-05), np.complex128(0.0006070268426833716+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbaaee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "77b84ea8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 9.268506983062252e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00044994365), np.complex128(0.00026421844420115593+0j)) <f>: (np.float32(-5.5164703e-05), np.complex128(0.0007210447600211797+0j))\n",
      "Epoch 400: <Test loss>: 7.965499389683828e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00069196726), np.complex128(0.0002966870177362642+0j)) <f>: (np.float32(-0.0002971896), np.complex128(0.0006596705261851364+0j))\n",
      "Epoch 600: <Test loss>: 8.562606672057882e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004782409), np.complex128(0.0003254887950176558+0j)) <f>: (np.float32(-8.3461375e-05), np.complex128(0.0005843417470683561+0j))\n",
      "Epoch 800: <Test loss>: 8.078209066297859e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00050102384), np.complex128(0.00032404312269897147+0j)) <f>: (np.float32(-0.000106245905), np.complex128(0.0005899975756872626+0j))\n",
      "Epoch 1000: <Test loss>: 8.974078082246706e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003904637), np.complex128(0.00034172033049473656+0j)) <f>: (np.float32(4.316083e-06), np.complex128(0.0005598974113086063+0j))\n",
      "Epoch 1200: <Test loss>: 8.845463889883831e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00043464062), np.complex128(0.00035441603748766404+0j)) <f>: (np.float32(-3.986151e-05), np.complex128(0.0005520836195036892+0j))\n",
      "Epoch 1400: <Test loss>: 8.99124497664161e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00044255427), np.complex128(0.00036099430165028055+0j)) <f>: (np.float32(-4.7775888e-05), np.complex128(0.0005460499364213906+0j))\n",
      "Epoch 1600: <Test loss>: 9.260433580493554e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004564464), np.complex128(0.0003634743957566096+0j)) <f>: (np.float32(-6.166755e-05), np.complex128(0.0005385954139937864+0j))\n",
      "Epoch 1800: <Test loss>: 9.509681694908068e-05 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00044788053), np.complex128(0.00037414927593247966+0j)) <f>: (np.float32(-5.310212e-05), np.complex128(0.0005264393088004355+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "60fec04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac6e0f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.00011193117825314403 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005914355), np.complex128(0.00042982638800614356+0j)) <f>: (np.float32(-0.00019665675), np.complex128(0.0006720911358458946+0j))\n",
      "Epoch 1200: <Test loss>: 0.00010801247844938189 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00044121753), np.complex128(0.0003587288874887813+0j)) <f>: (np.float32(-4.6438206e-05), np.complex128(0.0005278561203199182+0j))\n",
      "Epoch 1600: <Test loss>: 0.0001189841641462408 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00047323227), np.complex128(0.0004447219856318337+0j)) <f>: (np.float32(-7.845335e-05), np.complex128(0.0004628672458172867+0j))\n",
      "Epoch 2000: <Test loss>: 0.00012103419430786744 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00044688332), np.complex128(0.00043048213073613317+0j)) <f>: (np.float32(-5.2103784e-05), np.complex128(0.0004462295659505635+0j))\n",
      "Epoch 2400: <Test loss>: 0.00012453878298401833 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00045569843), np.complex128(0.0004485642777453104+0j)) <f>: (np.float32(-6.0919276e-05), np.complex128(0.00043443856572469364+0j))\n",
      "Epoch 2800: <Test loss>: 0.00012545670324470848 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00045535457), np.complex128(0.0004634808073792126+0j)) <f>: (np.float32(-6.057561e-05), np.complex128(0.0004246369308732352+0j))\n",
      "Epoch 3200: <Test loss>: 0.0001353741972707212 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00040570128), np.complex128(0.0004888946122708217+0j)) <f>: (np.float32(-1.0922326e-05), np.complex128(0.00039201950221609143+0j))\n",
      "Epoch 3600: <Test loss>: 0.00013574576587416232 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00044330314), np.complex128(0.0004989936084995477+0j)) <f>: (np.float32(-4.8523638e-05), np.complex128(0.0003925441344582799+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "34ae0ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "15c8d5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.00028865449712611735 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005621541), np.complex128(0.00047856450764233594+0j)) <f>: (np.float32(-0.00016737482), np.complex128(0.0005856009025078052+0j))\n",
      "Epoch 1600: <Test loss>: 0.00017362303333356977 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0006264718), np.complex128(0.0005246065211904833+0j)) <f>: (np.float32(-0.00023169373), np.complex128(0.0003451093812000936+0j))\n",
      "Epoch 2400: <Test loss>: 0.0001636623201193288 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00048560684), np.complex128(0.0004862494724512568+0j)) <f>: (np.float32(-9.082769e-05), np.complex128(0.00036989281549287925+0j))\n",
      "Epoch 3200: <Test loss>: 0.00017525989096611738 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.000515913), np.complex128(0.0005424256543449592+0j)) <f>: (np.float32(-0.000121133664), np.complex128(0.0003013531746638703+0j))\n",
      "Epoch 4000: <Test loss>: 0.00018046134209726006 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00054435723), np.complex128(0.0005656926597882709+0j)) <f>: (np.float32(-0.0001495787), np.complex128(0.0002792541435636293+0j))\n",
      "Epoch 4800: <Test loss>: 0.00018985096539836377 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00052574807), np.complex128(0.0005948507569367273+0j)) <f>: (np.float32(-0.00013096914), np.complex128(0.0002660624748367311+0j))\n",
      "Epoch 5600: <Test loss>: 0.000189775470062159 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00053436443), np.complex128(0.0005939244838581785+0j)) <f>: (np.float32(-0.00013958522), np.complex128(0.0002604169219309469+0j))\n",
      "Epoch 6400: <Test loss>: 0.00019695318769663572 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005439752), np.complex128(0.0006038865974392629+0j)) <f>: (np.float32(-0.00014919686), np.complex128(0.0002510923624333223+0j))\n",
      "Epoch 7200: <Test loss>: 0.00019713237998075783 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005582836), np.complex128(0.0006141545086312276+0j)) <f>: (np.float32(-0.00016350496), np.complex128(0.0002518452487105144+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "617e467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b61486d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00032038590870797634 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.00027966024936176836 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0005801644), np.complex128(0.0006403412754988141+0j)) <f>: (np.float32(-0.00018538568), np.complex128(0.00027072990037089556+0j))\n",
      "Epoch 3200: <Test loss>: 0.00028032291447743773 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0004224315), np.complex128(0.0007657169639228295+0j)) <f>: (np.float32(-2.7652019e-05), np.complex128(0.0002417288884295216+0j))\n",
      "Epoch 4800: <Test loss>: 0.0002248897944809869 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00047898156), np.complex128(0.0006502185827314756+0j)) <f>: (np.float32(-8.420303e-05), np.complex128(0.00028014280245772434+0j))\n",
      "Epoch 6400: <Test loss>: 0.0002261918707517907 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.0003440453), np.complex128(0.0006604901094521315+0j)) <f>: (np.float32(5.073344e-05), np.complex128(0.00022685114644113965+0j))\n",
      "Epoch 8000: <Test loss>: 0.0002284552901983261 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00035911), np.complex128(0.0006598204754803284+0j)) <f>: (np.float32(3.566904e-05), np.complex128(0.00019467354367776142+0j))\n",
      "Epoch 9600: <Test loss>: 0.00023495181812904775 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00035976156), np.complex128(0.0007058555434075691+0j)) <f>: (np.float32(3.5017263e-05), np.complex128(0.00015620579315874048+0j))\n",
      "Epoch 11200: <Test loss>: 0.00024581982870586216 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00032774964), np.complex128(0.000730248754323019+0j)) <f>: (np.float32(6.702955e-05), np.complex128(0.00015156588052930937+0j))\n",
      "Epoch 12800: <Test loss>: 0.00024161631881725043 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00036603957), np.complex128(0.0007196969289833049+0j)) <f>: (np.float32(2.8739283e-05), np.complex128(0.00015695022734473786+0j))\n",
      "Epoch 14400: <Test loss>: 0.00024050202046055347 <O>: (np.float32(0.00039477917), np.complex128(0.0008224727721530231+0j)) <O-f>: (np.float32(0.00037544477), np.complex128(0.0007108579127882909+0j)) <f>: (np.float32(1.933434e-05), np.complex128(0.0001607325460831705+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar2d/cv_64x64_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxgpu_3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
