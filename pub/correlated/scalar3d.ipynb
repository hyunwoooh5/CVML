{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "694fad71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.4'\n",
    "os.environ['OMP_NUM_THREADS']='1'\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    ")\n",
    "#os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "#os.environ['XLA_FLAGS']='--xla_gpu_deterministic_reductions --xla_gpu_autotune_level=1'\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models import scalar\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "#import sympy\n",
    "#import optuna\n",
    "from util import *\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as special\n",
    "\n",
    "matplotlib.style.use('default') # 'classic'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['axes.prop_cycle'] = plt.cycler(color='krbg')\n",
    "matplotlib.rcParams['legend.numpoints'] = 1\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "@jax.jit\n",
    "def arcsinh(x: any) -> any:\n",
    "    return jnp.arcsinh(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def sinh(x: any) -> any:\n",
    "    return jnp.sinh(x)\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# jax.config.update(\"jax_platform_name\",\"cpu\")\n",
    "num_devices = jax.local_device_count()\n",
    "jax.devices()\n",
    "\n",
    "# jax.config.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab801701",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    volume: int\n",
    "    features: Sequence[int]\n",
    "    kernel_init: Callable = nn.initializers.variance_scaling(\n",
    "        2, \"fan_in\", \"truncated_normal\")  # for ReLU / CELU\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features:\n",
    "            x = nn.Dense(feat, use_bias=False,\n",
    "                         kernel_init=self.kernel_init,\n",
    "                         bias_init=self.bias_init)(x)\n",
    "            x = arcsinh(x)\n",
    "        x = nn.Dense(1, use_bias=False,\n",
    "                     kernel_init=self.bias_init)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CV_MLP(nn.Module):\n",
    "    volume: int\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = MLP(self.volume, self.features)(x)\n",
    "        y = self.param('bias', nn.initializers.zeros, (1,))\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9add1526",
   "metadata": {},
   "source": [
    "# 8x8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253ee1d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.009076067), np.complex128(5.9482865379369954e-05+0j))\n",
      "bin size 1: (np.float32(0.009076067), np.complex128(5.9482611472871364e-05+0j))\n",
      "jack bin size 2: (np.float32(0.009076067), np.complex128(8.337282840750121e-05+0j))\n",
      "bin size 2: (np.float32(0.009076067), np.complex128(8.337335318090729e-05+0j))\n",
      "jack bin size 4: (np.float32(0.009076067), np.complex128(0.0001164335992975979+0j))\n",
      "bin size 4: (np.float32(0.009076067), np.complex128(0.00011643306295285131+0j))\n",
      "jack bin size 5: (np.float32(0.009076067), np.complex128(0.0001294412032391845+0j))\n",
      "bin size 5: (np.float32(0.009076067), np.complex128(0.00012944200305915393+0j))\n",
      "jack bin size 10: (np.float32(0.009076067), np.complex128(0.00017844013374461085+0j))\n",
      "bin size 10: (np.float32(0.009076067), np.complex128(0.00017844008297521492+0j))\n",
      "jack bin size 20: (np.float32(0.009076067), np.complex128(0.00024152857028353098+0j))\n",
      "bin size 20: (np.float32(0.009076067), np.complex128(0.00024152851996479813+0j))\n",
      "jack bin size 50: (np.float32(0.009076067), np.complex128(0.00034216192868359016+0j))\n",
      "bin size 50: (np.float32(0.009076067), np.complex128(0.00034216149137114685+0j))\n",
      "jack bin size 100: (np.float32(0.009076067), np.complex128(0.00041757698219041804+0j))\n",
      "bin size 100: (np.float32(0.009076067), np.complex128(0.00041757739097004185+0j))\n",
      "jack bin size 200: (np.float32(0.009076067), np.complex128(0.00048781821988622966+0j))\n",
      "bin size 200: (np.float32(0.009076067), np.complex128(0.00048781785589793345+0j))\n",
      "jack bin size 500: (np.float32(0.009076067), np.complex128(0.0005325398044939067+0j))\n",
      "bin size 500: (np.float32(0.009076067), np.complex128(0.0005325400763094185+0j))\n",
      "jack bin size 1000: (np.float32(0.009076067), np.complex128(0.000554385789168444+0j))\n",
      "bin size 1000: (np.float32(0.009076067), np.complex128(0.0005543863562615469+0j))\n",
      "jack bin size 2000: (np.float32(0.009076067), np.complex128(0.0005876646100659855+0j))\n",
      "bin size 2000: (np.float32(0.009076067), np.complex128(0.0005876653428588595+0j))\n",
      "jack bin size 5000: (np.float32(0.009076067), np.complex128(0.000522211423723561+0j))\n",
      "bin size 5000: (np.float32(0.009076067), np.complex128(0.0005222107526974603+0j))\n",
      "jack bin size 10000: (np.float32(0.009076067), np.complex128(0.00045314402086660266+0j))\n",
      "bin size 10000: (np.float32(0.009076067), np.complex128(0.0004531443119049072+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_8x8x8_0.1_0.5_sweep1.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(8,8,8), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 4))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298afeed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0085690245), np.complex128(5.9957810803187504e-05+0j))\n",
      "bin size 1: (np.float32(0.0085690245), np.complex128(5.995668668234702e-05+0j))\n",
      "jack bin size 2: (np.float32(0.0085690245), np.complex128(7.945475026811272e-05+0j))\n",
      "bin size 2: (np.float32(0.0085690245), np.complex128(7.945407287540145e-05+0j))\n",
      "jack bin size 4: (np.float32(0.0085690245), np.complex128(0.00010318584595187719+0j))\n",
      "bin size 4: (np.float32(0.0085690245), np.complex128(0.00010318542933368745+0j))\n",
      "jack bin size 5: (np.float32(0.0085690245), np.complex128(0.00011118464666754193+0j))\n",
      "bin size 5: (np.float32(0.0085690245), np.complex128(0.00011118416403528099+0j))\n",
      "jack bin size 10: (np.float32(0.0085690245), np.complex128(0.00013570921850915876+0j))\n",
      "bin size 10: (np.float32(0.0085690245), np.complex128(0.00013570942545910973+0j))\n",
      "jack bin size 20: (np.float32(0.0085690245), np.complex128(0.0001532437686790061+0j))\n",
      "bin size 20: (np.float32(0.0085690245), np.complex128(0.0001532431958101853+0j))\n",
      "jack bin size 50: (np.float32(0.0085690245), np.complex128(0.0001656156654567244+0j))\n",
      "bin size 50: (np.float32(0.0085690245), np.complex128(0.0001656152307989129+0j))\n",
      "jack bin size 100: (np.float32(0.0085690245), np.complex128(0.0001720531247144205+0j))\n",
      "bin size 100: (np.float32(0.0085690245), np.complex128(0.00017205263488961786+0j))\n",
      "jack bin size 200: (np.float32(0.0085690245), np.complex128(0.0001767836007565493+0j))\n",
      "bin size 200: (np.float32(0.0085690245), np.complex128(0.0001767841678291432+0j))\n",
      "jack bin size 500: (np.float32(0.0085690245), np.complex128(0.00017083526942142382+0j))\n",
      "bin size 500: (np.float32(0.0085690245), np.complex128(0.00017083498677455367+0j))\n",
      "jack bin size 1000: (np.float32(0.0085690245), np.complex128(0.00017385923454250364+0j))\n",
      "bin size 1000: (np.float32(0.0085690245), np.complex128(0.00017385901297034353+0j))\n",
      "jack bin size 2000: (np.float32(0.0085690245), np.complex128(0.00016803918515506666+0j))\n",
      "bin size 2000: (np.float32(0.0085690245), np.complex128(0.00016803859866091182+0j))\n",
      "jack bin size 5000: (np.float32(0.0085690245), np.complex128(0.00016225536553818196+0j))\n",
      "bin size 5000: (np.float32(0.0085690245), np.complex128(0.00016225491234518117+0j))\n",
      "jack bin size 10000: (np.float32(0.0085690245), np.complex128(0.0002003146946663037+0j))\n",
      "bin size 10000: (np.float32(0.0085690245), np.complex128(0.00020031452489395935+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_8x8x8_0.1_0.5_sweep10.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(8,8,8), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 4))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d16b11",
   "metadata": {},
   "source": [
    "## I'm using sweep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5f77ef04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.008446614), np.complex128(0.0007794027481581457+0j)),\n",
       " (np.float32(0.008466811), np.complex128(0.00099826198688424+0j)),\n",
       " (np.float32(0.008459467), np.complex128(0.0012388249701372812+0j)),\n",
       " (np.float32(0.008560926), np.complex128(0.0014180525031406432+0j)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td= 4\n",
    "bs=25\n",
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:2]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:4]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:8]), Bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c006fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.008446614), np.complex128(0.0009978273064289202+0j)),\n",
       " (np.float32(0.008466811), np.complex128(0.001244613373898483+0j)),\n",
       " (np.float32(0.008459467), np.complex128(0.0013917367032263428+0j)),\n",
       " (np.float32(0.008560926), np.complex128(0.0016133927998384493+0j)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td= 4\n",
    "bs=50\n",
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:2]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:4]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:8]), Bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a98145a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((np.float32(0.008446614), np.complex128(0.0012475678083060902+0j)),\n",
       " (np.float32(0.008466811), np.complex128(0.001396513995132409+0j)),\n",
       " (np.float32(0.008459467), np.complex128(0.001579639771827701+0j)),\n",
       " (np.float32(0.0088221785), np.complex128(0.00191851448092213+0j)))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td= 4\n",
    "bs=100\n",
    "jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:2]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:4]), Bs=bs), jackknife(jax.vmap(lambda x: model.observe(x, td))(conf[:10000:8]), Bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2d6a77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j, -k) for i, j, k in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1, 2)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1, 2)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ac1d74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0004116384079679847 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 5.093511845188914e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008454044), np.complex128(7.721216827082695e-05+0j)) <f>: (np.float32(1.0718157e-05), np.complex128(0.0006762014845251419+0j))\n",
      "Epoch 200: <Test loss>: 3.1222068628267152e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008482894), np.complex128(6.22244856665856e-05+0j)) <f>: (np.float32(-1.8126695e-05), np.complex128(0.0006425260697135328+0j))\n",
      "Epoch 300: <Test loss>: 2.626277591843973e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008498714), np.complex128(5.3638774521706316e-05+0j)) <f>: (np.float32(-3.3949153e-05), np.complex128(0.0006496274120753038+0j))\n",
      "Epoch 400: <Test loss>: 2.1169325918890536e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008489807), np.complex128(5.323768870071513e-05+0j)) <f>: (np.float32(-2.5041238e-05), np.complex128(0.0006500635590100504+0j))\n",
      "Epoch 500: <Test loss>: 2.1113876300660195e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008483), np.complex128(5.805486531029688e-05+0j)) <f>: (np.float32(-1.8224217e-05), np.complex128(0.0006557571286740665+0j))\n",
      "Epoch 600: <Test loss>: 1.8488523210180574e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008487851), np.complex128(3.997767775988358e-05+0j)) <f>: (np.float32(-2.3081142e-05), np.complex128(0.000660725245677714+0j))\n",
      "Epoch 700: <Test loss>: 2.2260533114604186e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0084938025), np.complex128(5.182158284363037e-05+0j)) <f>: (np.float32(-2.9034223e-05), np.complex128(0.0006486257837671915+0j))\n",
      "Epoch 800: <Test loss>: 2.3798174879630096e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008535076), np.complex128(4.9394266734522465e-05+0j)) <f>: (np.float32(-7.030566e-05), np.complex128(0.0006554881840837051+0j))\n",
      "Epoch 900: <Test loss>: 1.8222114022137248e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008511536), np.complex128(4.831420682594262e-05+0j)) <f>: (np.float32(-4.6770292e-05), np.complex128(0.0006580022451306329+0j))\n",
      "Epoch 1000: <Test loss>: 1.9419073851167923e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008518719), np.complex128(4.5103090083574835e-05+0j)) <f>: (np.float32(-5.3954114e-05), np.complex128(0.000651661939843191+0j))\n",
      "Epoch 1100: <Test loss>: 2.0536829197226325e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008560584), np.complex128(4.864227641071217e-05+0j)) <f>: (np.float32(-9.5817886e-05), np.complex128(0.0006592317151766175+0j))\n",
      "Epoch 1200: <Test loss>: 2.131812607331085e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008534001), np.complex128(4.68545719063259e-05+0j)) <f>: (np.float32(-6.92337e-05), np.complex128(0.0006529808466515364+0j))\n",
      "Epoch 1300: <Test loss>: 2.1738401301263366e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008514874), np.complex128(4.4903355909760167e-05+0j)) <f>: (np.float32(-5.0114253e-05), np.complex128(0.0006511369270190351+0j))\n",
      "Epoch 1400: <Test loss>: 4.373025149106979e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008522118), np.complex128(7.747220090011622e-05+0j)) <f>: (np.float32(-5.7349218e-05), np.complex128(0.0006157050629971201+0j))\n",
      "Epoch 1500: <Test loss>: 2.3896861875982722e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0085114455), np.complex128(4.58824030211347e-05+0j)) <f>: (np.float32(-4.6680925e-05), np.complex128(0.0006691631919093188+0j))\n",
      "Epoch 1600: <Test loss>: 2.36583605328633e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008523005), np.complex128(5.220597063079782e-05+0j)) <f>: (np.float32(-5.8227004e-05), np.complex128(0.0006453602001951219+0j))\n",
      "Epoch 1700: <Test loss>: 2.283061576235923e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008536973), np.complex128(4.789262508024376e-05+0j)) <f>: (np.float32(-7.220554e-05), np.complex128(0.0006538754679964556+0j))\n",
      "Epoch 1800: <Test loss>: 2.278685769852018e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0085231), np.complex128(4.5496580122798154e-05+0j)) <f>: (np.float32(-5.8328915e-05), np.complex128(0.0006660195848578305+0j))\n",
      "Epoch 1900: <Test loss>: 2.2551485017174855e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00854506), np.complex128(4.7994113604908434e-05+0j)) <f>: (np.float32(-8.028645e-05), np.complex128(0.0006495012491530801+0j))\n",
      "Epoch 2000: <Test loss>: 2.616948904687888e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008532074), np.complex128(5.560108686126176e-05+0j)) <f>: (np.float32(-6.7296154e-05), np.complex128(0.0006414046215159882+0j))\n",
      "Epoch 2100: <Test loss>: 2.403851794952061e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008538828), np.complex128(4.985020789597401e-05+0j)) <f>: (np.float32(-7.406023e-05), np.complex128(0.0006504499765677111+0j))\n",
      "Epoch 2200: <Test loss>: 2.7420269361755345e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008550588), np.complex128(5.4065046147275425e-05+0j)) <f>: (np.float32(-8.581857e-05), np.complex128(0.0006465998190935729+0j))\n",
      "Epoch 2300: <Test loss>: 2.6327027171646478e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008536966), np.complex128(5.1880148858273775e-05+0j)) <f>: (np.float32(-7.22013e-05), np.complex128(0.000646923694347909+0j))\n",
      "Epoch 2400: <Test loss>: 2.537700083848904e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008554167), np.complex128(4.999037306308511e-05+0j)) <f>: (np.float32(-8.9390735e-05), np.complex128(0.0006573910939211677+0j))\n",
      "Epoch 2500: <Test loss>: 3.0740250167582417e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008516834), np.complex128(5.802845847190659e-05+0j)) <f>: (np.float32(-5.2070463e-05), np.complex128(0.0006421559537501464+0j))\n",
      "Epoch 2600: <Test loss>: 2.682003696463653e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008548454), np.complex128(5.4595874739622114e-05+0j)) <f>: (np.float32(-8.368023e-05), np.complex128(0.0006475461361557431+0j))\n",
      "Epoch 2700: <Test loss>: 2.777490635708091e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008515243), np.complex128(5.017092749149636e-05+0j)) <f>: (np.float32(-5.0475686e-05), np.complex128(0.0006579365313109125+0j))\n",
      "Epoch 2800: <Test loss>: 2.984143065987155e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00853615), np.complex128(6.175167996669954e-05+0j)) <f>: (np.float32(-7.137974e-05), np.complex128(0.0006395129388465665+0j))\n",
      "Epoch 2900: <Test loss>: 2.815860852933838e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008548512), np.complex128(5.849536118690102e-05+0j)) <f>: (np.float32(-8.373592e-05), np.complex128(0.0006470076761020691+0j))\n",
      "Epoch 3000: <Test loss>: 3.0099974992481293e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008538925), np.complex128(6.0342539552498985e-05+0j)) <f>: (np.float32(-7.416269e-05), np.complex128(0.0006458129658757821+0j))\n",
      "Epoch 3100: <Test loss>: 2.782150204438949e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008533652), np.complex128(5.61588812717635e-05+0j)) <f>: (np.float32(-6.887737e-05), np.complex128(0.0006506954519367437+0j))\n",
      "Epoch 3200: <Test loss>: 2.9630573408212513e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008505388), np.complex128(6.140517990727506e-05+0j)) <f>: (np.float32(-4.0611827e-05), np.complex128(0.0006437726659480547+0j))\n",
      "Epoch 3300: <Test loss>: 3.127711579509196e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008550378), np.complex128(6.151240491222058e-05+0j)) <f>: (np.float32(-8.5609296e-05), np.complex128(0.0006452868747360516+0j))\n",
      "Epoch 3400: <Test loss>: 3.5749906146520516e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008543638), np.complex128(6.423958393215315e-05+0j)) <f>: (np.float32(-7.887046e-05), np.complex128(0.0006415713164177499+0j))\n",
      "Epoch 3500: <Test loss>: 3.1750885227666004e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0085443305), np.complex128(6.388123825899047e-05+0j)) <f>: (np.float32(-7.955557e-05), np.complex128(0.0006395663471826713+0j))\n",
      "Epoch 3600: <Test loss>: 3.3397250263078604e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008527289), np.complex128(6.206347967235878e-05+0j)) <f>: (np.float32(-6.251971e-05), np.complex128(0.000639730124289349+0j))\n",
      "Epoch 3700: <Test loss>: 3.111255409748992e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008527499), np.complex128(5.9598767420549235e-05+0j)) <f>: (np.float32(-6.272697e-05), np.complex128(0.0006528487212784886+0j))\n",
      "Epoch 3800: <Test loss>: 3.652826308098156e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008487259), np.complex128(6.439304171735803e-05+0j)) <f>: (np.float32(-2.2491995e-05), np.complex128(0.0006364575633812087+0j))\n",
      "Epoch 3900: <Test loss>: 3.3884539334394503e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00855327), np.complex128(6.262918305187246e-05+0j)) <f>: (np.float32(-8.8500026e-05), np.complex128(0.000640404325244762+0j))\n",
      "Epoch 4000: <Test loss>: 3.353297415742418e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008532571), np.complex128(5.878287500575965e-05+0j)) <f>: (np.float32(-6.780463e-05), np.complex128(0.0006513520192609963+0j))\n",
      "Epoch 4100: <Test loss>: 3.6015142086398555e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008523604), np.complex128(5.857350735089282e-05+0j)) <f>: (np.float32(-5.8835023e-05), np.complex128(0.0006497415232352237+0j))\n",
      "Epoch 4200: <Test loss>: 3.754230192498653e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008532739), np.complex128(6.993510804314821e-05+0j)) <f>: (np.float32(-6.796283e-05), np.complex128(0.0006333836028297715+0j))\n",
      "Epoch 4300: <Test loss>: 3.511198428896023e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00854075), np.complex128(5.909141994271773e-05+0j)) <f>: (np.float32(-7.598223e-05), np.complex128(0.0006470008256266542+0j))\n",
      "Epoch 4400: <Test loss>: 3.746006314031547e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008531778), np.complex128(6.711837332899005e-05+0j)) <f>: (np.float32(-6.700091e-05), np.complex128(0.0006377852235748063+0j))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     79\u001b[39m     \u001b[38;5;66;03m# save()\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_train//nt):  \u001b[38;5;66;03m# one epoch\u001b[39;00m\n\u001b[32m     82\u001b[39m     g_params, opt_state = train_batch_shard(\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         \u001b[43mconf_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnt\u001b[49m\u001b[43m*\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnt\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m, g_params, opt_state)\n\u001b[32m     85\u001b[39m epochs+=\u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/array.py:382\u001b[39m, in \u001b[36mArrayImpl.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    377\u001b[39m       out = lax.squeeze(out, dimensions=dims)\n\u001b[32m    379\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayImpl(\n\u001b[32m    380\u001b[39m         out.aval, sharding, [out], committed=\u001b[38;5;28;01mFalse\u001b[39;00m, _skip_checks=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mindexing\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrewriting_take\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/numpy/indexing.py:618\u001b[39m, in \u001b[36mrewriting_take\u001b[39m\u001b[34m(arr, idx, indices_are_sorted, unique_indices, mode, fill_value, out_sharding)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrewriting_take\u001b[39m(arr, idx, indices_are_sorted=\u001b[38;5;28;01mFalse\u001b[39;00m, unique_indices=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    610\u001b[39m                    mode=\u001b[38;5;28;01mNone\u001b[39;00m, fill_value=\u001b[38;5;28;01mNone\u001b[39;00m, out_sharding=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    611\u001b[39m   \u001b[38;5;66;03m# Computes arr[idx].\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    615\u001b[39m   \u001b[38;5;66;03m# For simplicity of generated primitives, we call lax.dynamic_slice in the\u001b[39;00m\n\u001b[32m    616\u001b[39m   \u001b[38;5;66;03m# simplest cases: i.e. non-dynamic arrays indexed with integers and slices.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m (result := \u001b[43m_attempt_rewriting_take_via_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    619\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[32m    621\u001b[39m   \u001b[38;5;66;03m# TODO(mattjj,dougalm): expand dynamic shape indexing support\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/numpy/indexing.py:594\u001b[39m, in \u001b[36m_attempt_rewriting_take_via_slice\u001b[39m\u001b[34m(arr, idx, mode)\u001b[39m\n\u001b[32m    592\u001b[39m   int_start_indices = [\u001b[38;5;28mint\u001b[39m(i) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m start_indices]  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    593\u001b[39m   int_limit_indices = [i + s \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(int_start_indices, slice_sizes)]\n\u001b[32m--> \u001b[39m\u001b[32m594\u001b[39m   arr = \u001b[43mlax\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    595\u001b[39m \u001b[43m      \u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mint_start_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mint_limit_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    597\u001b[39m   \u001b[38;5;66;03m# We must be careful with dtypes because dynamic_slice requires all\u001b[39;00m\n\u001b[32m    598\u001b[39m   \u001b[38;5;66;03m# start indices to have matching types.\u001b[39;00m\n\u001b[32m    599\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(start_indices) > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/lax/slicing.py:108\u001b[39m, in \u001b[36mslice\u001b[39m\u001b[34m(operand, start_indices, limit_indices, strides)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mslice\u001b[39m(operand: ArrayLike, start_indices: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[32m     58\u001b[39m           limit_indices: Sequence[\u001b[38;5;28mint\u001b[39m],\n\u001b[32m     59\u001b[39m           strides: Sequence[\u001b[38;5;28mint\u001b[39m] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m) -> Array:\n\u001b[32m     60\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Wraps XLA's `Slice\u001b[39;00m\n\u001b[32m     61\u001b[39m \u001b[33;03m  <https://www.tensorflow.org/xla/operation_semantics#slice>`_\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03m  operator.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m \u001b[33;03m    - :func:`jax.lax.dynamic_slice`\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33;03m  \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mslice_p\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlimit_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstrides\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/core.py:502\u001b[39m, in \u001b[36mPrimitive.bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **params):\n\u001b[32m    501\u001b[39m   args = args \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.skip_canonicalization \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmap\u001b[39m(canonicalize_value, args)\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_true_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/core.py:520\u001b[39m, in \u001b[36mPrimitive._true_bind\u001b[39m\u001b[34m(self, *args, **params)\u001b[39m\n\u001b[32m    518\u001b[39m trace_ctx.set_trace(eval_trace)\n\u001b[32m    519\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbind_with_trace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_trace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    521\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    522\u001b[39m   trace_ctx.set_trace(prev_trace)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/core.py:525\u001b[39m, in \u001b[36mPrimitive.bind_with_trace\u001b[39m\u001b[34m(self, trace, args, params)\u001b[39m\n\u001b[32m    524\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind_with_trace\u001b[39m(\u001b[38;5;28mself\u001b[39m, trace, args, params):\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrace\u001b[49m\u001b[43m.\u001b[49m\u001b[43mprocess_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/core.py:1029\u001b[39m, in \u001b[36mEvalTrace.process_primitive\u001b[39m\u001b[34m(self, primitive, args, params)\u001b[39m\n\u001b[32m   1027\u001b[39m       \u001b[38;5;28;01mreturn\u001b[39;00m primitive.bind_with_trace(arg._trace, args, params)\n\u001b[32m   1028\u001b[39m check_eval_args(args)\n\u001b[32m-> \u001b[39m\u001b[32m1029\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprimitive\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimpl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/lax/slicing.py:1414\u001b[39m, in \u001b[36m_slice_impl\u001b[39m\u001b[34m(x, start_indices, limit_indices, strides)\u001b[39m\n\u001b[32m   1410\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch.apply_primitive(\n\u001b[32m   1411\u001b[39m     slice_p, x, start_indices=start_indices,\n\u001b[32m   1412\u001b[39m     limit_indices=limit_indices, strides=strides)\n\u001b[32m   1413\u001b[39m slice_sizes = \u001b[38;5;28mtuple\u001b[39m(np.array(limit_indices) - np.array(start_indices))\n\u001b[32m-> \u001b[39m\u001b[32m1414\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_primitive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_slice_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mstart_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1415\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mslice_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mslice_sizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/jaxgpu_3.13.2/lib/python3.13/site-packages/jax/_src/dispatch.py:88\u001b[39m, in \u001b[36mapply_primitive\u001b[39m\u001b[34m(prim, *args, **params)\u001b[39m\n\u001b[32m     86\u001b[39m prev = lib.jax_jit.swap_thread_local_state_disable_jit(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m   outs = \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     90\u001b[39m   lib.jax_jit.swap_thread_local_state_disable_jit(prev)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aaeab4b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0004116384079679847 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 6.204214514582418e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008411732), np.complex128(8.208354609561244e-05+0j)) <f>: (np.float32(5.3042404e-05), np.complex128(0.0006650910917178045+0j))\n",
      "Epoch 400: <Test loss>: 4.669863301387522e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008420012), np.complex128(6.434317358643002e-05+0j)) <f>: (np.float32(4.4762834e-05), np.complex128(0.0006591482408650807+0j))\n",
      "Epoch 600: <Test loss>: 4.8386987145931926e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008468689), np.complex128(7.180135515107576e-05+0j)) <f>: (np.float32(-3.9231927e-06), np.complex128(0.0006459440763635833+0j))\n",
      "Epoch 800: <Test loss>: 5.683576091541909e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008491711), np.complex128(7.521102864179627e-05+0j)) <f>: (np.float32(-2.6937689e-05), np.complex128(0.0006383154376858513+0j))\n",
      "Epoch 1000: <Test loss>: 3.3819242162280716e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008474106), np.complex128(7.043162479248815e-05+0j)) <f>: (np.float32(-9.334773e-06), np.complex128(0.0006523715983519088+0j))\n",
      "Epoch 1200: <Test loss>: 3.377068878762657e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008472163), np.complex128(6.562363368726765e-05+0j)) <f>: (np.float32(-7.3929814e-06), np.complex128(0.0006503722744160147+0j))\n",
      "Epoch 1400: <Test loss>: 3.2358620956074446e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008481288), np.complex128(6.538667780415311e-05+0j)) <f>: (np.float32(-1.6521371e-05), np.complex128(0.0006598011926606421+0j))\n",
      "Epoch 1600: <Test loss>: 3.3303995223832317e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008482952), np.complex128(6.335040570224828e-05+0j)) <f>: (np.float32(-1.8185627e-05), np.complex128(0.0006567517162157803+0j))\n",
      "Epoch 1800: <Test loss>: 3.446411938057281e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008496233), np.complex128(7.132245617531431e-05+0j)) <f>: (np.float32(-3.146021e-05), np.complex128(0.0006458110629659447+0j))\n",
      "Epoch 2000: <Test loss>: 4.15780050389003e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008480538), np.complex128(7.569256790495718e-05+0j)) <f>: (np.float32(-1.5768768e-05), np.complex128(0.0006365340603566746+0j))\n",
      "Epoch 2200: <Test loss>: 4.612343673215946e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00850519), np.complex128(5.958375821920625e-05+0j)) <f>: (np.float32(-4.0418505e-05), np.complex128(0.000694166666009643+0j))\n",
      "Epoch 2400: <Test loss>: 3.859000571537763e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008510147), np.complex128(7.21080566441288e-05+0j)) <f>: (np.float32(-4.537929e-05), np.complex128(0.000646566771892729+0j))\n",
      "Epoch 2600: <Test loss>: 3.7187903672020184e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008497035), np.complex128(6.969321648764275e-05+0j)) <f>: (np.float32(-3.226385e-05), np.complex128(0.0006391514494077766+0j))\n",
      "Epoch 2800: <Test loss>: 3.729798436324927e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008495), np.complex128(6.973496157220209e-05+0j)) <f>: (np.float32(-3.023491e-05), np.complex128(0.0006408716164705149+0j))\n",
      "Epoch 3000: <Test loss>: 3.919650225725491e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00850458), np.complex128(7.586491603469438e-05+0j)) <f>: (np.float32(-3.9808117e-05), np.complex128(0.0006358043578643356+0j))\n",
      "Epoch 3200: <Test loss>: 3.935698259738274e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008496998), np.complex128(7.17281248374974e-05+0j)) <f>: (np.float32(-3.2221382e-05), np.complex128(0.0006431121025131434+0j))\n",
      "Epoch 3400: <Test loss>: 4.187291779089719e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008512589), np.complex128(6.959599365253039e-05+0j)) <f>: (np.float32(-4.7820555e-05), np.complex128(0.0006410292408353847+0j))\n",
      "Epoch 3600: <Test loss>: 4.434190941537963e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008471387), np.complex128(7.902279490996365e-05+0j)) <f>: (np.float32(-6.6163907e-06), np.complex128(0.0006313377844635153+0j))\n",
      "Epoch 3800: <Test loss>: 4.371780050860252e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00851727), np.complex128(7.365015099003555e-05+0j)) <f>: (np.float32(-5.2498915e-05), np.complex128(0.0006390182457191542+0j))\n",
      "Epoch 4000: <Test loss>: 4.201678621029714e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008484451), np.complex128(6.192919766816182e-05+0j)) <f>: (np.float32(-1.9686448e-05), np.complex128(0.0006552324330015501+0j))\n",
      "Epoch 4200: <Test loss>: 4.386699856695486e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008497299), np.complex128(7.705197497767655e-05+0j)) <f>: (np.float32(-3.253013e-05), np.complex128(0.0006302129110282632+0j))\n",
      "Epoch 4400: <Test loss>: 4.467042344913352e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008498666), np.complex128(6.826606582470972e-05+0j)) <f>: (np.float32(-3.3893368e-05), np.complex128(0.0006401836511339443+0j))\n",
      "Epoch 4600: <Test loss>: 4.5191668505140115e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008500375), np.complex128(6.98263726035192e-05+0j)) <f>: (np.float32(-3.5612975e-05), np.complex128(0.0006585761627376116+0j))\n",
      "Epoch 4800: <Test loss>: 4.169544354226673e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008496913), np.complex128(7.005926498125166e-05+0j)) <f>: (np.float32(-3.2143915e-05), np.complex128(0.0006455883590846336+0j))\n",
      "Epoch 5000: <Test loss>: 4.524161795416148e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008493591), np.complex128(7.538692886989673e-05+0j)) <f>: (np.float32(-2.882624e-05), np.complex128(0.0006333700287395976+0j))\n",
      "Epoch 5200: <Test loss>: 4.494135282584466e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008486234), np.complex128(6.398319458232352e-05+0j)) <f>: (np.float32(-2.146469e-05), np.complex128(0.0006410521391837622+0j))\n",
      "Epoch 5400: <Test loss>: 4.742429155157879e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008488487), np.complex128(7.51350470377446e-05+0j)) <f>: (np.float32(-2.3717777e-05), np.complex128(0.0006336543234693145+0j))\n",
      "Epoch 5600: <Test loss>: 4.713585440185852e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008483312), np.complex128(6.816900156541715e-05+0j)) <f>: (np.float32(-1.8541285e-05), np.complex128(0.0006568662079576677+0j))\n",
      "Epoch 5800: <Test loss>: 4.593446647049859e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00848579), np.complex128(6.909432318904731e-05+0j)) <f>: (np.float32(-2.1027183e-05), np.complex128(0.0006432116881279706+0j))\n",
      "Epoch 6000: <Test loss>: 4.494741915550549e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00850641), np.complex128(7.008493047768443e-05+0j)) <f>: (np.float32(-4.1638505e-05), np.complex128(0.0006469720282577806+0j))\n",
      "Epoch 6200: <Test loss>: 4.790668299392564e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008486164), np.complex128(7.321514580119159e-05+0j)) <f>: (np.float32(-2.1400258e-05), np.complex128(0.000642169781561632+0j))\n",
      "Epoch 6400: <Test loss>: 4.763091055792756e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008490962), np.complex128(7.072079572866357e-05+0j)) <f>: (np.float32(-2.6191457e-05), np.complex128(0.0006456623822773109+0j))\n",
      "Epoch 6600: <Test loss>: 4.758623163070297e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008495562), np.complex128(7.373807335331732e-05+0j)) <f>: (np.float32(-3.079002e-05), np.complex128(0.000641161810220728+0j))\n",
      "Epoch 6800: <Test loss>: 4.953725692757871e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008476439), np.complex128(7.198703157846617e-05+0j)) <f>: (np.float32(-1.1667883e-05), np.complex128(0.0006391079362028266+0j))\n",
      "Epoch 7000: <Test loss>: 5.119409252074547e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008481485), np.complex128(7.022469920524607e-05+0j)) <f>: (np.float32(-1.6714941e-05), np.complex128(0.0006415388400898573+0j))\n",
      "Epoch 7200: <Test loss>: 4.9669010877551045e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008487756), np.complex128(7.171765090460037e-05+0j)) <f>: (np.float32(-2.298298e-05), np.complex128(0.0006408987012205348+0j))\n",
      "Epoch 7400: <Test loss>: 5.139021141076228e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00848841), np.complex128(7.231729743833979e-05+0j)) <f>: (np.float32(-2.3640492e-05), np.complex128(0.0006386812403869395+0j))\n",
      "Epoch 7600: <Test loss>: 5.261284513835562e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008499323), np.complex128(7.107966073763606e-05+0j)) <f>: (np.float32(-3.4554592e-05), np.complex128(0.000647658724987793+0j))\n",
      "Epoch 7800: <Test loss>: 5.602978944807546e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008490837), np.complex128(7.860438468066056e-05+0j)) <f>: (np.float32(-2.6071124e-05), np.complex128(0.0006337759194079283+0j))\n",
      "Epoch 8000: <Test loss>: 5.257296834315639e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008478038), np.complex128(7.489298104883737e-05+0j)) <f>: (np.float32(-1.3267667e-05), np.complex128(0.0006425846793365267+0j))\n",
      "Epoch 8200: <Test loss>: 5.087751560495235e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008467723), np.complex128(6.950125252899772e-05+0j)) <f>: (np.float32(-2.962686e-06), np.complex128(0.0006513255053839276+0j))\n",
      "Epoch 8400: <Test loss>: 5.38928952664719e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008490848), np.complex128(7.342726474653176e-05+0j)) <f>: (np.float32(-2.607948e-05), np.complex128(0.0006432265308247029+0j))\n",
      "Epoch 8600: <Test loss>: 5.475455054693157e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008485053), np.complex128(7.26855897797987e-05+0j)) <f>: (np.float32(-2.028444e-05), np.complex128(0.0006410696459542668+0j))\n",
      "Epoch 8800: <Test loss>: 5.534918273042422e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00848127), np.complex128(7.508679241578294e-05+0j)) <f>: (np.float32(-1.6499622e-05), np.complex128(0.0006350477609129605+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbf29b35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0004116384079679847 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 9.714633961266372e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00852201), np.complex128(0.00011939324118909051+0j)) <f>: (np.float32(-5.725056e-05), np.complex128(0.0006589277570452468+0j))\n",
      "Epoch 800: <Test loss>: 8.504705874656793e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008502993), np.complex128(0.00011343431074824018+0j)) <f>: (np.float32(-3.8224793e-05), np.complex128(0.0006334640959158961+0j))\n",
      "Epoch 1200: <Test loss>: 6.822808245487977e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008518245), np.complex128(0.00010459784762392476+0j)) <f>: (np.float32(-5.3474716e-05), np.complex128(0.000640078864232228+0j))\n",
      "Epoch 1600: <Test loss>: 7.2507136792410165e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008470342), np.complex128(0.00011266402870361727+0j)) <f>: (np.float32(-5.572076e-06), np.complex128(0.0006171644679817981+0j))\n",
      "Epoch 2000: <Test loss>: 5.636372407025192e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008490419), np.complex128(9.248323379382144e-05+0j)) <f>: (np.float32(-2.5651358e-05), np.complex128(0.0006335494731372703+0j))\n",
      "Epoch 2400: <Test loss>: 6.374131771735847e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008498369), np.complex128(0.00010076346051506454+0j)) <f>: (np.float32(-3.3601948e-05), np.complex128(0.0006270219215218221+0j))\n",
      "Epoch 2800: <Test loss>: 5.222843810770428e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008482984), np.complex128(9.053982369198479e-05+0j)) <f>: (np.float32(-1.8215964e-05), np.complex128(0.0006347100578471388+0j))\n",
      "Epoch 3200: <Test loss>: 5.444413091026945e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008506484), np.complex128(0.00010172061623451727+0j)) <f>: (np.float32(-4.171931e-05), np.complex128(0.0006253386709099306+0j))\n",
      "Epoch 3600: <Test loss>: 5.486384452524362e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008468239), np.complex128(9.515341273533101e-05+0j)) <f>: (np.float32(-3.471538e-06), np.complex128(0.0006303419917455712+0j))\n",
      "Epoch 4000: <Test loss>: 5.399681867856998e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008475223), np.complex128(9.60648431171477e-05+0j)) <f>: (np.float32(-1.045391e-05), np.complex128(0.0006338832435227612+0j))\n",
      "Epoch 4400: <Test loss>: 6.467670573329087e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008476154), np.complex128(9.988137458869973e-05+0j)) <f>: (np.float32(-1.1387112e-05), np.complex128(0.0006263429632918155+0j))\n",
      "Epoch 4800: <Test loss>: 5.682233222614741e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008486423), np.complex128(0.0001003019731643159+0j)) <f>: (np.float32(-2.1656691e-05), np.complex128(0.0006308474680287292+0j))\n",
      "Epoch 5200: <Test loss>: 5.961014267086284e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00849522), np.complex128(9.127174624580084e-05+0j)) <f>: (np.float32(-3.0446194e-05), np.complex128(0.0006351236235851473+0j))\n",
      "Epoch 5600: <Test loss>: 5.712623988074483e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008480686), np.complex128(9.31118045570903e-05+0j)) <f>: (np.float32(-1.5914626e-05), np.complex128(0.0006319307311688687+0j))\n",
      "Epoch 6000: <Test loss>: 5.950517333985772e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008470124), np.complex128(8.726264822748859e-05+0j)) <f>: (np.float32(-5.3582808e-06), np.complex128(0.0006361827197703511+0j))\n",
      "Epoch 6400: <Test loss>: 6.129189841885818e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008493183), np.complex128(9.742022236404444e-05+0j)) <f>: (np.float32(-2.8415769e-05), np.complex128(0.0006254836092092174+0j))\n",
      "Epoch 6800: <Test loss>: 6.06376897849259e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008485277), np.complex128(9.469160823294279e-05+0j)) <f>: (np.float32(-2.0507654e-05), np.complex128(0.0006250267205572426+0j))\n",
      "Epoch 7200: <Test loss>: 6.4281766753993e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008480671), np.complex128(9.386172546645235e-05+0j)) <f>: (np.float32(-1.5896638e-05), np.complex128(0.0006335456038872675+0j))\n",
      "Epoch 7600: <Test loss>: 5.90611170991906e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008495703), np.complex128(9.326192828568412e-05+0j)) <f>: (np.float32(-3.093281e-05), np.complex128(0.0006359059098193283+0j))\n",
      "Epoch 8000: <Test loss>: 6.281925379880704e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008489188), np.complex128(9.170258881937536e-05+0j)) <f>: (np.float32(-2.4411616e-05), np.complex128(0.0006335386899815246+0j))\n",
      "Epoch 8400: <Test loss>: 6.286234111030353e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008482585), np.complex128(9.556007249638777e-05+0j)) <f>: (np.float32(-1.781547e-05), np.complex128(0.0006326230731980656+0j))\n",
      "Epoch 8800: <Test loss>: 6.366495199472411e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00847755), np.complex128(8.809326044275005e-05+0j)) <f>: (np.float32(-1.278293e-05), np.complex128(0.0006341828883918337+0j))\n",
      "Epoch 9200: <Test loss>: 6.47836759526399e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008478349), np.complex128(9.603804380360344e-05+0j)) <f>: (np.float32(-1.3575253e-05), np.complex128(0.0006292761085152804+0j))\n",
      "Epoch 9600: <Test loss>: 6.4198061409115326e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008488727), np.complex128(9.418647288779733e-05+0j)) <f>: (np.float32(-2.3957833e-05), np.complex128(0.0006348660964538107+0j))\n",
      "Epoch 10000: <Test loss>: 6.740270237060031e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008487811), np.complex128(9.978092473565467e-05+0j)) <f>: (np.float32(-2.3042585e-05), np.complex128(0.0006303640654996858+0j))\n",
      "Epoch 10400: <Test loss>: 6.837375622126274e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008478017), np.complex128(9.1942093808793e-05+0j)) <f>: (np.float32(-1.3248025e-05), np.complex128(0.000641113793462496+0j))\n",
      "Epoch 10800: <Test loss>: 6.950597253307933e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008480623), np.complex128(9.950913370932807e-05+0j)) <f>: (np.float32(-1.5851336e-05), np.complex128(0.0006269227164889625+0j))\n",
      "Epoch 11200: <Test loss>: 7.45526494938531e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008493481), np.complex128(0.00010183530619617935+0j)) <f>: (np.float32(-2.8705246e-05), np.complex128(0.0006200262540863582+0j))\n",
      "Epoch 11600: <Test loss>: 7.440921763191e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008481866), np.complex128(0.00010143972295613482+0j)) <f>: (np.float32(-1.7095574e-05), np.complex128(0.0006277029095223221+0j))\n",
      "Epoch 12000: <Test loss>: 7.2534171522420365e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008478707), np.complex128(9.924368571579302e-05+0j)) <f>: (np.float32(-1.3937245e-05), np.complex128(0.0006274717059770704+0j))\n",
      "Epoch 12400: <Test loss>: 7.332056611630833e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008483193), np.complex128(9.960847353163461e-05+0j)) <f>: (np.float32(-1.8426928e-05), np.complex128(0.000627285284243327+0j))\n",
      "Epoch 12800: <Test loss>: 7.4121162469964474e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008482815), np.complex128(9.942071183221398e-05+0j)) <f>: (np.float32(-1.8050514e-05), np.complex128(0.0006320241006115602+0j))\n",
      "Epoch 13200: <Test loss>: 7.622890734637622e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008484501), np.complex128(9.99257203167036e-05+0j)) <f>: (np.float32(-1.9729885e-05), np.complex128(0.0006301487829667408+0j))\n",
      "Epoch 13600: <Test loss>: 7.627123522979673e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008462116), np.complex128(0.00010001088346072104+0j)) <f>: (np.float32(2.6496323e-06), np.complex128(0.0006316801179432749+0j))\n",
      "Epoch 14000: <Test loss>: 7.694729902141262e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008487675), np.complex128(9.95603378415378e-05+0j)) <f>: (np.float32(-2.2899572e-05), np.complex128(0.0006267636332265505+0j))\n",
      "Epoch 14400: <Test loss>: 7.940359864733182e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008475321), np.complex128(9.919302867016157e-05+0j)) <f>: (np.float32(-1.0556551e-05), np.complex128(0.0006297709285033486+0j))\n",
      "Epoch 14800: <Test loss>: 7.776364327582996e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008468661), np.complex128(9.78632594181607e-05+0j)) <f>: (np.float32(-3.8873022e-06), np.complex128(0.0006407719039950319+0j))\n",
      "Epoch 15200: <Test loss>: 8.557823093724437e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0084425695), np.complex128(0.00010766679717945661+0j)) <f>: (np.float32(2.2197928e-05), np.complex128(0.0006274879441410168+0j))\n",
      "Epoch 15600: <Test loss>: 8.083264219749253e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008474563), np.complex128(9.885067933282006e-05+0j)) <f>: (np.float32(-9.803843e-06), np.complex128(0.0006289652364781669+0j))\n",
      "Epoch 16000: <Test loss>: 8.169882676156703e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008478109), np.complex128(0.00010119718916855921+0j)) <f>: (np.float32(-1.334024e-05), np.complex128(0.0006280954163914627+0j))\n",
      "Epoch 16400: <Test loss>: 8.31420584290754e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008473971), np.complex128(9.633674514654815e-05+0j)) <f>: (np.float32(-9.201922e-06), np.complex128(0.0006357065482986901+0j))\n",
      "Epoch 16800: <Test loss>: 8.716137926967349e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008467215), np.complex128(0.0001035368564576745+0j)) <f>: (np.float32(-2.4454887e-06), np.complex128(0.000623593258576682+0j))\n",
      "Epoch 17200: <Test loss>: 8.836236702336464e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008490216), np.complex128(0.00010381591818533838+0j)) <f>: (np.float32(-2.544913e-05), np.complex128(0.0006271921050916193+0j))\n",
      "Epoch 17600: <Test loss>: 8.540365342923906e-06 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008490059), np.complex128(0.00010058550673009784+0j)) <f>: (np.float32(-2.5288564e-05), np.complex128(0.0006297612870935055+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f6b7036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0004116384079679847 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 1.4150861716188956e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008495023), np.complex128(0.00012304829490335185+0j)) <f>: (np.float32(-3.025377e-05), np.complex128(0.0006557016905674684+0j))\n",
      "Epoch 1600: <Test loss>: 1.2699520993919577e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008502849), np.complex128(0.00012946914877845524+0j)) <f>: (np.float32(-3.8081413e-05), np.complex128(0.0006477386472009663+0j))\n",
      "Epoch 2400: <Test loss>: 1.1274249118287116e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008565077), np.complex128(0.00012585021863594176+0j)) <f>: (np.float32(-0.00010031302), np.complex128(0.0006374731463614624+0j))\n",
      "Epoch 3200: <Test loss>: 1.0693399417505134e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008574231), np.complex128(0.00012102482779889495+0j)) <f>: (np.float32(-0.00010946259), np.complex128(0.0006337342456824879+0j))\n",
      "Epoch 4000: <Test loss>: 1.1313052709738258e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008588253), np.complex128(0.00012741372864631095+0j)) <f>: (np.float32(-0.000123491), np.complex128(0.0006419095903565229+0j))\n",
      "Epoch 4800: <Test loss>: 1.1490180440887343e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008535743), np.complex128(0.00012698635888318982+0j)) <f>: (np.float32(-7.0976595e-05), np.complex128(0.0006113939573300206+0j))\n",
      "Epoch 5600: <Test loss>: 1.1160333997395355e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008565984), np.complex128(0.0001204039717492588+0j)) <f>: (np.float32(-0.000101217694), np.complex128(0.00062112131526749+0j))\n",
      "Epoch 6400: <Test loss>: 1.1956326488871127e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00847373), np.complex128(0.00013173680678780456+0j)) <f>: (np.float32(-8.964051e-06), np.complex128(0.0006225898542193879+0j))\n",
      "Epoch 7200: <Test loss>: 1.0960256986436434e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008569537), np.complex128(0.0001195849355688419+0j)) <f>: (np.float32(-0.000104767176), np.complex128(0.0006400138481461147+0j))\n",
      "Epoch 8000: <Test loss>: 1.0422638297313824e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008541627), np.complex128(0.00011122441398124886+0j)) <f>: (np.float32(-7.686085e-05), np.complex128(0.000642942870398265+0j))\n",
      "Epoch 8800: <Test loss>: 1.082829294318799e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0085384725), np.complex128(0.0001120007456933967+0j)) <f>: (np.float32(-7.369957e-05), np.complex128(0.0006344018498837977+0j))\n",
      "Epoch 9600: <Test loss>: 1.1047667612729128e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008523751), np.complex128(0.00011453377240837116+0j)) <f>: (np.float32(-5.898133e-05), np.complex128(0.0006320260035213976+0j))\n",
      "Epoch 10400: <Test loss>: 1.1393949534976855e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008536192), np.complex128(0.00011820125846690391+0j)) <f>: (np.float32(-7.142095e-05), np.complex128(0.0006317515404925077+0j))\n",
      "Epoch 11200: <Test loss>: 1.1616104529821314e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008528028), np.complex128(0.00011576619989339484+0j)) <f>: (np.float32(-6.326473e-05), np.complex128(0.0006367207992420576+0j))\n",
      "Epoch 12000: <Test loss>: 1.18240632218658e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008533331), np.complex128(0.00011738540966046477+0j)) <f>: (np.float32(-6.856559e-05), np.complex128(0.0006369834642299554+0j))\n",
      "Epoch 12800: <Test loss>: 1.2205904567963444e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0085218595), np.complex128(0.00011982241078776626+0j)) <f>: (np.float32(-5.7092053e-05), np.complex128(0.0006368065570453992+0j))\n",
      "Epoch 13600: <Test loss>: 1.5664647435187362e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0085129365), np.complex128(0.00013199177291965157+0j)) <f>: (np.float32(-4.816521e-05), np.complex128(0.000616616303087953+0j))\n",
      "Epoch 14400: <Test loss>: 1.2904429240734316e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008500723), np.complex128(0.00012707796813428166+0j)) <f>: (np.float32(-3.5957815e-05), np.complex128(0.0006252760017459502+0j))\n",
      "Epoch 15200: <Test loss>: 1.2739669728034642e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008503014), np.complex128(0.0001233786241935534+0j)) <f>: (np.float32(-3.8241946e-05), np.complex128(0.0006354865084911514+0j))\n",
      "Epoch 16000: <Test loss>: 1.336221339443e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008508706), np.complex128(0.00012385620699001057+0j)) <f>: (np.float32(-4.3936954e-05), np.complex128(0.0006417734054424885+0j))\n",
      "Epoch 16800: <Test loss>: 1.3041801139479503e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008493949), np.complex128(0.0001279713684453884+0j)) <f>: (np.float32(-2.9182967e-05), np.complex128(0.0006322058285010379+0j))\n",
      "Epoch 17600: <Test loss>: 1.3278561709739733e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00849326), np.complex128(0.00013127631846472057+0j)) <f>: (np.float32(-2.848796e-05), np.complex128(0.0006332501454198374+0j))\n",
      "Epoch 18400: <Test loss>: 1.3414643944997806e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008488529), np.complex128(0.00013094716263558546+0j)) <f>: (np.float32(-2.3762208e-05), np.complex128(0.000631851823840942+0j))\n",
      "Epoch 19200: <Test loss>: 1.3793954167340416e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008481561), np.complex128(0.00013613418562966008+0j)) <f>: (np.float32(-1.6791544e-05), np.complex128(0.0006316096468489609+0j))\n",
      "Epoch 20000: <Test loss>: 1.3719761227548588e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00847132), np.complex128(0.0001349198912896292+0j)) <f>: (np.float32(-6.5416566e-06), np.complex128(0.0006420252872746406+0j))\n",
      "Epoch 20800: <Test loss>: 1.3978895367472433e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008479095), np.complex128(0.00013634937301711312+0j)) <f>: (np.float32(-1.4318015e-05), np.complex128(0.0006321170894722843+0j))\n",
      "Epoch 21600: <Test loss>: 1.4110735719441436e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008495301), np.complex128(0.0001378929817620989+0j)) <f>: (np.float32(-3.0528005e-05), np.complex128(0.0006363928644467348+0j))\n",
      "Epoch 22400: <Test loss>: 1.4328210454550572e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008473354), np.complex128(0.00013563662227990953+0j)) <f>: (np.float32(-8.587373e-06), np.complex128(0.0006313023269102106+0j))\n",
      "Epoch 23200: <Test loss>: 1.4405441106646322e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008464608), np.complex128(0.00014720182639598242+0j)) <f>: (np.float32(1.6462057e-07), np.complex128(0.0006232671632608687+0j))\n",
      "Epoch 24000: <Test loss>: 1.5230998542392626e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.0084663015), np.complex128(0.0001410326719908384+0j)) <f>: (np.float32(-1.5294058e-06), np.complex128(0.0006308083949467333+0j))\n",
      "Epoch 24800: <Test loss>: 1.5028873349365313e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008471713), np.complex128(0.00014251105850114512+0j)) <f>: (np.float32(-6.9362636e-06), np.complex128(0.0006290287936667381+0j))\n",
      "Epoch 25600: <Test loss>: 1.468491700507002e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008474486), np.complex128(0.00013862943958436089+0j)) <f>: (np.float32(-9.713173e-06), np.complex128(0.0006301116127945824+0j))\n",
      "Epoch 26400: <Test loss>: 1.511208938609343e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008471395), np.complex128(0.0001395671935522625+0j)) <f>: (np.float32(-6.62423e-06), np.complex128(0.0006266791440297672+0j))\n",
      "Epoch 27200: <Test loss>: 1.534614057163708e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008504595), np.complex128(0.00013871714787028593+0j)) <f>: (np.float32(-3.982994e-05), np.complex128(0.0006308460091311871+0j))\n",
      "Epoch 28000: <Test loss>: 1.5284998880815692e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008458044), np.complex128(0.00014106529103696892+0j)) <f>: (np.float32(6.724851e-06), np.complex128(0.0006263744247344615+0j))\n",
      "Epoch 28800: <Test loss>: 1.528291068098042e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008441969), np.complex128(0.0001405198377896422+0j)) <f>: (np.float32(2.2797714e-05), np.complex128(0.000625489127647746+0j))\n",
      "Epoch 29600: <Test loss>: 1.5665089449612424e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008444428), np.complex128(0.0001403664156839968+0j)) <f>: (np.float32(2.034772e-05), np.complex128(0.0006232574584206977+0j))\n",
      "Epoch 30400: <Test loss>: 1.5238675587170292e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008456793), np.complex128(0.00014314791485099793+0j)) <f>: (np.float32(7.973806e-06), np.complex128(0.0006320327271361568+0j))\n",
      "Epoch 31200: <Test loss>: 1.6191341273952276e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008429286), np.complex128(0.00014134693755049537+0j)) <f>: (np.float32(3.548374e-05), np.complex128(0.0006179315309372793+0j))\n",
      "Epoch 32000: <Test loss>: 1.606790829100646e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008448359), np.complex128(0.00014361460348863564+0j)) <f>: (np.float32(1.641528e-05), np.complex128(0.0006266276386034999+0j))\n",
      "Epoch 32800: <Test loss>: 1.633636020414997e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008441755), np.complex128(0.0001420495552928145+0j)) <f>: (np.float32(2.3010645e-05), np.complex128(0.0006322091268780895+0j))\n",
      "Epoch 33600: <Test loss>: 1.6926162061281502e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.00843029), np.complex128(0.00014539926333728196+0j)) <f>: (np.float32(3.4474033e-05), np.complex128(0.0006171490544121147+0j))\n",
      "Epoch 34400: <Test loss>: 1.643160248931963e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008436579), np.complex128(0.00014581669832529337+0j)) <f>: (np.float32(2.8194303e-05), np.complex128(0.0006160500605506521+0j))\n",
      "Epoch 35200: <Test loss>: 1.6787826098152436e-05 <O>: (np.float32(0.008464773), np.complex128(0.0006801483732493511+0j)) <O-f>: (np.float32(0.008429839), np.complex128(0.00014598646959795898+0j)) <f>: (np.float32(3.4932666e-05), np.complex128(0.0006143371245452961+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12968597",
   "metadata": {},
   "source": [
    "## Binary sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f52bd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From list: [1 2 3]\n",
      "From NumPy array: [4 5 6]\n",
      "From JAX array: [7 8 9]\n",
      "Same object?: True\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "# From a list\n",
    "list_data = [1, 2, 3]\n",
    "jnp_array_from_list = jnp.asarray(list_data)\n",
    "print(f\"From list: {jnp_array_from_list}\")\n",
    "\n",
    "# From a NumPy array\n",
    "np_array = np.array([4, 5, 6])\n",
    "jnp_array_from_numpy = jnp.asarray(np_array)\n",
    "print(f\"From NumPy array: {jnp_array_from_numpy}\")\n",
    "\n",
    "# From a JAX array (no copy if possible)\n",
    "jnp_array_original = jnp.array([7, 8, 9])\n",
    "jnp_array_from_jnp = jnp.asarray(jnp_array_original)\n",
    "print(f\"From JAX array: {jnp_array_from_jnp}\")\n",
    "print(f\"Same object?: {jnp_array_original is jnp_array_from_jnp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63cbc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.008224264), np.complex128(5.870871530352762e-05+0j))\n",
      "bin size 1: (np.float32(0.008224264), np.complex128(5.870830531971449e-05+0j))\n",
      "jack bin size 2: (np.float32(0.008224264), np.complex128(8.229531290924402e-05+0j))\n",
      "bin size 2: (np.float32(0.008224264), np.complex128(8.229573998158517e-05+0j))\n",
      "jack bin size 4: (np.float32(0.008224264), np.complex128(0.00011496463753419317+0j))\n",
      "bin size 4: (np.float32(0.008224264), np.complex128(0.0001149657020374948+0j))\n",
      "jack bin size 5: (np.float32(0.008224264), np.complex128(0.00012779536544911882+0j))\n",
      "bin size 5: (np.float32(0.008224264), np.complex128(0.00012779541623154963+0j))\n",
      "jack bin size 10: (np.float32(0.008224264), np.complex128(0.0001764079353449893+0j))\n",
      "bin size 10: (np.float32(0.008224264), np.complex128(0.0001764090276549945+0j))\n",
      "jack bin size 20: (np.float32(0.008224264), np.complex128(0.00023874286477887542+0j))\n",
      "bin size 20: (np.float32(0.008224264), np.complex128(0.00023874288787847656+0j))\n",
      "jack bin size 50: (np.float32(0.008224264), np.complex128(0.0003378392179253933+0j))\n",
      "bin size 50: (np.float32(0.008224264), np.complex128(0.0003378392011228964+0j))\n",
      "jack bin size 100: (np.float32(0.008224264), np.complex128(0.00040331249029005976+0j))\n",
      "bin size 100: (np.float32(0.008224264), np.complex128(0.000403312465284464+0j))\n",
      "jack bin size 200: (np.float32(0.008224264), np.complex128(0.0004503247163927209+0j))\n",
      "bin size 200: (np.float32(0.008224264), np.complex128(0.0004503249051454884+0j))\n",
      "jack bin size 500: (np.float32(0.008224264), np.complex128(0.00047332016067729325+0j))\n",
      "bin size 500: (np.float32(0.008224264), np.complex128(0.00047332053126447604+0j))\n",
      "jack bin size 1000: (np.float32(0.008224264), np.complex128(0.0005434472509661743+0j))\n",
      "bin size 1000: (np.float32(0.008224264), np.complex128(0.000543447764311575+0j))\n",
      "jack bin size 2000: (np.float32(0.008224264), np.complex128(0.0005533004805329256+0j))\n",
      "bin size 2000: (np.float32(0.008224264), np.complex128(0.0005533007705318076+0j))\n",
      "jack bin size 5000: (np.float32(0.008224264), np.complex128(0.00057447978997505+0j))\n",
      "bin size 5000: (np.float32(0.008224264), np.complex128(0.0005744804943855336+0j))\n",
      "jack bin size 10000: (np.float32(0.008224264), np.complex128(0.0006597807223442942+0j))\n",
      "bin size 10000: (np.float32(0.008224264), np.complex128(0.0006597806544353564+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_8x8x8_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = jnp.asarray(np.fromfile(aa, dtype=np.float64, count=m * n))\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(8,8,8), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 4))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c877ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.007873619), np.complex128(0.0005608745011450747+0j))\n",
      "bin size 1: (np.float32(0.007873619), np.complex128(0.0005608742164438501+0j))\n",
      "jack bin size 2: (np.float32(0.007873619), np.complex128(0.0006136921871104548+0j))\n",
      "bin size 2: (np.float32(0.007873619), np.complex128(0.0006136917171828357+0j))\n",
      "jack bin size 4: (np.float32(0.007873619), np.complex128(0.000629525587110019+0j))\n",
      "bin size 4: (np.float32(0.007873619), np.complex128(0.000629525305380926+0j))\n",
      "jack bin size 5: (np.float32(0.007873619), np.complex128(0.0006032722193356978+0j))\n",
      "bin size 5: (np.float32(0.007873619), np.complex128(0.0006032719727933547+0j))\n",
      "jack bin size 10: (np.float32(0.007873619), np.complex128(0.0006368842615923343+0j))\n",
      "bin size 10: (np.float32(0.007873619), np.complex128(0.0006368838111354016+0j))\n",
      "jack bin size 20: (np.float32(0.007873619), np.complex128(0.0006968027955736034+0j))\n",
      "bin size 20: (np.float32(0.007873619), np.complex128(0.0006968026448573385+0j))\n",
      "jack bin size 50: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j))\n",
      "bin size 50: (np.float32(0.007873619), np.complex128(0.0005754640784345733+0j))\n",
      "jack bin size 100: (np.float32(0.007873619), np.complex128(0.0007351268723141402+0j))\n",
      "bin size 100: (np.float32(0.007873619), np.complex128(0.0007351267461975415+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf[-100000::100])\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100:\n",
    "    jac, bi = jackknife(obs, Bs=i), bin(obs, Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21976e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05580316",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0003762592386920005 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 4.557084139378276e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085253315), np.complex128(8.516739384941342e-05+0j)) <f>: (np.float32(-0.0006517145), np.complex128(0.0005966450106224708+0j))\n",
      "Epoch 200: <Test loss>: 3.4119357223971747e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008468222), np.complex128(5.635140024577779e-05+0j)) <f>: (np.float32(-0.00059459964), np.complex128(0.0005750865010615611+0j))\n",
      "Epoch 300: <Test loss>: 2.89844547296525e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00851235), np.complex128(6.085506453084658e-05+0j)) <f>: (np.float32(-0.0006387453), np.complex128(0.0006108235283910771+0j))\n",
      "Epoch 400: <Test loss>: 1.7723225482768612e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008464606), np.complex128(3.6263598161101456e-05+0j)) <f>: (np.float32(-0.00059099204), np.complex128(0.0005852864149420004+0j))\n",
      "Epoch 500: <Test loss>: 1.8507236063669552e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008470254), np.complex128(3.277630180659443e-05+0j)) <f>: (np.float32(-0.00059664395), np.complex128(0.0005641205391108871+0j))\n",
      "Epoch 600: <Test loss>: 1.6145701238201582e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008444706), np.complex128(3.278126522975381e-05+0j)) <f>: (np.float32(-0.0005710868), np.complex128(0.0005783221455188546+0j))\n",
      "Epoch 700: <Test loss>: 1.5186349173745839e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008492589), np.complex128(3.91326866821628e-05+0j)) <f>: (np.float32(-0.000618973), np.complex128(0.0005947546599899355+0j))\n",
      "Epoch 800: <Test loss>: 1.3731246326642577e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008477077), np.complex128(3.0319494559401348e-05+0j)) <f>: (np.float32(-0.0006034625), np.complex128(0.0005794425154008248+0j))\n",
      "Epoch 900: <Test loss>: 1.6862115899130004e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008476634), np.complex128(3.607873443478745e-05+0j)) <f>: (np.float32(-0.00060301524), np.complex128(0.000592718800185162+0j))\n",
      "Epoch 1000: <Test loss>: 1.3627930002257926e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008476766), np.complex128(2.406010258400665e-05+0j)) <f>: (np.float32(-0.0006031448), np.complex128(0.0005803748143605257+0j))\n",
      "Epoch 1100: <Test loss>: 1.3556991689256392e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008466494), np.complex128(3.1595175581470104e-05+0j)) <f>: (np.float32(-0.00059287745), np.complex128(0.000576930737845702+0j))\n",
      "Epoch 1200: <Test loss>: 1.477463456467376e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008461233), np.complex128(3.402597837641561e-05+0j)) <f>: (np.float32(-0.00058761105), np.complex128(0.0005742920362044204+0j))\n",
      "Epoch 1300: <Test loss>: 1.3768103599431925e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084592765), np.complex128(3.09325110169467e-05+0j)) <f>: (np.float32(-0.00058565434), np.complex128(0.0005730083332280681+0j))\n",
      "Epoch 1400: <Test loss>: 1.6555923139094375e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008451198), np.complex128(3.631684395699096e-05+0j)) <f>: (np.float32(-0.0005775747), np.complex128(0.0005610718872602883+0j))\n",
      "Epoch 1500: <Test loss>: 3.464657311269548e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008514006), np.complex128(5.445374323234563e-05+0j)) <f>: (np.float32(-0.00064038543), np.complex128(0.0006110913312355361+0j))\n",
      "Epoch 1600: <Test loss>: 1.5388164911200874e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008472206), np.complex128(3.894746219585878e-05+0j)) <f>: (np.float32(-0.000598586), np.complex128(0.0005702520951891593+0j))\n",
      "Epoch 1700: <Test loss>: 1.5011693221822497e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008476049), np.complex128(3.5386998958101314e-05+0j)) <f>: (np.float32(-0.0006024242), np.complex128(0.0005750961424714042+0j))\n",
      "Epoch 1800: <Test loss>: 1.5837808859942015e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008469343), np.complex128(3.2881962857517295e-05+0j)) <f>: (np.float32(-0.00059572933), np.complex128(0.0005718475582272159+0j))\n",
      "Epoch 1900: <Test loss>: 2.3499853796238312e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008490973), np.complex128(4.594104039489701e-05+0j)) <f>: (np.float32(-0.0006173585), np.complex128(0.0005678157362939279+0j))\n",
      "Epoch 2000: <Test loss>: 1.5998534763639327e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00847477), np.complex128(2.9896053512215468e-05+0j)) <f>: (np.float32(-0.0006011549), np.complex128(0.000575346819127326+0j))\n",
      "Epoch 2100: <Test loss>: 1.7204632740686066e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008479541), np.complex128(4.316204276145304e-05+0j)) <f>: (np.float32(-0.0006059166), np.complex128(0.0005771679038417777+0j))\n",
      "Epoch 2200: <Test loss>: 2.976934183607227e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008497794), np.complex128(5.180572922604701e-05+0j)) <f>: (np.float32(-0.00062417623), np.complex128(0.0005814894120826554+0j))\n",
      "Epoch 2300: <Test loss>: 1.7164754808618454e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008441249), np.complex128(3.705053463119765e-05+0j)) <f>: (np.float32(-0.0005676329), np.complex128(0.000571963699157629+0j))\n",
      "Epoch 2400: <Test loss>: 1.8914012116511003e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008461598), np.complex128(4.071546365463632e-05+0j)) <f>: (np.float32(-0.00058798), np.complex128(0.0005701124850374175+0j))\n",
      "Epoch 2500: <Test loss>: 1.7663658127275994e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008450937), np.complex128(3.764901166826614e-05+0j)) <f>: (np.float32(-0.0005773157), np.complex128(0.0005737031490400537+0j))\n",
      "Epoch 2600: <Test loss>: 1.8594860193843488e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008473012), np.complex128(3.449116450816083e-05+0j)) <f>: (np.float32(-0.0005993876), np.complex128(0.0005795243405238357+0j))\n",
      "Epoch 2700: <Test loss>: 2.1140094759175554e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008422986), np.complex128(4.161620999059476e-05+0j)) <f>: (np.float32(-0.00054936064), np.complex128(0.0005665588009159559+0j))\n",
      "Epoch 2800: <Test loss>: 2.011612423302722e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008465073), np.complex128(3.624123500611577e-05+0j)) <f>: (np.float32(-0.00059145415), np.complex128(0.0005691590003481928+0j))\n",
      "Epoch 2900: <Test loss>: 2.239497916889377e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008416606), np.complex128(4.335175097906158e-05+0j)) <f>: (np.float32(-0.000542983), np.complex128(0.0005622211179414599+0j))\n",
      "Epoch 3000: <Test loss>: 2.2451542918133782e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008442775), np.complex128(4.448255514997376e-05+0j)) <f>: (np.float32(-0.0005691568), np.complex128(0.000575846269529332+0j))\n",
      "Epoch 3100: <Test loss>: 2.1139192085684044e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084250225), np.complex128(4.13193401983686e-05+0j)) <f>: (np.float32(-0.00055140245), np.complex128(0.0005656450870423343+0j))\n",
      "Epoch 3200: <Test loss>: 2.1827672753715888e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008459624), np.complex128(3.9492883728021514e-05+0j)) <f>: (np.float32(-0.0005860101), np.complex128(0.0005694068860696863+0j))\n",
      "Epoch 3300: <Test loss>: 2.2827941847936017e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008456485), np.complex128(4.876292882319829e-05+0j)) <f>: (np.float32(-0.0005828595), np.complex128(0.0005765853597102025+0j))\n",
      "Epoch 3400: <Test loss>: 2.5658721369836712e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00846374), np.complex128(4.649657113557126e-05+0j)) <f>: (np.float32(-0.0005901188), np.complex128(0.0005822888245053735+0j))\n",
      "Epoch 3500: <Test loss>: 2.2712215468345676e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008450265), np.complex128(4.1619619370720215e-05+0j)) <f>: (np.float32(-0.0005766395), np.complex128(0.0005695745324263668+0j))\n",
      "Epoch 3600: <Test loss>: 2.3007385152595816e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00847564), np.complex128(4.488423166589366e-05+0j)) <f>: (np.float32(-0.00060202286), np.complex128(0.0005769602963785106+0j))\n",
      "Epoch 3700: <Test loss>: 2.4100802420434775e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008476586), np.complex128(4.592275660287539e-05+0j)) <f>: (np.float32(-0.0006029654), np.complex128(0.0005821978019848148+0j))\n",
      "Epoch 3800: <Test loss>: 2.9394218472589273e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008485184), np.complex128(5.4120670580461755e-05+0j)) <f>: (np.float32(-0.0006115644), np.complex128(0.0005941636796247475+0j))\n",
      "Epoch 3900: <Test loss>: 2.5271024242101703e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008450226), np.complex128(4.6886243461933175e-05+0j)) <f>: (np.float32(-0.0005766074), np.complex128(0.0005659003306818659+0j))\n",
      "Epoch 4000: <Test loss>: 2.4122132344928104e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00845198), np.complex128(4.887045512220142e-05+0j)) <f>: (np.float32(-0.0005783605), np.complex128(0.0005791395087243727+0j))\n",
      "Epoch 4100: <Test loss>: 2.6757115847431123e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008460485), np.complex128(4.693628999065844e-05+0j)) <f>: (np.float32(-0.00058686454), np.complex128(0.000576144709222174+0j))\n",
      "Epoch 4200: <Test loss>: 2.5101332994381664e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008443065), np.complex128(4.776035302456263e-05+0j)) <f>: (np.float32(-0.00056944153), np.complex128(0.0005719645237518919+0j))\n",
      "Epoch 4300: <Test loss>: 2.5322094643343007e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008434962), np.complex128(4.654910730466721e-05+0j)) <f>: (np.float32(-0.0005613428), np.complex128(0.0005698438575986956+0j))\n",
      "Epoch 4400: <Test loss>: 2.6433835955685936e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008435474), np.complex128(4.770381281601702e-05+0j)) <f>: (np.float32(-0.00056185125), np.complex128(0.00057231542032592+0j))\n",
      "Epoch 4500: <Test loss>: 2.595195610410883e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008456325), np.complex128(4.636304236651831e-05+0j)) <f>: (np.float32(-0.0005827001), np.complex128(0.0005715883819073534+0j))\n",
      "Epoch 4600: <Test loss>: 2.9159239147702465e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008436055), np.complex128(5.0717328229346286e-05+0j)) <f>: (np.float32(-0.00056243397), np.complex128(0.0005663048893133105+0j))\n",
      "Epoch 4700: <Test loss>: 2.8534443572425516e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008441705), np.complex128(4.7701715650800315e-05+0j)) <f>: (np.float32(-0.00056808535), np.complex128(0.0005759442059556334+0j))\n",
      "Epoch 4800: <Test loss>: 3.1773436148796463e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008433264), np.complex128(5.111878670351398e-05+0j)) <f>: (np.float32(-0.0005596386), np.complex128(0.0005772893729197357+0j))\n",
      "Epoch 4900: <Test loss>: 2.84561792796012e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008441951), np.complex128(5.358258317996424e-05+0j)) <f>: (np.float32(-0.00056833244), np.complex128(0.0005722124729037134+0j))\n",
      "Epoch 5000: <Test loss>: 2.8453753202484222e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084533645), np.complex128(5.13293595346112e-05+0j)) <f>: (np.float32(-0.00057974766), np.complex128(0.0005798432682125944+0j))\n",
      "Epoch 5100: <Test loss>: 3.159816060360754e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008428626), np.complex128(5.150617950246643e-05+0j)) <f>: (np.float32(-0.0005550108), np.complex128(0.0005823627842677228+0j))\n",
      "Epoch 5200: <Test loss>: 3.158189429086633e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008454827), np.complex128(5.0516063798870655e-05+0j)) <f>: (np.float32(-0.00058120926), np.complex128(0.0005759731936154907+0j))\n",
      "Epoch 5300: <Test loss>: 2.9564846499852138e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008434167), np.complex128(5.3147942715505795e-05+0j)) <f>: (np.float32(-0.0005605483), np.complex128(0.0005747619280736179+0j))\n",
      "Epoch 5400: <Test loss>: 3.0612709451816045e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008437382), np.complex128(5.295389348483054e-05+0j)) <f>: (np.float32(-0.0005637537), np.complex128(0.0005807631348280238+0j))\n",
      "Epoch 5500: <Test loss>: 3.0597063869208796e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008432271), np.complex128(5.3843289756487615e-05+0j)) <f>: (np.float32(-0.00055864936), np.complex128(0.0005707403184231243+0j))\n",
      "Epoch 5600: <Test loss>: 3.0647915991721675e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008450096), np.complex128(5.621089414065414e-05+0j)) <f>: (np.float32(-0.0005764722), np.complex128(0.000573840856281958+0j))\n",
      "Epoch 5700: <Test loss>: 3.0878936740919016e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008439245), np.complex128(5.6293341673757705e-05+0j)) <f>: (np.float32(-0.00056562707), np.complex128(0.0005757379305292525+0j))\n",
      "Epoch 5800: <Test loss>: 3.20100707540405e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008450243), np.complex128(5.7734466830809605e-05+0j)) <f>: (np.float32(-0.000576626), np.complex128(0.0005761117254516581+0j))\n",
      "Epoch 5900: <Test loss>: 3.3808114494604524e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008460332), np.complex128(5.803097189865024e-05+0j)) <f>: (np.float32(-0.0005867094), np.complex128(0.0005827523733417793+0j))\n",
      "Epoch 6000: <Test loss>: 3.526555929056485e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084537), np.complex128(5.601487460541801e-05+0j)) <f>: (np.float32(-0.0005800794), np.complex128(0.0005817764977468006+0j))\n",
      "Epoch 6100: <Test loss>: 3.3322173749184003e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008444082), np.complex128(5.5349672827778e-05+0j)) <f>: (np.float32(-0.0005704646), np.complex128(0.0005786738032568178+0j))\n",
      "Epoch 6200: <Test loss>: 3.2188954719458707e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008432811), np.complex128(5.717505891134152e-05+0j)) <f>: (np.float32(-0.0005591837), np.complex128(0.0005759029762424884+0j))\n",
      "Epoch 6300: <Test loss>: 3.3267360777244903e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008436238), np.complex128(5.5139587617326625e-05+0j)) <f>: (np.float32(-0.0005626141), np.complex128(0.00057013005523825+0j))\n",
      "Epoch 6400: <Test loss>: 3.3242399695154745e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008438789), np.complex128(5.578557000560827e-05+0j)) <f>: (np.float32(-0.000565174), np.complex128(0.0005765645545626462+0j))\n",
      "Epoch 6500: <Test loss>: 3.570916760509135e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008460171), np.complex128(6.108005983275357e-05+0j)) <f>: (np.float32(-0.0005865546), np.complex128(0.0005816448798163761+0j))\n",
      "Epoch 6600: <Test loss>: 3.2885877772059757e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008452836), np.complex128(5.300412237574856e-05+0j)) <f>: (np.float32(-0.00057921285), np.complex128(0.0005705107006360705+0j))\n",
      "Epoch 6700: <Test loss>: 3.4353824958088808e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084362235), np.complex128(5.5991064446076764e-05+0j)) <f>: (np.float32(-0.0005626007), np.complex128(0.0005708248710502356+0j))\n",
      "Epoch 6800: <Test loss>: 3.369073965586722e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008431586), np.complex128(5.699131314455703e-05+0j)) <f>: (np.float32(-0.0005579685), np.complex128(0.0005736096527367064+0j))\n",
      "Epoch 6900: <Test loss>: 3.4893323572759982e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008452014), np.complex128(5.84851766548751e-05+0j)) <f>: (np.float32(-0.0005783965), np.complex128(0.0005764545029437129+0j))\n",
      "Epoch 7000: <Test loss>: 3.91350931749912e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008447335), np.complex128(6.05469675705794e-05+0j)) <f>: (np.float32(-0.00057371164), np.complex128(0.0005770093914523171+0j))\n",
      "Epoch 7100: <Test loss>: 3.751103349713958e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008448389), np.complex128(6.452065957272837e-05+0j)) <f>: (np.float32(-0.00057477463), np.complex128(0.0005713198178989595+0j))\n",
      "Epoch 7200: <Test loss>: 3.6002538763568737e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008426589), np.complex128(5.799174024083455e-05+0j)) <f>: (np.float32(-0.0005529703), np.complex128(0.0005700118211070156+0j))\n",
      "Epoch 7300: <Test loss>: 3.93087930206093e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008444155), np.complex128(6.876826751718908e-05+0j)) <f>: (np.float32(-0.00057053234), np.complex128(0.0005729263812444014+0j))\n",
      "Epoch 7400: <Test loss>: 3.675816969916923e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00842764), np.complex128(6.173631396976202e-05+0j)) <f>: (np.float32(-0.0005540249), np.complex128(0.0005731595511298185+0j))\n",
      "Epoch 7500: <Test loss>: 3.6986159557272913e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008410299), np.complex128(6.438985830777577e-05+0j)) <f>: (np.float32(-0.0005366822), np.complex128(0.000568806391155311+0j))\n",
      "Epoch 7600: <Test loss>: 3.904880031768698e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084449295), np.complex128(6.523295836884557e-05+0j)) <f>: (np.float32(-0.0005713083), np.complex128(0.0005785916609821673+0j))\n",
      "Epoch 7700: <Test loss>: 3.7416614304675022e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008428252), np.complex128(6.035773904482572e-05+0j)) <f>: (np.float32(-0.00055463635), np.complex128(0.0005730278063387381+0j))\n",
      "Epoch 7800: <Test loss>: 4.042568434670102e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008437447), np.complex128(6.514386650889287e-05+0j)) <f>: (np.float32(-0.0005638256), np.complex128(0.0005713336457104451+0j))\n",
      "Epoch 7900: <Test loss>: 3.989481683674967e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00843516), np.complex128(6.289000856026068e-05+0j)) <f>: (np.float32(-0.0005615444), np.complex128(0.0005658775591941443+0j))\n",
      "Epoch 8000: <Test loss>: 3.8946654967730865e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008414316), np.complex128(6.413803197716056e-05+0j)) <f>: (np.float32(-0.00054069114), np.complex128(0.0005693402842253751+0j))\n",
      "Epoch 8100: <Test loss>: 3.907611699105473e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084229335), np.complex128(6.373717609110806e-05+0j)) <f>: (np.float32(-0.0005493088), np.complex128(0.0005668703706866765+0j))\n",
      "Epoch 8200: <Test loss>: 4.119404820812633e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008430439), np.complex128(6.308990130989515e-05+0j)) <f>: (np.float32(-0.00055682193), np.complex128(0.0005744037370118795+0j))\n",
      "Epoch 8300: <Test loss>: 4.172224635112798e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008431496), np.complex128(6.553903745180595e-05+0j)) <f>: (np.float32(-0.00055787474), np.complex128(0.0005698388466027903+0j))\n",
      "Epoch 8400: <Test loss>: 4.293598522053799e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008426462), np.complex128(6.459359255664462e-05+0j)) <f>: (np.float32(-0.00055283855), np.complex128(0.0005743369448765846+0j))\n",
      "Epoch 8500: <Test loss>: 4.176943548372947e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008422167), np.complex128(6.60971172987833e-05+0j)) <f>: (np.float32(-0.00054854347), np.complex128(0.0005667363424037912+0j))\n",
      "Epoch 8600: <Test loss>: 4.15780550611089e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008410916), np.complex128(6.494274876105055e-05+0j)) <f>: (np.float32(-0.0005372964), np.complex128(0.0005714046876777104+0j))\n",
      "Epoch 8700: <Test loss>: 4.354020347818732e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008414117), np.complex128(7.090779626414922e-05+0j)) <f>: (np.float32(-0.0005404964), np.complex128(0.0005578677676759686+0j))\n",
      "Epoch 8800: <Test loss>: 4.481118594412692e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008424169), np.complex128(6.993520318864008e-05+0j)) <f>: (np.float32(-0.0005505507), np.complex128(0.000572111428391344+0j))\n",
      "Epoch 8900: <Test loss>: 4.459163392311893e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008420849), np.complex128(6.91084285082175e-05+0j)) <f>: (np.float32(-0.0005472267), np.complex128(0.0005733724867406306+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36577000",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0003762592386920005 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 7.104713859007461e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008443876), np.complex128(0.00010871905874682751+0j)) <f>: (np.float32(-0.00057025463), np.complex128(0.0005826645223376165+0j))\n",
      "Epoch 400: <Test loss>: 4.407500455272384e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008466654), np.complex128(9.701649625565377e-05+0j)) <f>: (np.float32(-0.0005930295), np.complex128(0.0005739748211345154+0j))\n",
      "Epoch 600: <Test loss>: 3.9727569856040645e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008476453), np.complex128(8.602807996889777e-05+0j)) <f>: (np.float32(-0.0006028326), np.complex128(0.0005857791417292475+0j))\n",
      "Epoch 800: <Test loss>: 3.591165295802057e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008462036), np.complex128(7.353414484906924e-05+0j)) <f>: (np.float32(-0.0005884215), np.complex128(0.0005749305258852171+0j))\n",
      "Epoch 1000: <Test loss>: 3.2258305964205647e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008474395), np.complex128(6.931259487619528e-05+0j)) <f>: (np.float32(-0.0006007731), np.complex128(0.0005724517955309382+0j))\n",
      "Epoch 1200: <Test loss>: 2.8870513233414385e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008500923), np.complex128(6.316141107582879e-05+0j)) <f>: (np.float32(-0.00062730623), np.complex128(0.0005677383512938711+0j))\n",
      "Epoch 1400: <Test loss>: 3.126507635897724e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008528864), np.complex128(6.794648797508985e-05+0j)) <f>: (np.float32(-0.000655245), np.complex128(0.0005846366980931628+0j))\n",
      "Epoch 1600: <Test loss>: 3.139201680824044e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008513322), np.complex128(6.346373984065117e-05+0j)) <f>: (np.float32(-0.0006396975), np.complex128(0.0005690199610694023+0j))\n",
      "Epoch 1800: <Test loss>: 2.758554728643503e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008496974), np.complex128(5.696779238608689e-05+0j)) <f>: (np.float32(-0.00062335684), np.complex128(0.0005746599321063299+0j))\n",
      "Epoch 2000: <Test loss>: 2.8873730570921907e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008525531), np.complex128(6.411780166695104e-05+0j)) <f>: (np.float32(-0.0006519086), np.complex128(0.0005875206848124933+0j))\n",
      "Epoch 2200: <Test loss>: 2.919354074037983e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008514285), np.complex128(6.245821849616293e-05+0j)) <f>: (np.float32(-0.0006406673), np.complex128(0.0005765386749888567+0j))\n",
      "Epoch 2400: <Test loss>: 2.906083409470739e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008498335), np.complex128(6.009722672368159e-05+0j)) <f>: (np.float32(-0.0006247201), np.complex128(0.000578767362990493+0j))\n",
      "Epoch 2600: <Test loss>: 2.7880785182787804e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008503937), np.complex128(5.6260231042585854e-05+0j)) <f>: (np.float32(-0.0006303178), np.complex128(0.0005790046558472246+0j))\n",
      "Epoch 2800: <Test loss>: 2.9353170702961506e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008501597), np.complex128(6.203917792797618e-05+0j)) <f>: (np.float32(-0.00062797614), np.complex128(0.0005754242675577107+0j))\n",
      "Epoch 3000: <Test loss>: 2.894723820645595e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008505472), np.complex128(6.113289729590712e-05+0j)) <f>: (np.float32(-0.00063185144), np.complex128(0.0005745711296472483+0j))\n",
      "Epoch 3200: <Test loss>: 3.0353951387951383e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008510395), np.complex128(6.112013590680963e-05+0j)) <f>: (np.float32(-0.0006367824), np.complex128(0.0005840398187074787+0j))\n",
      "Epoch 3400: <Test loss>: 2.892532393161673e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00852171), np.complex128(5.976060593782994e-05+0j)) <f>: (np.float32(-0.00064808957), np.complex128(0.0005833858520266706+0j))\n",
      "Epoch 3600: <Test loss>: 2.864939688151935e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008519801), np.complex128(5.8984460552270355e-05+0j)) <f>: (np.float32(-0.00064618373), np.complex128(0.0005792671305441386+0j))\n",
      "Epoch 3800: <Test loss>: 3.851107067021076e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008501675), np.complex128(7.649984568833676e-05+0j)) <f>: (np.float32(-0.00062805007), np.complex128(0.0005803525503154273+0j))\n",
      "Epoch 4000: <Test loss>: 3.2396756068919785e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008528041), np.complex128(5.937706653129835e-05+0j)) <f>: (np.float32(-0.00065442006), np.complex128(0.0005680352686588433+0j))\n",
      "Epoch 4200: <Test loss>: 3.03912293020403e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008515783), np.complex128(5.957257069512017e-05+0j)) <f>: (np.float32(-0.0006421565), np.complex128(0.0005772984434566277+0j))\n",
      "Epoch 4400: <Test loss>: 3.0292260362330126e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008510798), np.complex128(6.250794787324862e-05+0j)) <f>: (np.float32(-0.00063717325), np.complex128(0.0005792989091384243+0j))\n",
      "Epoch 4600: <Test loss>: 3.2400178042735206e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008493695), np.complex128(6.502541037151083e-05+0j)) <f>: (np.float32(-0.0006200737), np.complex128(0.0005842890364658584+0j))\n",
      "Epoch 4800: <Test loss>: 3.380082716830657e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008539671), np.complex128(6.662428278969289e-05+0j)) <f>: (np.float32(-0.0006660539), np.complex128(0.000588234910304821+0j))\n",
      "Epoch 5000: <Test loss>: 3.2233942874881905e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008525142), np.complex128(6.356424123083765e-05+0j)) <f>: (np.float32(-0.00065152615), np.complex128(0.0005815996539925724+0j))\n",
      "Epoch 5200: <Test loss>: 3.507518840706325e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008511653), np.complex128(6.430057615683932e-05+0j)) <f>: (np.float32(-0.0006380248), np.complex128(0.000564073410377246+0j))\n",
      "Epoch 5400: <Test loss>: 3.2390598789788783e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008546038), np.complex128(5.9501865701471946e-05+0j)) <f>: (np.float32(-0.00067242514), np.complex128(0.0005863172846312819+0j))\n",
      "Epoch 5600: <Test loss>: 3.2853583888936555e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008533618), np.complex128(6.186535900751045e-05+0j)) <f>: (np.float32(-0.00065999496), np.complex128(0.000582300432255382+0j))\n",
      "Epoch 5800: <Test loss>: 3.218342726540868e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008528335), np.complex128(6.573315804160011e-05+0j)) <f>: (np.float32(-0.0006547183), np.complex128(0.000576611937017599+0j))\n",
      "Epoch 6000: <Test loss>: 3.3902247196238022e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008531825), np.complex128(6.373915432445993e-05+0j)) <f>: (np.float32(-0.0006582112), np.complex128(0.0005871213274679378+0j))\n",
      "Epoch 6200: <Test loss>: 3.4444171888026176e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008534186), np.complex128(6.771020207481391e-05+0j)) <f>: (np.float32(-0.00066056335), np.complex128(0.0005833789381209279+0j))\n",
      "Epoch 6400: <Test loss>: 3.4494044029997895e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008514015), np.complex128(6.502729345937082e-05+0j)) <f>: (np.float32(-0.00064039126), np.complex128(0.0005836696393137642+0j))\n",
      "Epoch 6600: <Test loss>: 3.6098194868827704e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008547526), np.complex128(7.076041589723774e-05+0j)) <f>: (np.float32(-0.0006739054), np.complex128(0.0005859878909384171+0j))\n",
      "Epoch 6800: <Test loss>: 3.758892717087292e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008555346), np.complex128(7.046879496464659e-05+0j)) <f>: (np.float32(-0.00068172044), np.complex128(0.0005932967773331274+0j))\n",
      "Epoch 7000: <Test loss>: 3.6344497402751585e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008532378), np.complex128(6.68517360168066e-05+0j)) <f>: (np.float32(-0.0006587626), np.complex128(0.0005794508247737816+0j))\n",
      "Epoch 7200: <Test loss>: 3.5793757433566498e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008546366), np.complex128(6.45192046395818e-05+0j)) <f>: (np.float32(-0.00067274686), np.complex128(0.0005888469495388769+0j))\n",
      "Epoch 7400: <Test loss>: 3.608161250667763e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008532702), np.complex128(6.63113056585717e-05+0j)) <f>: (np.float32(-0.00065907626), np.complex128(0.0005876115170420682+0j))\n",
      "Epoch 7600: <Test loss>: 3.7829499888175633e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008539204), np.complex128(6.78334313443716e-05+0j)) <f>: (np.float32(-0.00066558685), np.complex128(0.0005896287917607623+0j))\n",
      "Epoch 7800: <Test loss>: 3.5948560253018513e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008533506), np.complex128(6.75215999235484e-05+0j)) <f>: (np.float32(-0.0006598802), np.complex128(0.0005908266100731172+0j))\n",
      "Epoch 8000: <Test loss>: 3.539165163601865e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008522499), np.complex128(6.573413328289181e-05+0j)) <f>: (np.float32(-0.0006488793), np.complex128(0.0005837422036088995+0j))\n",
      "Epoch 8200: <Test loss>: 3.722719156940002e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00854617), np.complex128(6.838893829867289e-05+0j)) <f>: (np.float32(-0.00067255035), np.complex128(0.0005912308515529221+0j))\n",
      "Epoch 8400: <Test loss>: 3.819297035079217e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008536155), np.complex128(6.63831603269134e-05+0j)) <f>: (np.float32(-0.0006625422), np.complex128(0.0005799482454052944+0j))\n",
      "Epoch 8600: <Test loss>: 4.098132649232866e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008533523), np.complex128(7.309562720581696e-05+0j)) <f>: (np.float32(-0.00065990345), np.complex128(0.000593114161419059+0j))\n",
      "Epoch 8800: <Test loss>: 3.851586825476261e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008513682), np.complex128(7.300358980001169e-05+0j)) <f>: (np.float32(-0.0006400622), np.complex128(0.0005924419268038113+0j))\n",
      "Epoch 9000: <Test loss>: 4.559440640150569e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008518945), np.complex128(6.562886668932067e-05+0j)) <f>: (np.float32(-0.0006453266), np.complex128(0.0005778072181168373+0j))\n",
      "Epoch 9200: <Test loss>: 3.975177151005482e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008511124), np.complex128(6.951606351056596e-05+0j)) <f>: (np.float32(-0.00063751335), np.complex128(0.0005784987355517712+0j))\n",
      "Epoch 9400: <Test loss>: 3.835844381683273e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00853998), np.complex128(7.174647205984693e-05+0j)) <f>: (np.float32(-0.0006663542), np.complex128(0.0005905025445277972+0j))\n",
      "Epoch 9600: <Test loss>: 3.957284206990153e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008520687), np.complex128(6.940004151201768e-05+0j)) <f>: (np.float32(-0.0006470648), np.complex128(0.0005822756944274949+0j))\n",
      "Epoch 9800: <Test loss>: 3.850866050925106e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00853362), np.complex128(6.918114345038155e-05+0j)) <f>: (np.float32(-0.00065999717), np.complex128(0.0005896323438591256+0j))\n",
      "Epoch 10000: <Test loss>: 3.981512236350682e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008520848), np.complex128(6.838409380737834e-05+0j)) <f>: (np.float32(-0.00064722815), np.complex128(0.0005984502377549438+0j))\n",
      "Epoch 10200: <Test loss>: 4.05036507800105e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085363), np.complex128(7.198947364609091e-05+0j)) <f>: (np.float32(-0.0006626747), np.complex128(0.0005877927374889225+0j))\n",
      "Epoch 10400: <Test loss>: 4.0513136809749994e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008534051), np.complex128(6.802416634041327e-05+0j)) <f>: (np.float32(-0.00066043617), np.complex128(0.0005856547548562053+0j))\n",
      "Epoch 10600: <Test loss>: 4.947793513565557e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008520053), np.complex128(8.04893833792781e-05+0j)) <f>: (np.float32(-0.0006464305), np.complex128(0.0005981608685989937+0j))\n",
      "Epoch 10800: <Test loss>: 4.575927505356958e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008540313), np.complex128(7.248849589338351e-05+0j)) <f>: (np.float32(-0.00066669245), np.complex128(0.0005941355799894809+0j))\n",
      "Epoch 11000: <Test loss>: 4.40140911450726e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085539585), np.complex128(7.462344179915177e-05+0j)) <f>: (np.float32(-0.00068033836), np.complex128(0.0005959033197981558+0j))\n",
      "Epoch 11200: <Test loss>: 4.410584551806096e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008525178), np.complex128(7.560328178962524e-05+0j)) <f>: (np.float32(-0.0006515641), np.complex128(0.0005838381736950355+0j))\n",
      "Epoch 11400: <Test loss>: 4.642505246010842e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008545754), np.complex128(7.795048935897147e-05+0j)) <f>: (np.float32(-0.00067213166), np.complex128(0.00059638761035179+0j))\n",
      "Epoch 11600: <Test loss>: 4.52945141660166e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008540818), np.complex128(7.820652587760206e-05+0j)) <f>: (np.float32(-0.0006671967), np.complex128(0.0005945532052684761+0j))\n",
      "Epoch 11800: <Test loss>: 4.919867478747619e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008545925), np.complex128(7.593364279499074e-05+0j)) <f>: (np.float32(-0.00067230954), np.complex128(0.0005856695975529375+0j))\n",
      "Epoch 12000: <Test loss>: 5.0531198212411255e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008531482), np.complex128(8.980948689636663e-05+0j)) <f>: (np.float32(-0.00065786025), np.complex128(0.0006004893325064408+0j))\n",
      "Epoch 12200: <Test loss>: 4.965258540323703e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008545169), np.complex128(8.252089820658927e-05+0j)) <f>: (np.float32(-0.0006715463), np.complex128(0.0005934583978086559+0j))\n",
      "Epoch 12400: <Test loss>: 5.124995823280187e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008553783), np.complex128(7.98405069822853e-05+0j)) <f>: (np.float32(-0.00068015733), np.complex128(0.0005938894068868412+0j))\n",
      "Epoch 12600: <Test loss>: 5.13152235726011e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008548819), np.complex128(8.48545951160895e-05+0j)) <f>: (np.float32(-0.00067519373), np.complex128(0.0005970156340284807+0j))\n",
      "Epoch 12800: <Test loss>: 5.097765097161755e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008546658), np.complex128(8.3988771140044e-05+0j)) <f>: (np.float32(-0.00067303487), np.complex128(0.0005915348731146208+0j))\n",
      "Epoch 13000: <Test loss>: 5.2692666940856725e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008539475), np.complex128(8.557171461712829e-05+0j)) <f>: (np.float32(-0.0006658573), np.complex128(0.0006026892231392035+0j))\n",
      "Epoch 13200: <Test loss>: 5.1539414016588125e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008540268), np.complex128(8.997023520488628e-05+0j)) <f>: (np.float32(-0.00066664896), np.complex128(0.0005988773141527983+0j))\n",
      "Epoch 13400: <Test loss>: 5.31238720213878e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00854184), np.complex128(8.778997625861346e-05+0j)) <f>: (np.float32(-0.0006682171), np.complex128(0.0005904838325810622+0j))\n",
      "Epoch 13600: <Test loss>: 5.481085736391833e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085495645), np.complex128(8.764559297469598e-05+0j)) <f>: (np.float32(-0.00067595), np.complex128(0.0005989446137307165+0j))\n",
      "Epoch 13800: <Test loss>: 5.3914886848360766e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008540374), np.complex128(8.806834818146127e-05+0j)) <f>: (np.float32(-0.000666754), np.complex128(0.0005936460881489576+0j))\n",
      "Epoch 14000: <Test loss>: 5.830308509757742e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008551104), np.complex128(8.98963547304468e-05+0j)) <f>: (np.float32(-0.0006774814), np.complex128(0.0006036810831768169+0j))\n",
      "Epoch 14200: <Test loss>: 5.504045020643389e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008533899), np.complex128(9.036872831122394e-05+0j)) <f>: (np.float32(-0.00066027645), np.complex128(0.0005941044991288023+0j))\n",
      "Epoch 14400: <Test loss>: 6.087578185542952e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008552367), np.complex128(9.0836645911465e-05+0j)) <f>: (np.float32(-0.0006787466), np.complex128(0.0006031271461231316+0j))\n",
      "Epoch 14600: <Test loss>: 5.833112027175957e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008539084), np.complex128(9.480058153630148e-05+0j)) <f>: (np.float32(-0.00066546036), np.complex128(0.0005927120131400751+0j))\n",
      "Epoch 14800: <Test loss>: 5.964043339190539e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008538333), np.complex128(9.076840280741901e-05+0j)) <f>: (np.float32(-0.00066471525), np.complex128(0.0005960574554949903+0j))\n",
      "Epoch 15000: <Test loss>: 6.1464234022423625e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00855214), np.complex128(0.00010077105629683242+0j)) <f>: (np.float32(-0.0006785202), np.complex128(0.0005975196514140966+0j))\n",
      "Epoch 15200: <Test loss>: 6.2138692555890884e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008546585), np.complex128(9.485893743798366e-05+0j)) <f>: (np.float32(-0.0006729684), np.complex128(0.0006041427925337132+0j))\n",
      "Epoch 15400: <Test loss>: 6.173155270516872e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008542028), np.complex128(9.522425648282155e-05+0j)) <f>: (np.float32(-0.0006684149), np.complex128(0.0005954769411339084+0j))\n",
      "Epoch 15600: <Test loss>: 6.743794529029401e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008528828), np.complex128(0.00010357619911856403+0j)) <f>: (np.float32(-0.0006552025), np.complex128(0.00059893903186186+0j))\n",
      "Epoch 15800: <Test loss>: 6.172396751935594e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008545034), np.complex128(9.371501111798398e-05+0j)) <f>: (np.float32(-0.0006714084), np.complex128(0.0005947806029940528+0j))\n",
      "Epoch 16000: <Test loss>: 6.25379607299692e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008552825), np.complex128(9.684897675962914e-05+0j)) <f>: (np.float32(-0.0006792087), np.complex128(0.0006011153264126382+0j))\n",
      "Epoch 16200: <Test loss>: 6.3597544794902205e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008551033), np.complex128(9.501153494936619e-05+0j)) <f>: (np.float32(-0.0006774136), np.complex128(0.0005979246540578365+0j))\n",
      "Epoch 16400: <Test loss>: 6.588918495253893e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008542316), np.complex128(9.779603912815232e-05+0j)) <f>: (np.float32(-0.00066869764), np.complex128(0.0006021365546920764+0j))\n",
      "Epoch 16600: <Test loss>: 6.541136826854199e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008564791), np.complex128(9.656286633677563e-05+0j)) <f>: (np.float32(-0.000691168), np.complex128(0.0006026969616392091+0j))\n",
      "Epoch 16800: <Test loss>: 7.033486326690763e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008549206), np.complex128(0.00010300397034406605+0j)) <f>: (np.float32(-0.0006755872), np.complex128(0.0006091751644595391+0j))\n",
      "Epoch 17000: <Test loss>: 6.9704292400274426e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008540892), np.complex128(0.00010003522484905858+0j)) <f>: (np.float32(-0.0006672727), np.complex128(0.0006005971006335691+0j))\n",
      "Epoch 17200: <Test loss>: 6.791601208533393e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008538353), np.complex128(9.881156660686921e-05+0j)) <f>: (np.float32(-0.000664734), np.complex128(0.0005946201242644269+0j))\n",
      "Epoch 17400: <Test loss>: 6.908057912369259e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008584855), np.complex128(9.883195945729403e-05+0j)) <f>: (np.float32(-0.00071123824), np.complex128(0.0006027495453810511+0j))\n",
      "Epoch 17600: <Test loss>: 6.9528259700746275e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008547897), np.complex128(9.941621620772297e-05+0j)) <f>: (np.float32(-0.00067427946), np.complex128(0.0006012902038267009+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dc052f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0003762592386920005 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 1.1993038242508192e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008536739), np.complex128(0.0001362082563950833+0j)) <f>: (np.float32(-0.00066312135), np.complex128(0.0006147727006070913+0j))\n",
      "Epoch 800: <Test loss>: 8.794293535174802e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008559615), np.complex128(0.000124857835298123+0j)) <f>: (np.float32(-0.0006859995), np.complex128(0.0005895842636705657+0j))\n",
      "Epoch 1200: <Test loss>: 7.581493719044374e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008573094), np.complex128(0.0001221515327848656+0j)) <f>: (np.float32(-0.0006994708), np.complex128(0.0006027338146597281+0j))\n",
      "Epoch 1600: <Test loss>: 6.559085250046337e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085780015), np.complex128(0.00010586130046809006+0j)) <f>: (np.float32(-0.0007043782), np.complex128(0.0005843659140232918+0j))\n",
      "Epoch 2000: <Test loss>: 5.767021775682224e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008553386), np.complex128(0.0001020251610964212+0j)) <f>: (np.float32(-0.0006797616), np.complex128(0.0005993588772023322+0j))\n",
      "Epoch 2400: <Test loss>: 5.523518211703049e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008526229), np.complex128(0.00010157754912990404+0j)) <f>: (np.float32(-0.0006526054), np.complex128(0.0005942710671699083+0j))\n",
      "Epoch 2800: <Test loss>: 5.072869498690125e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008532459), np.complex128(9.63405351086411e-05+0j)) <f>: (np.float32(-0.0006588379), np.complex128(0.0005953847768674472+0j))\n",
      "Epoch 3200: <Test loss>: 5.20514686286333e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008496094), np.complex128(9.146021360761954e-05+0j)) <f>: (np.float32(-0.00062247104), np.complex128(0.0005957124579414584+0j))\n",
      "Epoch 3600: <Test loss>: 5.062138825451257e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085099945), np.complex128(9.055145522836628e-05+0j)) <f>: (np.float32(-0.0006363791), np.complex128(0.0005925937155785129+0j))\n",
      "Epoch 4000: <Test loss>: 4.953741154167801e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00851962), np.complex128(8.998218389290735e-05+0j)) <f>: (np.float32(-0.00064600154), np.complex128(0.0005878609885217595+0j))\n",
      "Epoch 4400: <Test loss>: 5.079451057099504e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008484954), np.complex128(8.97441853737777e-05+0j)) <f>: (np.float32(-0.0006113384), np.complex128(0.0005980910952382867+0j))\n",
      "Epoch 4800: <Test loss>: 5.34518267159001e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008514661), np.complex128(9.24327987543377e-05+0j)) <f>: (np.float32(-0.0006410405), np.complex128(0.0006016499806466372+0j))\n",
      "Epoch 5200: <Test loss>: 5.526654604182113e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008412372), np.complex128(8.61992784239415e-05+0j)) <f>: (np.float32(-0.0005387528), np.complex128(0.0005663109151944625+0j))\n",
      "Epoch 5600: <Test loss>: 6.0686920733132865e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008510614), np.complex128(9.612671940202921e-05+0j)) <f>: (np.float32(-0.00063699944), np.complex128(0.0006078751598889126+0j))\n",
      "Epoch 6000: <Test loss>: 5.2148825488984585e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00848602), np.complex128(9.102490712471798e-05+0j)) <f>: (np.float32(-0.0006123987), np.complex128(0.0005984909600254655+0j))\n",
      "Epoch 6400: <Test loss>: 5.630955456581432e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008474215), np.complex128(9.859125722043697e-05+0j)) <f>: (np.float32(-0.0006005942), np.complex128(0.000610650427026196+0j))\n",
      "Epoch 6800: <Test loss>: 5.677125045622233e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008480275), np.complex128(9.831906182576992e-05+0j)) <f>: (np.float32(-0.00060666015), np.complex128(0.0005979623316726184+0j))\n",
      "Epoch 7200: <Test loss>: 5.41761710337596e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084726885), np.complex128(9.112101992909182e-05+0j)) <f>: (np.float32(-0.00059906533), np.complex128(0.0005918275406476225+0j))\n",
      "Epoch 7600: <Test loss>: 5.4968609219940845e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084698275), np.complex128(9.676859467657832e-05+0j)) <f>: (np.float32(-0.00059620984), np.complex128(0.0005997101543583278+0j))\n",
      "Epoch 8000: <Test loss>: 5.6745366237009875e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008459842), np.complex128(0.00010100365530929828+0j)) <f>: (np.float32(-0.0005862222), np.complex128(0.0006030034569836965+0j))\n",
      "Epoch 8400: <Test loss>: 5.591496119450312e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008475263), np.complex128(9.988045484894495e-05+0j)) <f>: (np.float32(-0.0006016396), np.complex128(0.00059711515621298+0j))\n",
      "Epoch 8800: <Test loss>: 6.524258878926048e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008476094), np.complex128(0.00010863656760537352+0j)) <f>: (np.float32(-0.00060247304), np.complex128(0.0006058085363751001+0j))\n",
      "Epoch 9200: <Test loss>: 5.749875981564401e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084672095), np.complex128(0.00010051138046313768+0j)) <f>: (np.float32(-0.0005935968), np.complex128(0.0005999676814896644+0j))\n",
      "Epoch 9600: <Test loss>: 5.914262146688998e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008459064), np.complex128(0.0001038704365521817+0j)) <f>: (np.float32(-0.00058544753), np.complex128(0.0006059662241702979+0j))\n",
      "Epoch 10000: <Test loss>: 5.9523058553168084e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008453967), np.complex128(0.00010150331978866101+0j)) <f>: (np.float32(-0.0005803504), np.complex128(0.0006008130809001212+0j))\n",
      "Epoch 10400: <Test loss>: 5.9408703236840665e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008466687), np.complex128(0.00010546238713582997+0j)) <f>: (np.float32(-0.0005930666), np.complex128(0.0006029419929959464+0j))\n",
      "Epoch 10800: <Test loss>: 6.549846148118377e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008493821), np.complex128(0.00010990898791214786+0j)) <f>: (np.float32(-0.0006202033), np.complex128(0.0006113819689980445+0j))\n",
      "Epoch 11200: <Test loss>: 6.172823304950725e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008471926), np.complex128(0.00011050861858830531+0j)) <f>: (np.float32(-0.00059830316), np.complex128(0.0006062086548835906+0j))\n",
      "Epoch 11600: <Test loss>: 6.269765435718e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008463599), np.complex128(0.00010908562261185074+0j)) <f>: (np.float32(-0.00058997865), np.complex128(0.0006057695901537601+0j))\n",
      "Epoch 12000: <Test loss>: 6.287645192060154e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008471752), np.complex128(0.00010634311723893561+0j)) <f>: (np.float32(-0.0005981364), np.complex128(0.0006049630735343155+0j))\n",
      "Epoch 12400: <Test loss>: 6.244256383070024e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008472531), np.complex128(0.00011029483459685737+0j)) <f>: (np.float32(-0.00059891486), np.complex128(0.0006051491146860915+0j))\n",
      "Epoch 12800: <Test loss>: 7.047154213069007e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008500438), np.complex128(0.00011546230519235205+0j)) <f>: (np.float32(-0.0006268209), np.complex128(0.0006138921607949695+0j))\n",
      "Epoch 13200: <Test loss>: 6.3826532823441084e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008485901), np.complex128(0.00010636718904837951+0j)) <f>: (np.float32(-0.00061229), np.complex128(0.0005867455027750389+0j))\n",
      "Epoch 13600: <Test loss>: 6.9046418502694e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00853531), np.complex128(0.00011742667901756473+0j)) <f>: (np.float32(-0.00066168915), np.complex128(0.0006152997432017405+0j))\n",
      "Epoch 14000: <Test loss>: 6.7134201344742905e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008492207), np.complex128(0.00011237004499131128+0j)) <f>: (np.float32(-0.00061858335), np.complex128(0.0006063843568919164+0j))\n",
      "Epoch 14400: <Test loss>: 6.927181402716087e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008527893), np.complex128(0.00011374401725307821+0j)) <f>: (np.float32(-0.0006542738), np.complex128(0.0006041224948287804+0j))\n",
      "Epoch 14800: <Test loss>: 6.711964033456752e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008505808), np.complex128(0.00011261193654681674+0j)) <f>: (np.float32(-0.00063218834), np.complex128(0.0006030725326107964+0j))\n",
      "Epoch 15200: <Test loss>: 6.805302746215602e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008484256), np.complex128(0.00011121160105500995+0j)) <f>: (np.float32(-0.0006106327), np.complex128(0.0006033431263896835+0j))\n",
      "Epoch 15600: <Test loss>: 7.109352736733854e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00850589), np.complex128(0.00011561583037228033+0j)) <f>: (np.float32(-0.0006322737), np.complex128(0.0006011953754864673+0j))\n",
      "Epoch 16000: <Test loss>: 7.034288501017727e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008510245), np.complex128(0.00011410776639730019+0j)) <f>: (np.float32(-0.00063662615), np.complex128(0.0006083010945408646+0j))\n",
      "Epoch 16400: <Test loss>: 7.860865480324719e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008495897), np.complex128(0.00012086038467377422+0j)) <f>: (np.float32(-0.0006222773), np.complex128(0.0006227066928834081+0j))\n",
      "Epoch 16800: <Test loss>: 7.257002380356425e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008514123), np.complex128(0.00011459533947040405+0j)) <f>: (np.float32(-0.0006404989), np.complex128(0.0005868844786235016+0j))\n",
      "Epoch 17200: <Test loss>: 8.842152055876795e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00853991), np.complex128(0.00013644608840960218+0j)) <f>: (np.float32(-0.0006662848), np.complex128(0.0006243669817165943+0j))\n",
      "Epoch 17600: <Test loss>: 7.179171916504856e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085387165), np.complex128(0.00011627859008229561+0j)) <f>: (np.float32(-0.000665095), np.complex128(0.0006058287072193772+0j))\n",
      "Epoch 18000: <Test loss>: 7.193937108240789e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008525089), np.complex128(0.00011452075333356652+0j)) <f>: (np.float32(-0.0006514693), np.complex128(0.0006030738012173547+0j))\n",
      "Epoch 18400: <Test loss>: 7.317581548704766e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008526836), np.complex128(0.00011322286575013412+0j)) <f>: (np.float32(-0.00065321045), np.complex128(0.0005928349411155752+0j))\n",
      "Epoch 18800: <Test loss>: 8.063292625593022e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008561166), np.complex128(0.00010506746991422856+0j)) <f>: (np.float32(-0.0006875451), np.complex128(0.0005959857157941179+0j))\n",
      "Epoch 19200: <Test loss>: 7.5510765782382805e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008531928), np.complex128(0.00011623381619957831+0j)) <f>: (np.float32(-0.0006583091), np.complex128(0.0006094575562794186+0j))\n",
      "Epoch 19600: <Test loss>: 7.620393262186553e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008539829), np.complex128(0.00012155895080389775+0j)) <f>: (np.float32(-0.0006662113), np.complex128(0.0006017174705155392+0j))\n",
      "Epoch 20000: <Test loss>: 7.91845923231449e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008560184), np.complex128(0.00011967892345723058+0j)) <f>: (np.float32(-0.00068656023), np.complex128(0.0006072501808679619+0j))\n",
      "Epoch 20400: <Test loss>: 7.82213555794442e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008556021), np.complex128(0.00011989318317613789+0j)) <f>: (np.float32(-0.00068239734), np.complex128(0.000601058429408498+0j))\n",
      "Epoch 20800: <Test loss>: 7.787311915308237e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008542689), np.complex128(0.00012214388943035178+0j)) <f>: (np.float32(-0.00066907296), np.complex128(0.0005991799402472827+0j))\n",
      "Epoch 21200: <Test loss>: 7.779621228110045e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008543604), np.complex128(0.00012050682402597366+0j)) <f>: (np.float32(-0.0006699814), np.complex128(0.0006003852433383316+0j))\n",
      "Epoch 21600: <Test loss>: 7.977020686666947e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008551176), np.complex128(0.00012175123191418236+0j)) <f>: (np.float32(-0.00067755755), np.complex128(0.0006021151786715689+0j))\n",
      "Epoch 22000: <Test loss>: 8.0372219599667e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008568915), np.complex128(0.00012165533318716524+0j)) <f>: (np.float32(-0.00069529377), np.complex128(0.0006054780643666609+0j))\n",
      "Epoch 22400: <Test loss>: 8.243711818067823e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008551063), np.complex128(0.00012336746838463125+0j)) <f>: (np.float32(-0.00067743566), np.complex128(0.000602631057528505+0j))\n",
      "Epoch 22800: <Test loss>: 8.259209607786033e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008586077), np.complex128(0.00012076299533405108+0j)) <f>: (np.float32(-0.00071245234), np.complex128(0.000609046210602887+0j))\n",
      "Epoch 23200: <Test loss>: 8.425686246482655e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085560195), np.complex128(0.0001272446630360434+0j)) <f>: (np.float32(-0.0006823985), np.complex128(0.0006010504371871807+0j))\n",
      "Epoch 23600: <Test loss>: 8.558215995435603e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008552091), np.complex128(0.00012504139473831923+0j)) <f>: (np.float32(-0.00067846617), np.complex128(0.0006018670392287638+0j))\n",
      "Epoch 24000: <Test loss>: 8.763439836911857e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008570828), np.complex128(0.00012816850197575855+0j)) <f>: (np.float32(-0.00069720496), np.complex128(0.0006054513601986085+0j))\n",
      "Epoch 24400: <Test loss>: 8.560613423469476e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008547545), np.complex128(0.0001231191386508424+0j)) <f>: (np.float32(-0.00067392545), np.complex128(0.0005989461994889145+0j))\n",
      "Epoch 24800: <Test loss>: 8.919807441998273e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008547693), np.complex128(0.00012709595856103667+0j)) <f>: (np.float32(-0.0006740763), np.complex128(0.0006029732007172808+0j))\n",
      "Epoch 25200: <Test loss>: 9.532276635582093e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008517117), np.complex128(0.0001386576185075373+0j)) <f>: (np.float32(-0.000643495), np.complex128(0.000598958568402858+0j))\n",
      "Epoch 25600: <Test loss>: 8.62996148498496e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085438), np.complex128(0.00012698506649025853+0j)) <f>: (np.float32(-0.0006701812), np.complex128(0.0005963515184952063+0j))\n",
      "Epoch 26000: <Test loss>: 8.963937034422997e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085651865), np.complex128(0.0001280660144234292+0j)) <f>: (np.float32(-0.00069156976), np.complex128(0.000598340566717978+0j))\n",
      "Epoch 26400: <Test loss>: 9.409363883605693e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008573676), np.complex128(0.00013182917720283138+0j)) <f>: (np.float32(-0.0007000654), np.complex128(0.0006048145831366655+0j))\n",
      "Epoch 26800: <Test loss>: 9.065726771950722e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008573795), np.complex128(0.00012471716268838858+0j)) <f>: (np.float32(-0.0007001742), np.complex128(0.0005989486732717031+0j))\n",
      "Epoch 27200: <Test loss>: 9.766840776137542e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008543998), np.complex128(0.00013258239648924504+0j)) <f>: (np.float32(-0.00067037554), np.complex128(0.0006001728786004708+0j))\n",
      "Epoch 27600: <Test loss>: 9.667049198469613e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008566076), np.complex128(0.0001308083057189876+0j)) <f>: (np.float32(-0.00069245376), np.complex128(0.0006013722826710235+0j))\n",
      "Epoch 28000: <Test loss>: 1.0027811185864266e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008563309), np.complex128(0.00013163930644500754+0j)) <f>: (np.float32(-0.0006896924), np.complex128(0.0006025287444095775+0j))\n",
      "Epoch 28400: <Test loss>: 9.851135473581962e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008537738), np.complex128(0.00012780384102057277+0j)) <f>: (np.float32(-0.0006641168), np.complex128(0.0005928194006852359+0j))\n",
      "Epoch 28800: <Test loss>: 1.0040462257165927e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008564106), np.complex128(0.00013283968575685196+0j)) <f>: (np.float32(-0.0006904894), np.complex128(0.0006060045360883589+0j))\n",
      "Epoch 29200: <Test loss>: 1.034486194839701e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008568416), np.complex128(0.00013384983751527803+0j)) <f>: (np.float32(-0.00069479475), np.complex128(0.0006001263207397809+0j))\n",
      "Epoch 29600: <Test loss>: 1.079970479622716e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008532637), np.complex128(0.00013708839976765572+0j)) <f>: (np.float32(-0.0006590129), np.complex128(0.0005851543529992806+0j))\n",
      "Epoch 30000: <Test loss>: 1.0596802894724533e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008552039), np.complex128(0.00013412271478597016+0j)) <f>: (np.float32(-0.0006784155), np.complex128(0.0006034730317012543+0j))\n",
      "Epoch 30400: <Test loss>: 1.0211665539827663e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008552612), np.complex128(0.00013223842967854168+0j)) <f>: (np.float32(-0.00067899487), np.complex128(0.0005983061240499199+0j))\n",
      "Epoch 30800: <Test loss>: 1.0392498552391771e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008559232), np.complex128(0.00013428755435064037+0j)) <f>: (np.float32(-0.0006856076), np.complex128(0.0006036040153283997+0j))\n",
      "Epoch 31200: <Test loss>: 1.0789183761517052e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008560289), np.complex128(0.00013515046053160176+0j)) <f>: (np.float32(-0.00068666873), np.complex128(0.0005911040543274191+0j))\n",
      "Epoch 31600: <Test loss>: 1.1007187822542619e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008571227), np.complex128(0.00013918914879788657+0j)) <f>: (np.float32(-0.00069760805), np.complex128(0.0006077958085486903+0j))\n",
      "Epoch 32000: <Test loss>: 1.126170809584437e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008563938), np.complex128(0.0001389135123079301+0j)) <f>: (np.float32(-0.0006903143), np.complex128(0.0006002398610267495+0j))\n",
      "Epoch 32400: <Test loss>: 1.1022734724974725e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008552769), np.complex128(0.0001384324884161835+0j)) <f>: (np.float32(-0.00067914696), np.complex128(0.0005946143521045865+0j))\n",
      "Epoch 32800: <Test loss>: 1.1099376024503727e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00856931), np.complex128(0.00013850676533017246+0j)) <f>: (np.float32(-0.00069569127), np.complex128(0.0006014126243595777+0j))\n",
      "Epoch 33200: <Test loss>: 1.1428969628468622e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00859395), np.complex128(0.00014125298137727065+0j)) <f>: (np.float32(-0.00072032964), np.complex128(0.0006000327610061056+0j))\n",
      "Epoch 33600: <Test loss>: 1.142871224146802e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008574703), np.complex128(0.00014658999331049224+0j)) <f>: (np.float32(-0.00070108834), np.complex128(0.0005921990520782231+0j))\n",
      "Epoch 34000: <Test loss>: 1.1657209142867941e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008566242), np.complex128(0.00014235668494058095+0j)) <f>: (np.float32(-0.00069261686), np.complex128(0.0006035913292628166+0j))\n",
      "Epoch 34400: <Test loss>: 1.191471892525442e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008577665), np.complex128(0.00014452652545549367+0j)) <f>: (np.float32(-0.0007040514), np.complex128(0.0006006366177278604+0j))\n",
      "Epoch 34800: <Test loss>: 1.1882479157065973e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008572686), np.complex128(0.00014333455859088905+0j)) <f>: (np.float32(-0.0006990649), np.complex128(0.0006035896166439629+0j))\n",
      "Epoch 35200: <Test loss>: 1.1975377674389165e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008576818), np.complex128(0.00014234298398975123+0j)) <f>: (np.float32(-0.00070319517), np.complex128(0.0006041975963370322+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25fa3117",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c11df465",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0003762592386920005 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 2.1752555767307058e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008527318), np.complex128(0.00013627893363796307+0j)) <f>: (np.float32(-0.00065369357), np.complex128(0.0005721314723749653+0j))\n",
      "Epoch 1600: <Test loss>: 1.887663165689446e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085477475), np.complex128(0.00013381989840050194+0j)) <f>: (np.float32(-0.0006741229), np.complex128(0.0005940476655549901+0j))\n",
      "Epoch 2400: <Test loss>: 1.6402718756580725e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008504394), np.complex128(0.00011636292070325917+0j)) <f>: (np.float32(-0.0006307752), np.complex128(0.0006002673263587368+0j))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3200: <Test loss>: 1.1711111255863216e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084332945), np.complex128(0.00010018829015910948+0j)) <f>: (np.float32(-0.0005596724), np.complex128(0.0005999651442765478+0j))\n",
      "Epoch 4000: <Test loss>: 1.1536608326423448e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008464999), np.complex128(0.00010321405159012192+0j)) <f>: (np.float32(-0.0005913751), np.complex128(0.0006120279434575352+0j))\n",
      "Epoch 4800: <Test loss>: 1.0086062502523419e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008367618), np.complex128(9.901415514544009e-05+0j)) <f>: (np.float32(-0.00049399625), np.complex128(0.0005918758745574941+0j))\n",
      "Epoch 5600: <Test loss>: 9.649710591475014e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0083829155), np.complex128(9.907521476484966e-05+0j)) <f>: (np.float32(-0.00050929654), np.complex128(0.0006018837848353334+0j))\n",
      "Epoch 6400: <Test loss>: 1.0221408047073055e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008388139), np.complex128(0.00010113863504710229+0j)) <f>: (np.float32(-0.0005145221), np.complex128(0.0006025839287948639+0j))\n",
      "Epoch 7200: <Test loss>: 9.525804671284277e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008392997), np.complex128(9.242497303763112e-05+0j)) <f>: (np.float32(-0.0005193716), np.complex128(0.0005952797996747472+0j))\n",
      "Epoch 8000: <Test loss>: 9.909771506499965e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008360305), np.complex128(0.00010128186072753532+0j)) <f>: (np.float32(-0.0004866821), np.complex128(0.0005974841938607919+0j))\n",
      "Epoch 8800: <Test loss>: 9.667272024671547e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008357979), np.complex128(0.00010404747059739363+0j)) <f>: (np.float32(-0.0004843501), np.complex128(0.0006010420643838958+0j))\n",
      "Epoch 9600: <Test loss>: 9.811578820517752e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008340841), np.complex128(9.978354123668118e-05+0j)) <f>: (np.float32(-0.00046721313), np.complex128(0.0006003914595104674+0j))\n",
      "Epoch 10400: <Test loss>: 9.855369171418715e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008355451), np.complex128(0.00010475934916758842+0j)) <f>: (np.float32(-0.00048182916), np.complex128(0.0005987187383330098+0j))\n",
      "Epoch 11200: <Test loss>: 9.872681403066963e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008354088), np.complex128(0.00010326078388421361+0j)) <f>: (np.float32(-0.00048046903), np.complex128(0.0005958110286710389+0j))\n",
      "Epoch 12000: <Test loss>: 1.0117521014763042e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008362221), np.complex128(0.00010896219512251831+0j)) <f>: (np.float32(-0.000488596), np.complex128(0.0006015198850440826+0j))\n",
      "Epoch 12800: <Test loss>: 1.012313168757828e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0083645675), np.complex128(0.00010374687427340245+0j)) <f>: (np.float32(-0.00049094687), np.complex128(0.0005995167552885136+0j))\n",
      "Epoch 13600: <Test loss>: 9.981076800613664e-06 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008369811), np.complex128(0.00010342185727316383+0j)) <f>: (np.float32(-0.00049619074), np.complex128(0.0005917775575492251+0j))\n",
      "Epoch 14400: <Test loss>: 1.1428891411924269e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008372441), np.complex128(0.00011456086508718203+0j)) <f>: (np.float32(-0.0004988289), np.complex128(0.0006093329156850647+0j))\n",
      "Epoch 15200: <Test loss>: 1.0676029887690675e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008388721), np.complex128(0.0001087896408442154+0j)) <f>: (np.float32(-0.0005150961), np.complex128(0.0006013930878185798+0j))\n",
      "Epoch 16000: <Test loss>: 1.1602463928284124e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008457032), np.complex128(8.394180098222262e-05+0j)) <f>: (np.float32(-0.00058341736), np.complex128(0.0005834176940512842+0j))\n",
      "Epoch 16800: <Test loss>: 1.1012428331014235e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008399407), np.complex128(0.0001104493033029134+0j)) <f>: (np.float32(-0.0005257861), np.complex128(0.0006022604975528231+0j))\n",
      "Epoch 17600: <Test loss>: 1.1340312994434498e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008383636), np.complex128(0.00011193959092849525+0j)) <f>: (np.float32(-0.0005100132), np.complex128(0.0006002436668464244+0j))\n",
      "Epoch 18400: <Test loss>: 1.3154331099940464e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008440125), np.complex128(0.0001279795588864805+0j)) <f>: (np.float32(-0.0005665044), np.complex128(0.0006061114796212242+0j))\n",
      "Epoch 19200: <Test loss>: 1.1048430678783916e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008382687), np.complex128(0.0001083252674135466+0j)) <f>: (np.float32(-0.00050906814), np.complex128(0.0005958766156301035+0j))\n",
      "Epoch 20000: <Test loss>: 1.1795642421930097e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008377122), np.complex128(0.00010967773679415021+0j)) <f>: (np.float32(-0.00050350744), np.complex128(0.0006017437306712962+0j))\n",
      "Epoch 20800: <Test loss>: 1.1113837899756618e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008386674), np.complex128(0.00010844696642644333+0j)) <f>: (np.float32(-0.0005130576), np.complex128(0.0005968096757537393+0j))\n",
      "Epoch 21600: <Test loss>: 1.1509063369885553e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008404097), np.complex128(0.00011423859144862575+0j)) <f>: (np.float32(-0.0005304746), np.complex128(0.0006015451937449209+0j))\n",
      "Epoch 22400: <Test loss>: 1.1743697541533038e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0083982125), np.complex128(0.00011608821981063945+0j)) <f>: (np.float32(-0.00052458985), np.complex128(0.0006024200882578584+0j))\n",
      "Epoch 23200: <Test loss>: 1.1743900358851533e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00840566), np.complex128(0.00011260598202478367+0j)) <f>: (np.float32(-0.00053204194), np.complex128(0.0005982989564228655+0j))\n",
      "Epoch 24000: <Test loss>: 1.2023994713672437e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008408016), np.complex128(0.00011760851791011627+0j)) <f>: (np.float32(-0.0005343937), np.complex128(0.0006032802035043915+0j))\n",
      "Epoch 24800: <Test loss>: 1.1965978046646342e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008403507), np.complex128(0.00011347536602798344+0j)) <f>: (np.float32(-0.0005298843), np.complex128(0.0006023370579586171+0j))\n",
      "Epoch 25600: <Test loss>: 1.2361678273009602e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084143365), np.complex128(0.00011968122280661752+0j)) <f>: (np.float32(-0.00054071716), np.complex128(0.0006085147913156117+0j))\n",
      "Epoch 26400: <Test loss>: 1.223085473611718e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008417317), np.complex128(0.00012050332742914732+0j)) <f>: (np.float32(-0.00054369774), np.complex128(0.000608101289007931+0j))\n",
      "Epoch 27200: <Test loss>: 1.2121607142034918e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008408847), np.complex128(0.00011677342592794579+0j)) <f>: (np.float32(-0.00053522724), np.complex128(0.0006063945057443828+0j))\n",
      "Epoch 28000: <Test loss>: 1.265166520170169e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008417877), np.complex128(0.000122115242708507+0j)) <f>: (np.float32(-0.0005442532), np.complex128(0.0006121773853101039+0j))\n",
      "Epoch 28800: <Test loss>: 1.2604462426679675e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008420101), np.complex128(0.00012160864846581949+0j)) <f>: (np.float32(-0.0005464756), np.complex128(0.0006093579706645914+0j))\n",
      "Epoch 29600: <Test loss>: 1.2594002328114584e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008416476), np.complex128(0.00012131953303118098+0j)) <f>: (np.float32(-0.00054285664), np.complex128(0.0006084622075737697+0j))\n",
      "Epoch 30400: <Test loss>: 1.2983205124328379e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008431642), np.complex128(0.00012235988555448578+0j)) <f>: (np.float32(-0.00055802014), np.complex128(0.0006146733052832478+0j))\n",
      "Epoch 31200: <Test loss>: 1.2859331036452204e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008417089), np.complex128(0.00012298595081980198+0j)) <f>: (np.float32(-0.0005434762), np.complex128(0.0006121475730559837+0j))\n",
      "Epoch 32000: <Test loss>: 1.2827465980080888e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008414591), np.complex128(0.00012200870354398205+0j)) <f>: (np.float32(-0.00054097193), np.complex128(0.0006111222218052308+0j))\n",
      "Epoch 32800: <Test loss>: 1.3027424756728578e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008415852), np.complex128(0.0001234165555296468+0j)) <f>: (np.float32(-0.00054222555), np.complex128(0.0006143894545658263+0j))\n",
      "Epoch 33600: <Test loss>: 1.3380374184635002e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084246), np.complex128(0.0001302572468164314+0j)) <f>: (np.float32(-0.00055097544), np.complex128(0.0006203838108448175+0j))\n",
      "Epoch 34400: <Test loss>: 1.3209733879193664e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008421742), np.complex128(0.00012622407698867526+0j)) <f>: (np.float32(-0.00054812676), np.complex128(0.0006158861565833186+0j))\n",
      "Epoch 35200: <Test loss>: 1.3362000572669785e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008427553), np.complex128(0.0001290236537991323+0j)) <f>: (np.float32(-0.0005539279), np.complex128(0.0006178825627241286+0j))\n",
      "Epoch 36000: <Test loss>: 1.3732801562582608e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008413453), np.complex128(0.00013399464895390894+0j)) <f>: (np.float32(-0.00053983345), np.complex128(0.0006287535060435852+0j))\n",
      "Epoch 36800: <Test loss>: 1.3699775081477128e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008431792), np.complex128(0.00012959600943428609+0j)) <f>: (np.float32(-0.0005581729), np.complex128(0.0006237858964825611+0j))\n",
      "Epoch 37600: <Test loss>: 1.3525355825549923e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008433819), np.complex128(0.00012118173857257573+0j)) <f>: (np.float32(-0.0005602017), np.complex128(0.000616968848850507+0j))\n",
      "Epoch 38400: <Test loss>: 1.4091332559473813e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008429734), np.complex128(0.00013288422177583958+0j)) <f>: (np.float32(-0.0005561104), np.complex128(0.0006200268883896373+0j))\n",
      "Epoch 39200: <Test loss>: 1.4037161236046813e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008436708), np.complex128(0.0001335354768101292+0j)) <f>: (np.float32(-0.00056308997), np.complex128(0.0006264988750378315+0j))\n",
      "Epoch 40000: <Test loss>: 1.37320275825914e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008426199), np.complex128(0.00013061023659128072+0j)) <f>: (np.float32(-0.00055257924), np.complex128(0.0006241530946508634+0j))\n",
      "Epoch 40800: <Test loss>: 1.4497526535706129e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008462191), np.complex128(0.00013746236912346306+0j)) <f>: (np.float32(-0.0005885759), np.complex128(0.0006289839484249019+0j))\n",
      "Epoch 41600: <Test loss>: 1.415482893207809e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008434574), np.complex128(0.00013261052783967553+0j)) <f>: (np.float32(-0.00056095124), np.complex128(0.0006255685424182961+0j))\n",
      "Epoch 42400: <Test loss>: 1.4466074389929418e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008430665), np.complex128(0.00013996566287222717+0j)) <f>: (np.float32(-0.0005570419), np.complex128(0.0006309537138279875+0j))\n",
      "Epoch 43200: <Test loss>: 1.4131468560663052e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008431942), np.complex128(0.0001336395025479105+0j)) <f>: (np.float32(-0.00055832486), np.complex128(0.0006217951356409356+0j))\n",
      "Epoch 44000: <Test loss>: 1.407456693414133e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008439478), np.complex128(0.00013458948271151777+0j)) <f>: (np.float32(-0.00056585204), np.complex128(0.0006249960837088595+0j))\n",
      "Epoch 44800: <Test loss>: 1.3970545296615455e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008398897), np.complex128(0.00012398615194553631+0j)) <f>: (np.float32(-0.000525277), np.complex128(0.0006082509211514835+0j))\n",
      "Epoch 45600: <Test loss>: 1.4308533536677714e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008434657), np.complex128(0.00013664281757163187+0j)) <f>: (np.float32(-0.00056103314), np.complex128(0.0006248053487128177+0j))\n",
      "Epoch 46400: <Test loss>: 1.3734040294366423e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008380876), np.complex128(0.0001302408262902923+0j)) <f>: (np.float32(-0.00050726277), np.complex128(0.000597759291192961+0j))\n",
      "Epoch 47200: <Test loss>: 1.4571014617104083e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084282765), np.complex128(0.00013584947860281169+0j)) <f>: (np.float32(-0.0005546551), np.complex128(0.0006260276511317479+0j))\n",
      "Epoch 48000: <Test loss>: 1.4689747331431136e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008434609), np.complex128(0.00013942233454088563+0j)) <f>: (np.float32(-0.0005609847), np.complex128(0.000623552028863537+0j))\n",
      "Epoch 48800: <Test loss>: 1.4738479876541533e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008434716), np.complex128(0.00013897790994834623+0j)) <f>: (np.float32(-0.00056109653), np.complex128(0.0006322813740215851+0j))\n",
      "Epoch 49600: <Test loss>: 1.538698052172549e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008465093), np.complex128(0.0001468983439920711+0j)) <f>: (np.float32(-0.0005914718), np.complex128(0.0006309596762788115+0j))\n",
      "Epoch 50400: <Test loss>: 1.51415197251481e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008439486), np.complex128(0.00014450787693908655+0j)) <f>: (np.float32(-0.00056586275), np.complex128(0.0006212775441651457+0j))\n",
      "Epoch 51200: <Test loss>: 1.6045984011725523e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008438807), np.complex128(0.0001345147776428154+0j)) <f>: (np.float32(-0.0005651867), np.complex128(0.0006184011056548372+0j))\n",
      "Epoch 52000: <Test loss>: 1.602958764124196e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008467052), np.complex128(0.00014682734959755176+0j)) <f>: (np.float32(-0.0005934245), np.complex128(0.0006424931493733447+0j))\n",
      "Epoch 52800: <Test loss>: 1.5317802535719238e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008439178), np.complex128(0.0001418922163644203+0j)) <f>: (np.float32(-0.00056555786), np.complex128(0.0006304105599300477+0j))\n",
      "Epoch 53600: <Test loss>: 1.531959969724994e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008436279), np.complex128(0.00014291059027910237+0j)) <f>: (np.float32(-0.0005626607), np.complex128(0.000632238748841226+0j))\n",
      "Epoch 54400: <Test loss>: 1.736254671413917e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008471018), np.complex128(0.00014660591432279901+0j)) <f>: (np.float32(-0.0005973989), np.complex128(0.0006404517076997148+0j))\n",
      "Epoch 55200: <Test loss>: 1.6199324818444438e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008428173), np.complex128(0.0001419190473931285+0j)) <f>: (np.float32(-0.0005545555), np.complex128(0.0006378585490338766+0j))\n",
      "Epoch 56000: <Test loss>: 1.5921137674013153e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008438913), np.complex128(0.00014395043535978384+0j)) <f>: (np.float32(-0.0005652922), np.complex128(0.0006336527377111165+0j))\n",
      "Epoch 56800: <Test loss>: 1.7655333067523316e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008442565), np.complex128(0.00015563596680791232+0j)) <f>: (np.float32(-0.0005689423), np.complex128(0.0006484412015129576+0j))\n",
      "Epoch 57600: <Test loss>: 1.5721467207185924e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008437643), np.complex128(0.00014199020036346764+0j)) <f>: (np.float32(-0.0005640231), np.complex128(0.0006286477676869502+0j))\n",
      "Epoch 58400: <Test loss>: 1.6884063370525837e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008455442), np.complex128(0.00015006250246980398+0j)) <f>: (np.float32(-0.0005818232), np.complex128(0.0006363504295573595+0j))\n",
      "Epoch 59200: <Test loss>: 1.6800582670839503e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00844737), np.complex128(0.00015030014419333912+0j)) <f>: (np.float32(-0.00057375), np.complex128(0.0006426608591603531+0j))\n",
      "Epoch 60000: <Test loss>: 1.617517409613356e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008453487), np.complex128(0.0001479650042438788+0j)) <f>: (np.float32(-0.0005798671), np.complex128(0.0006332063784935758+0j))\n",
      "Epoch 60800: <Test loss>: 1.6464457075926475e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008437491), np.complex128(0.00014838829067964038+0j)) <f>: (np.float32(-0.0005638726), np.complex128(0.0006355817808436804+0j))\n",
      "Epoch 61600: <Test loss>: 1.713044366624672e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008427396), np.complex128(0.00015390358940705026+0j)) <f>: (np.float32(-0.00055377855), np.complex128(0.0006460586315357985+0j))\n",
      "Epoch 62400: <Test loss>: 1.6764883184805512e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008434492), np.complex128(0.0001484416038702533+0j)) <f>: (np.float32(-0.000560871), np.complex128(0.0006392578220676908+0j))\n",
      "Epoch 63200: <Test loss>: 1.691855868557468e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008436225), np.complex128(0.00015065755823356054+0j)) <f>: (np.float32(-0.0005625988), np.complex128(0.0006376439642345387+0j))\n",
      "Epoch 64000: <Test loss>: 1.73466069099959e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008447897), np.complex128(0.0001522817552101628+0j)) <f>: (np.float32(-0.00057428284), np.complex128(0.0006398492464451741+0j))\n",
      "Epoch 64800: <Test loss>: 1.740004518069327e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008436701), np.complex128(0.00015154835790122273+0j)) <f>: (np.float32(-0.00056307943), np.complex128(0.0006424441811601941+0j))\n",
      "Epoch 65600: <Test loss>: 1.7387017578585073e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008444634), np.complex128(0.00015144756711016515+0j)) <f>: (np.float32(-0.0005710165), np.complex128(0.0006388965229198845+0j))\n",
      "Epoch 66400: <Test loss>: 1.7352656868752092e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008432039), np.complex128(0.00014967480044800294+0j)) <f>: (np.float32(-0.00055842625), np.complex128(0.0006335272725224998+0j))\n",
      "Epoch 67200: <Test loss>: 1.7340153135592118e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008436771), np.complex128(0.00015180441027743533+0j)) <f>: (np.float32(-0.00056314556), np.complex128(0.0006399639919083732+0j))\n",
      "Epoch 68000: <Test loss>: 1.742783388181124e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008440429), np.complex128(0.00015019616602830375+0j)) <f>: (np.float32(-0.00056682026), np.complex128(0.0006377972119067823+0j))\n",
      "Epoch 68800: <Test loss>: 1.8658034605323337e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008422255), np.complex128(0.00015537157334357884+0j)) <f>: (np.float32(-0.00054862734), np.complex128(0.0006492943394234201+0j))\n",
      "Epoch 69600: <Test loss>: 1.7997790564550087e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084278425), np.complex128(0.00015234775446635882+0j)) <f>: (np.float32(-0.00055422523), np.complex128(0.0006408442145688554+0j))\n",
      "Epoch 70400: <Test loss>: 1.8299360817763954e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008438831), np.complex128(0.00015606288462994704+0j)) <f>: (np.float32(-0.0005652127), np.complex128(0.0006457242902773564+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f2fc60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e46564d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0003762592386920005 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 4.469730265554972e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008447994), np.complex128(0.0002947130342163723+0j)) <f>: (np.float32(-0.00057436724), np.complex128(0.0005411321830981079+0j))\n",
      "Epoch 3200: <Test loss>: 4.818059460376389e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008561652), np.complex128(0.0002580028112232319+0j)) <f>: (np.float32(-0.00068802614), np.complex128(0.0005794928790811896+0j))\n",
      "Epoch 4800: <Test loss>: 3.511136310407892e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008586862), np.complex128(0.00020369982205836374+0j)) <f>: (np.float32(-0.000713238), np.complex128(0.0005560948265804046+0j))\n",
      "Epoch 6400: <Test loss>: 3.119329994660802e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008571655), np.complex128(0.00018748553963047595+0j)) <f>: (np.float32(-0.0006980393), np.complex128(0.0005690639182866476+0j))\n",
      "Epoch 8000: <Test loss>: 2.7978927391814068e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008591249), np.complex128(0.0001749549417811123+0j)) <f>: (np.float32(-0.0007176217), np.complex128(0.0005754995593569463+0j))\n",
      "Epoch 9600: <Test loss>: 2.5138362616416998e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008598998), np.complex128(0.0001639731222484595+0j)) <f>: (np.float32(-0.0007253757), np.complex128(0.0005917397530737876+0j))\n",
      "Epoch 11200: <Test loss>: 2.5644645575084724e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008605556), np.complex128(0.00017176084418860332+0j)) <f>: (np.float32(-0.000731937), np.complex128(0.0006016750990564917+0j))\n",
      "Epoch 12800: <Test loss>: 2.402544669166673e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008601317), np.complex128(0.0001667046224594813+0j)) <f>: (np.float32(-0.0007276991), np.complex128(0.0006049588237023452+0j))\n",
      "Epoch 14400: <Test loss>: 2.3436290575773455e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00860041), np.complex128(0.00016134978660411548+0j)) <f>: (np.float32(-0.000726794), np.complex128(0.0006047247657923373+0j))\n",
      "Epoch 16000: <Test loss>: 2.3505404897150584e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008597013), np.complex128(0.00016718564635122792+0j)) <f>: (np.float32(-0.0007233953), np.complex128(0.0006126125173596037+0j))\n",
      "Epoch 17600: <Test loss>: 2.400195262453053e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008602209), np.complex128(0.00017407543271181903+0j)) <f>: (np.float32(-0.0007285826), np.complex128(0.000594961252567956+0j))\n",
      "Epoch 19200: <Test loss>: 2.3608918127138168e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008609897), np.complex128(0.00018141543168237185+0j)) <f>: (np.float32(-0.00073628075), np.complex128(0.0006294724253801786+0j))\n",
      "Epoch 20800: <Test loss>: 2.3696462449152023e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008619554), np.complex128(0.00018639639332742222+0j)) <f>: (np.float32(-0.0007459355), np.complex128(0.0006363651453934359+0j))\n",
      "Epoch 22400: <Test loss>: 2.3432643502019346e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008620016), np.complex128(0.00018782658864609518+0j)) <f>: (np.float32(-0.0007464088), np.complex128(0.0006377167822509855+0j))\n",
      "Epoch 24000: <Test loss>: 2.3827933546272106e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0086465655), np.complex128(0.00018953563954386633+0j)) <f>: (np.float32(-0.0007729545), np.complex128(0.000643449171275686+0j))\n",
      "Epoch 25600: <Test loss>: 2.415003109490499e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008590808), np.complex128(0.00019677093090061275+0j)) <f>: (np.float32(-0.0007171892), np.complex128(0.000650781526891725+0j))\n",
      "Epoch 27200: <Test loss>: 2.384631079621613e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008602659), np.complex128(0.00019259983182480452+0j)) <f>: (np.float32(-0.0007290481), np.complex128(0.0006513241733470415+0j))\n",
      "Epoch 28800: <Test loss>: 2.4096638298942707e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008614977), np.complex128(0.00020040341134692718+0j)) <f>: (np.float32(-0.0007413603), np.complex128(0.0006556988362027122+0j))\n",
      "Epoch 30400: <Test loss>: 2.4253391529782675e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008594779), np.complex128(0.00020188387520047313+0j)) <f>: (np.float32(-0.0007211519), np.complex128(0.000653188136963164+0j))\n",
      "Epoch 32000: <Test loss>: 2.440491152810864e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008583749), np.complex128(0.0002064125944650685+0j)) <f>: (np.float32(-0.0007101328), np.complex128(0.0006604977845218093+0j))\n",
      "Epoch 33600: <Test loss>: 2.4679276975803077e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008569837), np.complex128(0.00020216999355211762+0j)) <f>: (np.float32(-0.0006962186), np.complex128(0.0006540695647998768+0j))\n",
      "Epoch 35200: <Test loss>: 2.4979990485007875e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085657025), np.complex128(0.00020592713045036783+0j)) <f>: (np.float32(-0.00069208146), np.complex128(0.0006535985946151047+0j))\n",
      "Epoch 36800: <Test loss>: 2.4864331862772815e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008547942), np.complex128(0.00020519433572954295+0j)) <f>: (np.float32(-0.0006743193), np.complex128(0.0006531509033606776+0j))\n",
      "Epoch 38400: <Test loss>: 2.580634281912353e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008556406), np.complex128(0.00020839357117861308+0j)) <f>: (np.float32(-0.00068278547), np.complex128(0.0006620000684081581+0j))\n",
      "Epoch 40000: <Test loss>: 2.9898159482399933e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008540723), np.complex128(0.0002180342040002402+0j)) <f>: (np.float32(-0.0006671029), np.complex128(0.0006684705961588103+0j))\n",
      "Epoch 41600: <Test loss>: 2.5505371013423428e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085607), np.complex128(0.00021072049690060835+0j)) <f>: (np.float32(-0.00068708), np.complex128(0.0006591521735454115+0j))\n",
      "Epoch 43200: <Test loss>: 2.679285353224259e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008555595), np.complex128(0.0002198469317689891+0j)) <f>: (np.float32(-0.00068197167), np.complex128(0.0006729448446293083+0j))\n",
      "Epoch 44800: <Test loss>: 2.615016455820296e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008510723), np.complex128(0.00021346292298561357+0j)) <f>: (np.float32(-0.0006370981), np.complex128(0.0006549908268825203+0j))\n",
      "Epoch 46400: <Test loss>: 2.6573528884910047e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008499082), np.complex128(0.00021080056183201958+0j)) <f>: (np.float32(-0.00062546064), np.complex128(0.0006557306147969978+0j))\n",
      "Epoch 48000: <Test loss>: 2.810401565511711e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008500439), np.complex128(0.00021974986750969655+0j)) <f>: (np.float32(-0.00062682363), np.complex128(0.0006619527493835332+0j))\n",
      "Epoch 49600: <Test loss>: 2.726740603975486e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0085047325), np.complex128(0.00021844475679767277+0j)) <f>: (np.float32(-0.0006311092), np.complex128(0.0006591103729593153+0j))\n",
      "Epoch 51200: <Test loss>: 2.774394852167461e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00848111), np.complex128(0.0002182409868692445+0j)) <f>: (np.float32(-0.0006074874), np.complex128(0.0006587424770574059+0j))\n",
      "Epoch 52800: <Test loss>: 2.8206097340444103e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084967185), np.complex128(0.00021931503675425435+0j)) <f>: (np.float32(-0.0006231023), np.complex128(0.0006645218045247636+0j))\n",
      "Epoch 54400: <Test loss>: 3.0057013646000996e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008511541), np.complex128(0.00022698793394328927+0j)) <f>: (np.float32(-0.0006379172), np.complex128(0.000673063586203166+0j))\n",
      "Epoch 56000: <Test loss>: 2.889174902520608e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008483781), np.complex128(0.0002234284459468235+0j)) <f>: (np.float32(-0.0006101702), np.complex128(0.0006626428079209251+0j))\n",
      "Epoch 57600: <Test loss>: 2.806464908644557e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008497915), np.complex128(0.0002181870552329344+0j)) <f>: (np.float32(-0.0006242923), np.complex128(0.0006547801747635132+0j))\n",
      "Epoch 59200: <Test loss>: 2.9031074518570676e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008451691), np.complex128(0.0002180384855473745+0j)) <f>: (np.float32(-0.00057806907), np.complex128(0.0006499259786288018+0j))\n",
      "Epoch 60800: <Test loss>: 3.0024870284250937e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008465987), np.complex128(0.0002240905792823506+0j)) <f>: (np.float32(-0.00059236976), np.complex128(0.0006563973944040448+0j))\n",
      "Epoch 62400: <Test loss>: 3.315750655019656e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008543122), np.complex128(0.00023994273796816928+0j)) <f>: (np.float32(-0.00066949794), np.complex128(0.0006729914659203261+0j))\n",
      "Epoch 64000: <Test loss>: 2.998702802869957e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008470917), np.complex128(0.00022908860368561022+0j)) <f>: (np.float32(-0.0005972987), np.complex128(0.0006621368241951438+0j))\n",
      "Epoch 65600: <Test loss>: 3.190202914993279e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008471421), np.complex128(0.00023785781480475333+0j)) <f>: (np.float32(-0.000597801), np.complex128(0.0006721694723008702+0j))\n",
      "Epoch 67200: <Test loss>: 3.0503433663398027e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008450402), np.complex128(0.00022539088514703478+0j)) <f>: (np.float32(-0.0005767792), np.complex128(0.0006529960699302361+0j))\n",
      "Epoch 68800: <Test loss>: 3.0059956770855933e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008452059), np.complex128(0.00022243959884978603+0j)) <f>: (np.float32(-0.00057843805), np.complex128(0.0006515976849210128+0j))\n",
      "Epoch 70400: <Test loss>: 3.1647534342482686e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008468417), np.complex128(0.0002367033352639467+0j)) <f>: (np.float32(-0.00059480395), np.complex128(0.0006665520190303525+0j))\n",
      "Epoch 72000: <Test loss>: 3.265436316723935e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008479422), np.complex128(0.00023727192472338055+0j)) <f>: (np.float32(-0.00060580106), np.complex128(0.0006660031564029004+0j))\n",
      "Epoch 73600: <Test loss>: 3.234722316847183e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008441843), np.complex128(0.00023472042392540073+0j)) <f>: (np.float32(-0.00056822004), np.complex128(0.0006608375173581243+0j))\n",
      "Epoch 75200: <Test loss>: 3.195339013473131e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008450435), np.complex128(0.0002332051526794112+0j)) <f>: (np.float32(-0.0005768179), np.complex128(0.0006569717560233189+0j))\n",
      "Epoch 76800: <Test loss>: 3.2333755370927975e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008437224), np.complex128(0.00023616441534039532+0j)) <f>: (np.float32(-0.000563598), np.complex128(0.0006600797152305187+0j))\n",
      "Epoch 78400: <Test loss>: 3.220219150534831e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008424219), np.complex128(0.00022985090936649777+0j)) <f>: (np.float32(-0.00055060076), np.complex128(0.0006522758185567565+0j))\n",
      "Epoch 80000: <Test loss>: 3.2962143450276926e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084256), np.complex128(0.00023479294064779004+0j)) <f>: (np.float32(-0.0005519689), np.complex128(0.0006573919819457585+0j))\n",
      "Epoch 81600: <Test loss>: 3.323704731883481e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008424566), np.complex128(0.00023427379512896626+0j)) <f>: (np.float32(-0.0005509412), np.complex128(0.0006578817909379214+0j))\n",
      "Epoch 83200: <Test loss>: 3.330553590785712e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008433827), np.complex128(0.00023818754150683967+0j)) <f>: (np.float32(-0.00056020316), np.complex128(0.0006611879064895291+0j))\n",
      "Epoch 84800: <Test loss>: 3.353929059812799e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008406559), np.complex128(0.0002312746189341414+0j)) <f>: (np.float32(-0.0005329335), np.complex128(0.0006560568369734669+0j))\n",
      "Epoch 86400: <Test loss>: 3.8498306821566075e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008439204), np.complex128(0.0002518765991500866+0j)) <f>: (np.float32(-0.000565574), np.complex128(0.0006802655290650109+0j))\n",
      "Epoch 88000: <Test loss>: 3.5029657738050446e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008480075), np.complex128(0.0002509327717282871+0j)) <f>: (np.float32(-0.00060645543), np.complex128(0.0006751418174669869+0j))\n",
      "Epoch 89600: <Test loss>: 3.4394175600027665e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008435608), np.complex128(0.00024136481420486907+0j)) <f>: (np.float32(-0.00056198286), np.complex128(0.0006621959412607609+0j))\n",
      "Epoch 91200: <Test loss>: 3.4531385608715937e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00840215), np.complex128(0.00023912087120936916+0j)) <f>: (np.float32(-0.0005285319), np.complex128(0.0006523280217166309+0j))\n",
      "Epoch 92800: <Test loss>: 3.5851211578119546e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008455786), np.complex128(0.00024307086801964622+0j)) <f>: (np.float32(-0.0005821675), np.complex128(0.0006621982881828938+0j))\n",
      "Epoch 94400: <Test loss>: 3.5295186535222456e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008393512), np.complex128(0.0002400911490779094+0j)) <f>: (np.float32(-0.00051988655), np.complex128(0.0006512438705519004+0j))\n",
      "Epoch 96000: <Test loss>: 3.5973189369542524e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008411301), np.complex128(0.00024472831834565815+0j)) <f>: (np.float32(-0.00053768937), np.complex128(0.0006577183309828834+0j))\n",
      "Epoch 97600: <Test loss>: 3.653512249002233e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008457901), np.complex128(0.00025592627672068273+0j)) <f>: (np.float32(-0.00058428716), np.complex128(0.0006731269531007535+0j))\n",
      "Epoch 99200: <Test loss>: 3.645797187346034e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008406287), np.complex128(0.0002491190766470375+0j)) <f>: (np.float32(-0.0005326721), np.complex128(0.0006618262693096698+0j))\n",
      "Epoch 100800: <Test loss>: 3.653249586932361e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008420257), np.complex128(0.0002509952347437018+0j)) <f>: (np.float32(-0.00054663717), np.complex128(0.0006646960476355473+0j))\n",
      "Epoch 102400: <Test loss>: 3.747902883333154e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008395631), np.complex128(0.00025203113128647057+0j)) <f>: (np.float32(-0.0005220094), np.complex128(0.0006610100478500543+0j))\n",
      "Epoch 104000: <Test loss>: 3.773088246816769e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0084073525), np.complex128(0.00025152754205556805+0j)) <f>: (np.float32(-0.0005337364), np.complex128(0.0006624480133838968+0j))\n",
      "Epoch 105600: <Test loss>: 3.7487217923626304e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0083752815), np.complex128(0.0002495148343204838+0j)) <f>: (np.float32(-0.00050166005), np.complex128(0.0006549562573538064+0j))\n",
      "Epoch 107200: <Test loss>: 3.7658639485016465e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008391331), np.complex128(0.00025530832260854875+0j)) <f>: (np.float32(-0.00051771075), np.complex128(0.0006615415939979854+0j))\n",
      "Epoch 108800: <Test loss>: 3.771704723476432e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008402429), np.complex128(0.0002524364352242681+0j)) <f>: (np.float32(-0.0005288048), np.complex128(0.0006587065754918057+0j))\n",
      "Epoch 110400: <Test loss>: 3.871970693580806e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008428747), np.complex128(0.0002518078565322083+0j)) <f>: (np.float32(-0.000555129), np.complex128(0.0006662963948088534+0j))\n",
      "Epoch 112000: <Test loss>: 3.823849692707881e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00838325), np.complex128(0.0002496142772170732+0j)) <f>: (np.float32(-0.00050963036), np.complex128(0.0006522569163190377+0j))\n",
      "Epoch 113600: <Test loss>: 3.7717083614552394e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008385564), np.complex128(0.0002475512058016241+0j)) <f>: (np.float32(-0.0005119413), np.complex128(0.00065301421100402+0j))\n",
      "Epoch 115200: <Test loss>: 3.7938902096357197e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008396463), np.complex128(0.0002478426364432315+0j)) <f>: (np.float32(-0.0005228393), np.complex128(0.0006444646908256119+0j))\n",
      "Epoch 116800: <Test loss>: 4.1745981434360147e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00839481), np.complex128(0.00027758738483699575+0j)) <f>: (np.float32(-0.0005211891), np.complex128(0.0006961821646791556+0j))\n",
      "Epoch 118400: <Test loss>: 3.900338924722746e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008366799), np.complex128(0.00025484003235513115+0j)) <f>: (np.float32(-0.00049318216), np.complex128(0.0006536640547135134+0j))\n",
      "Epoch 120000: <Test loss>: 4.5421118556987494e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008430059), np.complex128(0.0002962773212482585+0j)) <f>: (np.float32(-0.0005564372), np.complex128(0.0007098874922015128+0j))\n",
      "Epoch 121600: <Test loss>: 4.0242262912215665e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.00838287), np.complex128(0.00025626412250474227+0j)) <f>: (np.float32(-0.00050925615), np.complex128(0.0006590862694347074+0j))\n",
      "Epoch 123200: <Test loss>: 3.9821909012971446e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0083775995), np.complex128(0.00025647450504485573+0j)) <f>: (np.float32(-0.0005039726), np.complex128(0.0006581695109053458+0j))\n",
      "Epoch 124800: <Test loss>: 3.97732037527021e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008351001), np.complex128(0.00025780122964111666+0j)) <f>: (np.float32(-0.00047737968), np.complex128(0.0006504131869775201+0j))\n",
      "Epoch 126400: <Test loss>: 4.0118560718838125e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008369442), np.complex128(0.0002630727021880197+0j)) <f>: (np.float32(-0.0004958222), np.complex128(0.0006631349638352208+0j))\n",
      "Epoch 128000: <Test loss>: 4.124766564927995e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0083858175), np.complex128(0.00026087956688518+0j)) <f>: (np.float32(-0.0005122039), np.complex128(0.0006648551943282871+0j))\n",
      "Epoch 129600: <Test loss>: 4.0468738006893545e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008360054), np.complex128(0.0002578333888173698+0j)) <f>: (np.float32(-0.00048643988), np.complex128(0.0006556054667600207+0j))\n",
      "Epoch 131200: <Test loss>: 4.137068390264176e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008377255), np.complex128(0.00026375067724794364+0j)) <f>: (np.float32(-0.00050363), np.complex128(0.0006639174403603855+0j))\n",
      "Epoch 132800: <Test loss>: 4.0956103475764394e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008361624), np.complex128(0.00026091367654401654+0j)) <f>: (np.float32(-0.0004879992), np.complex128(0.0006585908151433601+0j))\n",
      "Epoch 134400: <Test loss>: 4.093112875125371e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008364707), np.complex128(0.0002633552525837189+0j)) <f>: (np.float32(-0.0004910882), np.complex128(0.0006587184369631259+0j))\n",
      "Epoch 136000: <Test loss>: 4.144263584748842e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008354424), np.complex128(0.0002618425978387559+0j)) <f>: (np.float32(-0.00048080328), np.complex128(0.0006597191138163196+0j))\n",
      "Epoch 137600: <Test loss>: 4.170836109551601e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008346148), np.complex128(0.00026105368313530784+0j)) <f>: (np.float32(-0.00047253084), np.complex128(0.000657288907662896+0j))\n",
      "Epoch 139200: <Test loss>: 4.1558760131010786e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008359282), np.complex128(0.0002620816191719232+0j)) <f>: (np.float32(-0.00048566237), np.complex128(0.0006593065629635576+0j))\n",
      "Epoch 140800: <Test loss>: 4.177433584118262e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008375577), np.complex128(0.00026370522941799224+0j)) <f>: (np.float32(-0.0005019602), np.complex128(0.0006631625560278641+0j))\n",
      "Epoch 142400: <Test loss>: 4.355134296929464e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.0083467), np.complex128(0.00026610915955807687+0j)) <f>: (np.float32(-0.00047307985), np.complex128(0.0006623118284698624+0j))\n",
      "Epoch 144000: <Test loss>: 4.465713573154062e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008346017), np.complex128(0.00027246506870618587+0j)) <f>: (np.float32(-0.0004723922), np.complex128(0.0006686055758966143+0j))\n",
      "Epoch 145600: <Test loss>: 4.271780198905617e-05 <O>: (np.float32(0.007873619), np.complex128(0.0005754639749429857+0j)) <O-f>: (np.float32(0.008351269), np.complex128(0.00026427445318070525+0j)) <f>: (np.float32(-0.00047765297), np.complex128(0.0006598746449803682+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a2f0a",
   "metadata": {},
   "source": [
    "## m^2=0.01, lambda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b6ae14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.06536658), np.complex128(0.0002904810215792188+0j))\n",
      "bin size 1: (np.float32(0.06536658), np.complex128(0.0002904963716244088+0j))\n",
      "jack bin size 2: (np.float32(0.06536658), np.complex128(0.0004102384207644181+0j))\n",
      "bin size 2: (np.float32(0.06536658), np.complex128(0.00041023336043415065+0j))\n",
      "jack bin size 4: (np.float32(0.06536658), np.complex128(0.0005789388941891799+0j))\n",
      "bin size 4: (np.float32(0.06536658), np.complex128(0.0005789581303401094+0j))\n",
      "jack bin size 5: (np.float32(0.06536658), np.complex128(0.0006466569185993317+0j))\n",
      "bin size 5: (np.float32(0.06536658), np.complex128(0.0006466596690625109+0j))\n",
      "jack bin size 10: (np.float32(0.06536658), np.complex128(0.000910345215278272+0j))\n",
      "bin size 10: (np.float32(0.06536658), np.complex128(0.0009103707935491196+0j))\n",
      "jack bin size 20: (np.float32(0.06536658), np.complex128(0.0012766607220471241+0j))\n",
      "bin size 20: (np.float32(0.06536658), np.complex128(0.0012766607780804599+0j))\n",
      "jack bin size 50: (np.float32(0.06536658), np.complex128(0.001968069082578528+0j))\n",
      "bin size 50: (np.float32(0.06536658), np.complex128(0.001968070453222887+0j))\n",
      "jack bin size 100: (np.float32(0.06536658), np.complex128(0.0026800996177997103+0j))\n",
      "bin size 100: (np.float32(0.06536658), np.complex128(0.002680103734025786+0j))\n",
      "jack bin size 200: (np.float32(0.06536658), np.complex128(0.0035510651508577763+0j))\n",
      "bin size 200: (np.float32(0.06536658), np.complex128(0.0035510689369874903+0j))\n",
      "jack bin size 500: (np.float32(0.06536658), np.complex128(0.004613795039656605+0j))\n",
      "bin size 500: (np.float32(0.06536658), np.complex128(0.00461379672315913+0j))\n",
      "jack bin size 1000: (np.float32(0.06536658), np.complex128(0.0054049084741826115+0j))\n",
      "bin size 1000: (np.float32(0.06536658), np.complex128(0.005404913347307611+0j))\n",
      "jack bin size 2000: (np.float32(0.06536658), np.complex128(0.005933516134973615+0j))\n",
      "bin size 2000: (np.float32(0.06536658), np.complex128(0.005933510937861033+0j))\n",
      "jack bin size 5000: (np.float32(0.06536658), np.complex128(0.005837449437990655+0j))\n",
      "bin size 5000: (np.float32(0.06536658), np.complex128(0.005837447728710238+0j))\n",
      "jack bin size 10000: (np.float32(0.06536658), np.complex128(0.005942517891526222+0j))\n",
      "bin size 10000: (np.float32(0.06536658), np.complex128(0.005942516649762789+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_8x8x8_0.01_0.1_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(8,8,8), m2=0.01, lamda=0.1)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f348e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee9703a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.00047336885472759604 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07330316), np.complex128(0.0010957713470831816+0j)) <f>: (np.float32(-0.0011749377), np.complex128(0.007947544039015254+0j))\n",
      "Epoch 200: <Test loss>: 0.0003100926987826824 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07351098), np.complex128(0.0008466927548429361+0j)) <f>: (np.float32(-0.0013827353), np.complex128(0.008162151165828552+0j))\n",
      "Epoch 300: <Test loss>: 0.00023666808556299657 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0737836), np.complex128(0.0006805291455078274+0j)) <f>: (np.float32(-0.0016553105), np.complex128(0.008381311067858306+0j))\n",
      "Epoch 400: <Test loss>: 0.00021514690888579935 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073683254), np.complex128(0.0006397689436500332+0j)) <f>: (np.float32(-0.0015549998), np.complex128(0.008429913414877595+0j))\n",
      "Epoch 500: <Test loss>: 0.0001945744879776612 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389396), np.complex128(0.0006908582035359026+0j)) <f>: (np.float32(-0.0017657032), np.complex128(0.008323332196045247+0j))\n",
      "Epoch 600: <Test loss>: 0.00016909290570765734 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0738153), np.complex128(0.0006028510339057158+0j)) <f>: (np.float32(-0.0016870467), np.complex128(0.008462161393589795+0j))\n",
      "Epoch 700: <Test loss>: 0.00015038528363220394 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073783144), np.complex128(0.0004934065700712654+0j)) <f>: (np.float32(-0.0016549904), np.complex128(0.008578767141445898+0j))\n",
      "Epoch 800: <Test loss>: 0.00014846495469100773 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073802404), np.complex128(0.0005761663389639932+0j)) <f>: (np.float32(-0.0016741473), np.complex128(0.008468259839036896+0j))\n",
      "Epoch 900: <Test loss>: 0.00013704394223168492 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0738586), np.complex128(0.0005487648813168265+0j)) <f>: (np.float32(-0.0017303244), np.complex128(0.008500756464634525+0j))\n",
      "Epoch 1000: <Test loss>: 0.00013357984425965697 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073855095), np.complex128(0.0005466072987127834+0j)) <f>: (np.float32(-0.0017268172), np.complex128(0.00849109678685694+0j))\n",
      "Epoch 1100: <Test loss>: 0.0001121656023315154 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07381983), np.complex128(0.00041371118375045966+0j)) <f>: (np.float32(-0.0016915849), np.complex128(0.008727078411849866+0j))\n",
      "Epoch 1200: <Test loss>: 9.960781608242542e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073816165), np.complex128(0.00036584440652884145+0j)) <f>: (np.float32(-0.0016879531), np.complex128(0.008784171796285226+0j))\n",
      "Epoch 1300: <Test loss>: 9.950702224159613e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07391162), np.complex128(0.00036178870307709333+0j)) <f>: (np.float32(-0.0017834137), np.complex128(0.008765601426042083+0j))\n",
      "Epoch 1400: <Test loss>: 9.002952720038593e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073726244), np.complex128(0.0003796653675389885+0j)) <f>: (np.float32(-0.0015979625), np.complex128(0.008729130509818587+0j))\n",
      "Epoch 1500: <Test loss>: 8.418344805249944e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0737844), np.complex128(0.00034533696921665416+0j)) <f>: (np.float32(-0.0016561623), np.complex128(0.008763842629909644+0j))\n",
      "Epoch 1600: <Test loss>: 8.671561954542994e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07366282), np.complex128(0.00039146137876076365+0j)) <f>: (np.float32(-0.0015345908), np.complex128(0.008651321287728672+0j))\n",
      "Epoch 1700: <Test loss>: 8.283387433039024e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073757514), np.complex128(0.0003828112263671178+0j)) <f>: (np.float32(-0.0016292898), np.complex128(0.008663683604918078+0j))\n",
      "Epoch 1800: <Test loss>: 8.15254679764621e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073711626), np.complex128(0.0003912025513077048+0j)) <f>: (np.float32(-0.0015834698), np.complex128(0.00863608075597978+0j))\n",
      "Epoch 1900: <Test loss>: 7.174158236011863e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07372895), np.complex128(0.0003595601736512768+0j)) <f>: (np.float32(-0.0016007683), np.complex128(0.008693354789989042+0j))\n",
      "Epoch 2000: <Test loss>: 7.141885726014152e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07370027), np.complex128(0.00036158455256669756+0j)) <f>: (np.float32(-0.0015720901), np.complex128(0.008699315211042597+0j))\n",
      "Epoch 2100: <Test loss>: 6.920457963133231e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07372484), np.complex128(0.000335823086047784+0j)) <f>: (np.float32(-0.0015966035), np.complex128(0.00868343124204733+0j))\n",
      "Epoch 2200: <Test loss>: 6.876746192574501e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073685326), np.complex128(0.0003578176791131123+0j)) <f>: (np.float32(-0.0015570879), np.complex128(0.008648774940644836+0j))\n",
      "Epoch 2300: <Test loss>: 6.348012539092451e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073775396), np.complex128(0.000327353773519025+0j)) <f>: (np.float32(-0.0016471432), np.complex128(0.008711481655379398+0j))\n",
      "Epoch 2400: <Test loss>: 6.396829121513292e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073706515), np.complex128(0.0003734223643745689+0j)) <f>: (np.float32(-0.0015782121), np.complex128(0.008671794567809279+0j))\n",
      "Epoch 2500: <Test loss>: 6.614856829401106e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073675014), np.complex128(0.00042502620279565213+0j)) <f>: (np.float32(-0.0015467857), np.complex128(0.008580055030823892+0j))\n",
      "Epoch 2600: <Test loss>: 5.82913780817762e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07370509), np.complex128(0.0003213264334695179+0j)) <f>: (np.float32(-0.0015768519), np.complex128(0.00874401989127214+0j))\n",
      "Epoch 2700: <Test loss>: 6.05775676376652e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073703155), np.complex128(0.00035876326672651144+0j)) <f>: (np.float32(-0.0015749652), np.complex128(0.008626191714136453+0j))\n",
      "Epoch 2800: <Test loss>: 5.5905155022628605e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07380841), np.complex128(0.0003421996734827934+0j)) <f>: (np.float32(-0.0016802263), np.complex128(0.008666977922428694+0j))\n",
      "Epoch 2900: <Test loss>: 7.388531230390072e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07364968), np.complex128(0.0005076368787026147+0j)) <f>: (np.float32(-0.001521434), np.complex128(0.008452268799648107+0j))\n",
      "Epoch 3000: <Test loss>: 6.336459773592651e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07365533), np.complex128(0.00045387866090904815+0j)) <f>: (np.float32(-0.0015270783), np.complex128(0.008523309752028132+0j))\n",
      "Epoch 3100: <Test loss>: 5.883019548491575e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07362708), np.complex128(0.00042642931336430524+0j)) <f>: (np.float32(-0.0014988492), np.complex128(0.008562348327925646+0j))\n",
      "Epoch 3200: <Test loss>: 5.431132376543246e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07375549), np.complex128(0.00039195477156645376+0j)) <f>: (np.float32(-0.0016272559), np.complex128(0.008607856797270533+0j))\n",
      "Epoch 3300: <Test loss>: 4.804890704690479e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07372742), np.complex128(0.00032409770449614265+0j)) <f>: (np.float32(-0.0015992383), np.complex128(0.008731227262738159+0j))\n",
      "Epoch 3400: <Test loss>: 5.680915273842402e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07369214), np.complex128(0.000405318907354954+0j)) <f>: (np.float32(-0.0015639012), np.complex128(0.008587100364206113+0j))\n",
      "Epoch 3500: <Test loss>: 5.267097367323004e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073792286), np.complex128(0.0003671344842531492+0j)) <f>: (np.float32(-0.0016640637), np.complex128(0.008651219799204008+0j))\n",
      "Epoch 3600: <Test loss>: 6.240110815269873e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07369341), np.complex128(0.00046373697075849905+0j)) <f>: (np.float32(-0.0015651745), np.complex128(0.008506001899031819+0j))\n",
      "Epoch 3700: <Test loss>: 5.1557737606344745e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0736904), np.complex128(0.0003732493264400157+0j)) <f>: (np.float32(-0.0015621865), np.complex128(0.008634768509355864+0j))\n",
      "Epoch 3800: <Test loss>: 5.9271704230923206e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073649004), np.complex128(0.0004633786845512688+0j)) <f>: (np.float32(-0.0015207635), np.complex128(0.008513128423233773+0j))\n",
      "Epoch 3900: <Test loss>: 5.203090768191032e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07372836), np.complex128(0.00036872836153300793+0j)) <f>: (np.float32(-0.0016000831), np.complex128(0.008657806404454746+0j))\n",
      "Epoch 4000: <Test loss>: 5.2285962738096714e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07381091), np.complex128(0.0003717447907770258+0j)) <f>: (np.float32(-0.0016827016), np.complex128(0.008620315528558369+0j))\n",
      "Epoch 4100: <Test loss>: 5.076644811197184e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0737256), np.complex128(0.00038107323538223524+0j)) <f>: (np.float32(-0.0015973634), np.complex128(0.008612269518322953+0j))\n",
      "Epoch 4200: <Test loss>: 5.9621845139190555e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07373961), np.complex128(0.00043838526415791913+0j)) <f>: (np.float32(-0.0016113631), np.complex128(0.008566334797174474+0j))\n",
      "Epoch 4300: <Test loss>: 5.105491800350137e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0737253), np.complex128(0.00038377286185347953+0j)) <f>: (np.float32(-0.0015970615), np.complex128(0.00859628406080302+0j))\n",
      "Epoch 4400: <Test loss>: 5.298335963743739e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07376819), np.complex128(0.0003651981783480391+0j)) <f>: (np.float32(-0.0016399877), np.complex128(0.008623150103052252+0j))\n",
      "Epoch 4500: <Test loss>: 5.236057404545136e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07376389), np.complex128(0.00038645935824684535+0j)) <f>: (np.float32(-0.0016357037), np.complex128(0.008596254629130868+0j))\n",
      "Epoch 4600: <Test loss>: 4.9066216888604686e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0738214), np.complex128(0.00036918442559071983+0j)) <f>: (np.float32(-0.0016932056), np.complex128(0.008625074325479895+0j))\n",
      "Epoch 4700: <Test loss>: 4.9614736781222746e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07379553), np.complex128(0.00031996531377794483+0j)) <f>: (np.float32(-0.0016673006), np.complex128(0.008727069277882646+0j))\n",
      "Epoch 4800: <Test loss>: 5.5712152970954776e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073767856), np.complex128(0.00041979098068115284+0j)) <f>: (np.float32(-0.0016396974), np.complex128(0.008548098324177479+0j))\n",
      "Epoch 4900: <Test loss>: 5.713123027817346e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07373179), np.complex128(0.0004351385511084043+0j)) <f>: (np.float32(-0.0016035716), np.complex128(0.00852974209472138+0j))\n",
      "Epoch 5000: <Test loss>: 6.188208499224856e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07377786), np.complex128(0.0004882849199588987+0j)) <f>: (np.float32(-0.0016496632), np.complex128(0.008475340185960127+0j))\n",
      "Epoch 5100: <Test loss>: 7.408590317936614e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07378731), np.complex128(0.0005421544262627928+0j)) <f>: (np.float32(-0.0016590651), np.complex128(0.008397423893476693+0j))\n",
      "Epoch 5200: <Test loss>: 5.101705755805597e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07376912), np.complex128(0.00038224247833186417+0j)) <f>: (np.float32(-0.0016408844), np.complex128(0.008615050303898765+0j))\n",
      "Epoch 5300: <Test loss>: 6.181086791912094e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07377048), np.complex128(0.0004602734528481693+0j)) <f>: (np.float32(-0.0016422443), np.complex128(0.008500062283125819+0j))\n",
      "Epoch 5400: <Test loss>: 5.262077320367098e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07382527), np.complex128(0.0003881063901414972+0j)) <f>: (np.float32(-0.0016970249), np.complex128(0.008593162273784335+0j))\n",
      "Epoch 5500: <Test loss>: 4.9978913011727855e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07382045), np.complex128(0.00035544478226096033+0j)) <f>: (np.float32(-0.0016922135), np.complex128(0.008649893344186642+0j))\n",
      "Epoch 5600: <Test loss>: 5.3956166084390134e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07385353), np.complex128(0.0003384136123550138+0j)) <f>: (np.float32(-0.0017253283), np.complex128(0.008654480625501485+0j))\n",
      "Epoch 5700: <Test loss>: 4.9316058721160516e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389782), np.complex128(0.00033956791746241863+0j)) <f>: (np.float32(-0.0017696527), np.complex128(0.008657607486946403+0j))\n",
      "Epoch 5800: <Test loss>: 5.383526877267286e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07383595), np.complex128(0.00039266408120836797+0j)) <f>: (np.float32(-0.0017077638), np.complex128(0.008594832774900316+0j))\n",
      "Epoch 5900: <Test loss>: 5.007622894481756e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07387997), np.complex128(0.0003396892596797208+0j)) <f>: (np.float32(-0.0017517622), np.complex128(0.008684637940605592+0j))\n",
      "Epoch 6000: <Test loss>: 5.255308496998623e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07381883), np.complex128(0.0003876818192415953+0j)) <f>: (np.float32(-0.0016906704), np.complex128(0.008607711668680263+0j))\n",
      "Epoch 6100: <Test loss>: 5.681185575667769e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07383635), np.complex128(0.00042808906303970414+0j)) <f>: (np.float32(-0.0017081503), np.complex128(0.008553641627394663+0j))\n",
      "Epoch 6200: <Test loss>: 4.9989695980912074e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0738192), np.complex128(0.0003610981370970781+0j)) <f>: (np.float32(-0.0016909975), np.complex128(0.008659443414357587+0j))\n",
      "Epoch 6300: <Test loss>: 5.219347440288402e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0738913), np.complex128(0.00034889753552869445+0j)) <f>: (np.float32(-0.0017631063), np.complex128(0.00868627596539368+0j))\n",
      "Epoch 6400: <Test loss>: 5.8801590057555586e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07387328), np.complex128(0.0004216243391640562+0j)) <f>: (np.float32(-0.0017450514), np.complex128(0.008549700827981935+0j))\n",
      "Epoch 6500: <Test loss>: 5.719781984225847e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07382213), np.complex128(0.0004316456332410858+0j)) <f>: (np.float32(-0.0016939207), np.complex128(0.008551332763458543+0j))\n",
      "Epoch 6600: <Test loss>: 5.418802538770251e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07382355), np.complex128(0.00037772300403756244+0j)) <f>: (np.float32(-0.0016953769), np.complex128(0.00862091431085389+0j))\n",
      "Epoch 6700: <Test loss>: 5.785521352663636e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07385644), np.complex128(0.00040854440295978105+0j)) <f>: (np.float32(-0.0017281937), np.complex128(0.008578719441839305+0j))\n",
      "Epoch 6800: <Test loss>: 7.230389746837318e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073852375), np.complex128(0.0005174102236278229+0j)) <f>: (np.float32(-0.0017241671), np.complex128(0.008439214838163112+0j))\n",
      "Epoch 6900: <Test loss>: 6.010267316014506e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07384811), np.complex128(0.00042340428931085484+0j)) <f>: (np.float32(-0.0017200075), np.complex128(0.008554459624903461+0j))\n",
      "Epoch 7000: <Test loss>: 5.828158828080632e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07387895), np.complex128(0.0003936623794242648+0j)) <f>: (np.float32(-0.0017507091), np.complex128(0.008605623034842663+0j))\n",
      "Epoch 7100: <Test loss>: 5.709529432351701e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07387351), np.complex128(0.00040017521520363655+0j)) <f>: (np.float32(-0.0017452952), np.complex128(0.00859613588755701+0j))\n",
      "Epoch 7200: <Test loss>: 6.210379797266796e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07387033), np.complex128(0.0004463940725060121+0j)) <f>: (np.float32(-0.001742109), np.complex128(0.00852695420494884+0j))\n",
      "Epoch 7300: <Test loss>: 6.0855807532789186e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07394544), np.complex128(0.0003839522745359883+0j)) <f>: (np.float32(-0.0018171828), np.complex128(0.00861989739583675+0j))\n",
      "Epoch 7400: <Test loss>: 5.718729880754836e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07390976), np.complex128(0.00039329629128670093+0j)) <f>: (np.float32(-0.0017815403), np.complex128(0.008611391642584603+0j))\n",
      "Epoch 7500: <Test loss>: 5.85659290663898e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07393054), np.complex128(0.00040512550828513983+0j)) <f>: (np.float32(-0.0018023064), np.complex128(0.008581149077119777+0j))\n",
      "Epoch 7600: <Test loss>: 6.0949179896852e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07386318), np.complex128(0.0004366393760972111+0j)) <f>: (np.float32(-0.0017349387), np.complex128(0.008546887566078229+0j))\n",
      "Epoch 7700: <Test loss>: 6.145008228486404e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388813), np.complex128(0.0004255956802796768+0j)) <f>: (np.float32(-0.0017599291), np.complex128(0.008563165310549197+0j))\n",
      "Epoch 7800: <Test loss>: 6.479743751697242e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073830225), np.complex128(0.00045264842969912854+0j)) <f>: (np.float32(-0.0017019766), np.complex128(0.008522673418978485+0j))\n",
      "Epoch 7900: <Test loss>: 5.9815996792167425e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389383), np.complex128(0.00042167356109851857+0j)) <f>: (np.float32(-0.0017656486), np.complex128(0.008580319915873267+0j))\n",
      "Epoch 8000: <Test loss>: 6.262831448111683e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389694), np.complex128(0.0004448220469741203+0j)) <f>: (np.float32(-0.0017687237), np.complex128(0.008533928496363797+0j))\n",
      "Epoch 8100: <Test loss>: 5.984525341773406e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07390051), np.complex128(0.0004012597786654984+0j)) <f>: (np.float32(-0.0017724066), np.complex128(0.008593403816473036+0j))\n",
      "Epoch 8200: <Test loss>: 6.149062392069027e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389998), np.complex128(0.0004293995336144367+0j)) <f>: (np.float32(-0.0017718056), np.complex128(0.008552660233361157+0j))\n",
      "Epoch 8300: <Test loss>: 7.152219041017815e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073948264), np.complex128(0.0005011949263146884+0j)) <f>: (np.float32(-0.0018200475), np.complex128(0.008455580370207915+0j))\n",
      "Epoch 8400: <Test loss>: 6.28504712949507e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07390142), np.complex128(0.0004429589079522606+0j)) <f>: (np.float32(-0.0017732686), np.complex128(0.00853952964804004+0j))\n",
      "Epoch 8500: <Test loss>: 6.057518839952536e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073948555), np.complex128(0.00042261607234101383+0j)) <f>: (np.float32(-0.001820344), np.complex128(0.008568184932979112+0j))\n",
      "Epoch 8600: <Test loss>: 6.54920659144409e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389619), np.complex128(0.00046634300578090416+0j)) <f>: (np.float32(-0.0017679906), np.complex128(0.008513873349004811+0j))\n",
      "Epoch 8700: <Test loss>: 6.679512443952262e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07386677), np.complex128(0.0004779348030619556+0j)) <f>: (np.float32(-0.0017385943), np.complex128(0.008498699292239573+0j))\n",
      "Epoch 8800: <Test loss>: 6.666954141110182e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07395623), np.complex128(0.00045013297318498665+0j)) <f>: (np.float32(-0.0018280149), np.complex128(0.008530975180296056+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72a472c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.0006900847074575722 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07398754), np.complex128(0.0012665401250856057+0j)) <f>: (np.float32(-0.0018592976), np.complex128(0.008256745067570131+0j))\n",
      "Epoch 400: <Test loss>: 0.0006106501095928252 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0740807), np.complex128(0.0011427135955602685+0j)) <f>: (np.float32(-0.0019524546), np.complex128(0.008175078266657714+0j))\n",
      "Epoch 600: <Test loss>: 0.0005515921511687338 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07407273), np.complex128(0.0011459489228659225+0j)) <f>: (np.float32(-0.0019445339), np.complex128(0.008092640152957846+0j))\n",
      "Epoch 800: <Test loss>: 0.0005087361787445843 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07423408), np.complex128(0.0010768498265446996+0j)) <f>: (np.float32(-0.0021059068), np.complex128(0.00813272761275777+0j))\n",
      "Epoch 1000: <Test loss>: 0.0004935286124236882 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07403796), np.complex128(0.001067744276111785+0j)) <f>: (np.float32(-0.0019097748), np.complex128(0.00811029763392163+0j))\n",
      "Epoch 1200: <Test loss>: 0.00047333596739917994 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073946185), np.complex128(0.0010484487703599137+0j)) <f>: (np.float32(-0.0018179588), np.complex128(0.008111150137528812+0j))\n",
      "Epoch 1400: <Test loss>: 0.00046546731027774513 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073990844), np.complex128(0.0010679261942922464+0j)) <f>: (np.float32(-0.001862638), np.complex128(0.008076342618224569+0j))\n",
      "Epoch 1600: <Test loss>: 0.0004607032460626215 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07385423), np.complex128(0.0011032816247690234+0j)) <f>: (np.float32(-0.0017260467), np.complex128(0.008009169900962137+0j))\n",
      "Epoch 1800: <Test loss>: 0.000483986281324178 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07391049), np.complex128(0.0011456017052509134+0j)) <f>: (np.float32(-0.0017822044), np.complex128(0.007934913284678111+0j))\n",
      "Epoch 2000: <Test loss>: 0.00044298853026703 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0739982), np.complex128(0.0010597921427616844+0j)) <f>: (np.float32(-0.0018699783), np.complex128(0.008048281041154785+0j))\n",
      "Epoch 2200: <Test loss>: 0.00044342363253235817 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07406451), np.complex128(0.001050732896468148+0j)) <f>: (np.float32(-0.0019362759), np.complex128(0.00803737965127793+0j))\n",
      "Epoch 2400: <Test loss>: 0.0004465141973923892 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07411356), np.complex128(0.0010512788413005161+0j)) <f>: (np.float32(-0.0019853495), np.complex128(0.008026118484581138+0j))\n",
      "Epoch 2600: <Test loss>: 0.0004490756255108863 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074150495), np.complex128(0.0010448590578121962+0j)) <f>: (np.float32(-0.0020222946), np.complex128(0.008020607150249222+0j))\n",
      "Epoch 2800: <Test loss>: 0.00041442783549427986 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07431254), np.complex128(0.0009626942653953151+0j)) <f>: (np.float32(-0.0021842849), np.complex128(0.008125445811113078+0j))\n",
      "Epoch 3000: <Test loss>: 0.00042433853377588093 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074229166), np.complex128(0.0009592514574870498+0j)) <f>: (np.float32(-0.002100925), np.complex128(0.008107902504739542+0j))\n",
      "Epoch 3200: <Test loss>: 0.00042263485374860466 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074285924), np.complex128(0.0010080879258466751+0j)) <f>: (np.float32(-0.0021576916), np.complex128(0.008046576541383042+0j))\n",
      "Epoch 3400: <Test loss>: 0.0004012691497337073 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07427268), np.complex128(0.0009527476922445699+0j)) <f>: (np.float32(-0.002144436), np.complex128(0.008116131194319354+0j))\n",
      "Epoch 5000: <Test loss>: 0.0003739649837370962 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07450146), np.complex128(0.0009438542528282045+0j)) <f>: (np.float32(-0.0023732285), np.complex128(0.008081284094490491+0j))\n",
      "Epoch 5200: <Test loss>: 0.0003671464219223708 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07446985), np.complex128(0.0009377100106843493+0j)) <f>: (np.float32(-0.002341669), np.complex128(0.008075118159174489+0j))\n",
      "Epoch 5400: <Test loss>: 0.00036602371255867183 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07451291), np.complex128(0.0009432477954630052+0j)) <f>: (np.float32(-0.0023846924), np.complex128(0.00807573064242084+0j))\n",
      "Epoch 5600: <Test loss>: 0.0003499393642414361 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07445632), np.complex128(0.0008910443818673015+0j)) <f>: (np.float32(-0.0023280997), np.complex128(0.008129261779640469+0j))\n",
      "Epoch 5800: <Test loss>: 0.00036444971919991076 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07444551), np.complex128(0.0009805506638674414+0j)) <f>: (np.float32(-0.0023173336), np.complex128(0.008029979108059382+0j))\n",
      "Epoch 6000: <Test loss>: 0.000330541079165414 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07458694), np.complex128(0.0008114711623580607+0j)) <f>: (np.float32(-0.0024587186), np.complex128(0.008239217491917918+0j))\n",
      "Epoch 6200: <Test loss>: 0.0003270710294600576 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07441784), np.complex128(0.0008496923750500564+0j)) <f>: (np.float32(-0.002289675), np.complex128(0.00817456777937865+0j))\n",
      "Epoch 6400: <Test loss>: 0.0003382368595339358 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074361175), np.complex128(0.0009221178846278199+0j)) <f>: (np.float32(-0.0022329595), np.complex128(0.008079230981636525+0j))\n",
      "Epoch 6600: <Test loss>: 0.00034265348222106695 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074440464), np.complex128(0.0009315736973314835+0j)) <f>: (np.float32(-0.002312322), np.complex128(0.00807761325455337+0j))\n",
      "Epoch 6800: <Test loss>: 0.0003317735390737653 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074441165), np.complex128(0.0009124165331247957+0j)) <f>: (np.float32(-0.0023130025), np.complex128(0.008099416034307083+0j))\n",
      "Epoch 7000: <Test loss>: 0.00032796349842101336 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07436403), np.complex128(0.000889645235694143+0j)) <f>: (np.float32(-0.0022358045), np.complex128(0.0081274065694096+0j))\n",
      "Epoch 7200: <Test loss>: 0.0003440727014094591 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07430848), np.complex128(0.0009747278231951336+0j)) <f>: (np.float32(-0.0021802231), np.complex128(0.008027950859893959+0j))\n",
      "Epoch 7400: <Test loss>: 0.0003371675848029554 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07438588), np.complex128(0.0009482525117658598+0j)) <f>: (np.float32(-0.0022575902), np.complex128(0.008052525798698886+0j))\n",
      "Epoch 7600: <Test loss>: 0.000323539279634133 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07435779), np.complex128(0.0009103637374224689+0j)) <f>: (np.float32(-0.002229553), np.complex128(0.008093664679614336+0j))\n",
      "Epoch 7800: <Test loss>: 0.00034787689219228923 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07426122), np.complex128(0.0010132174096045394+0j)) <f>: (np.float32(-0.0021330253), np.complex128(0.007983557749435136+0j))\n",
      "Epoch 8000: <Test loss>: 0.0003247759595979005 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07427334), np.complex128(0.0009259315696634067+0j)) <f>: (np.float32(-0.0021451267), np.complex128(0.008075818937437298+0j))\n",
      "Epoch 8200: <Test loss>: 0.00031088091782294214 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0742684), np.complex128(0.0008728277625629435+0j)) <f>: (np.float32(-0.0021402093), np.complex128(0.008142185835813893+0j))\n",
      "Epoch 8400: <Test loss>: 0.00032153315260075033 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074357845), np.complex128(0.000922605283267522+0j)) <f>: (np.float32(-0.0022296428), np.complex128(0.008075616467830593+0j))\n",
      "Epoch 8600: <Test loss>: 0.00031653023324906826 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07434063), np.complex128(0.0009042599003974957+0j)) <f>: (np.float32(-0.0022124997), np.complex128(0.008102584506047113+0j))\n",
      "Epoch 8800: <Test loss>: 0.00032606316381134093 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074337296), np.complex128(0.0009429441544832741+0j)) <f>: (np.float32(-0.0022091167), np.complex128(0.008063659597297225+0j))\n",
      "Epoch 9000: <Test loss>: 0.0003098040178883821 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0743508), np.complex128(0.0008975872201917772+0j)) <f>: (np.float32(-0.0022225536), np.complex128(0.008112462891595349+0j))\n",
      "Epoch 9200: <Test loss>: 0.00032622425351291895 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07425581), np.complex128(0.0009629912461906152+0j)) <f>: (np.float32(-0.0021276015), np.complex128(0.008027338376647607+0j))\n",
      "Epoch 9400: <Test loss>: 0.0003297329240012914 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074307196), np.complex128(0.000970466890917415+0j)) <f>: (np.float32(-0.0021790157), np.complex128(0.008023345818087299+0j))\n",
      "Epoch 9600: <Test loss>: 0.0003226821718271822 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074290596), np.complex128(0.0009559263762673955+0j)) <f>: (np.float32(-0.0021623673), np.complex128(0.008043518184692273+0j))\n",
      "Epoch 9800: <Test loss>: 0.00032496056519448757 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07421176), np.complex128(0.0009405145826331296+0j)) <f>: (np.float32(-0.0020836028), np.complex128(0.008068202223661215+0j))\n",
      "Epoch 10000: <Test loss>: 0.0003347613092046231 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0743077), np.complex128(0.0009898315991570422+0j)) <f>: (np.float32(-0.0021793726), np.complex128(0.008000930555087235+0j))\n",
      "Epoch 10200: <Test loss>: 0.0003307340375613421 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07422995), np.complex128(0.0009916619446993695+0j)) <f>: (np.float32(-0.002101704), np.complex128(0.00799987304466023+0j))\n",
      "Epoch 10400: <Test loss>: 0.000296636251732707 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07434911), np.complex128(0.0008428374595122368+0j)) <f>: (np.float32(-0.00222088), np.complex128(0.008179445317874035+0j))\n",
      "Epoch 10600: <Test loss>: 0.0003088554076384753 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07434445), np.complex128(0.0008954152389032973+0j)) <f>: (np.float32(-0.0022163119), np.complex128(0.008098744687716425+0j))\n",
      "Epoch 10800: <Test loss>: 0.00032196834217756987 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07426216), np.complex128(0.0009658730762787966+0j)) <f>: (np.float32(-0.002133971), np.complex128(0.008030253127075976+0j))\n",
      "Epoch 11000: <Test loss>: 0.00032806297531351447 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0742912), np.complex128(0.0009777810420293423+0j)) <f>: (np.float32(-0.0021629245), np.complex128(0.0080147649633269+0j))\n",
      "Epoch 11200: <Test loss>: 0.00032369454856961966 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07424054), np.complex128(0.0009668363926688481+0j)) <f>: (np.float32(-0.0021122813), np.complex128(0.00802596625179414+0j))\n",
      "Epoch 11400: <Test loss>: 0.00032813180587254465 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074245006), np.complex128(0.0009904650778419335+0j)) <f>: (np.float32(-0.002116802), np.complex128(0.007995857651181871+0j))\n",
      "Epoch 11600: <Test loss>: 0.0003084363997913897 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07430627), np.complex128(0.0009083208368512969+0j)) <f>: (np.float32(-0.0021781034), np.complex128(0.008098286974470188+0j))\n",
      "Epoch 11800: <Test loss>: 0.00032258094870485365 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07423793), np.complex128(0.0009702823720935092+0j)) <f>: (np.float32(-0.0021097837), np.complex128(0.008020664998708281+0j))\n",
      "Epoch 12000: <Test loss>: 0.0003313964116387069 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07427716), np.complex128(0.0010056300640702803+0j)) <f>: (np.float32(-0.0021489626), np.complex128(0.007983861707566507+0j))\n",
      "Epoch 12200: <Test loss>: 0.0003256316122133285 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0742384), np.complex128(0.0009840108516855554+0j)) <f>: (np.float32(-0.0021102128), np.complex128(0.008010638947356659+0j))\n",
      "Epoch 12400: <Test loss>: 0.00031492646667174995 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074300855), np.complex128(0.0009377948804631001+0j)) <f>: (np.float32(-0.0021725956), np.complex128(0.008062851748640893+0j))\n",
      "Epoch 12600: <Test loss>: 0.0002980875433422625 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07430759), np.complex128(0.0008675819475836823+0j)) <f>: (np.float32(-0.0021793968), np.complex128(0.00814370156692976+0j))\n",
      "Epoch 12800: <Test loss>: 0.00033011831692419946 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074271865), np.complex128(0.0009935795069425807+0j)) <f>: (np.float32(-0.0021436098), np.complex128(0.008001572977448362+0j))\n",
      "Epoch 13000: <Test loss>: 0.00032583263237029314 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07430723), np.complex128(0.0009833396953858825+0j)) <f>: (np.float32(-0.0021790154), np.complex128(0.008004268512663457+0j))\n",
      "Epoch 13200: <Test loss>: 0.0003179801278747618 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07426984), np.complex128(0.0009536312132821038+0j)) <f>: (np.float32(-0.0021416028), np.complex128(0.008043500931643079+0j))\n",
      "Epoch 13400: <Test loss>: 0.00032149857725016773 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074296564), np.complex128(0.0009648873689829909+0j)) <f>: (np.float32(-0.0021683262), np.complex128(0.008033445956061928+0j))\n",
      "Epoch 13600: <Test loss>: 0.0003196109610144049 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07429807), np.complex128(0.0009601758910860891+0j)) <f>: (np.float32(-0.0021699283), np.complex128(0.008035126606030374+0j))\n",
      "Epoch 13800: <Test loss>: 0.0003255814081057906 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07429474), np.complex128(0.0009906525144609236+0j)) <f>: (np.float32(-0.0021665771), np.complex128(0.008000912287152795+0j))\n",
      "Epoch 14000: <Test loss>: 0.00032372286659665406 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074309625), np.complex128(0.000978001018406553+0j)) <f>: (np.float32(-0.0021813668), np.complex128(0.00801409463162149+0j))\n",
      "Epoch 14200: <Test loss>: 0.0003273824695497751 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07428635), np.complex128(0.0009900118047186498+0j)) <f>: (np.float32(-0.0021580795), np.complex128(0.007999538132528836+0j))\n",
      "Epoch 14400: <Test loss>: 0.0003286974679213017 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07431431), np.complex128(0.0009932981300079479+0j)) <f>: (np.float32(-0.002186066), np.complex128(0.007998671927970822+0j))\n",
      "Epoch 14600: <Test loss>: 0.00031168421264737844 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07430929), np.complex128(0.0009148048752617951+0j)) <f>: (np.float32(-0.0021810485), np.complex128(0.008086350909084375+0j))\n",
      "Epoch 14800: <Test loss>: 0.00033192517003044486 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07429884), np.complex128(0.0009977937544989534+0j)) <f>: (np.float32(-0.0021705895), np.complex128(0.007995661778329268+0j))\n",
      "Epoch 15000: <Test loss>: 0.00034613930620253086 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074341424), np.complex128(0.0010635528634734617+0j)) <f>: (np.float32(-0.0022132641), np.complex128(0.007917432393747243+0j))\n",
      "Epoch 15200: <Test loss>: 0.00032749242382124066 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07435879), np.complex128(0.0009601573694303379+0j)) <f>: (np.float32(-0.002230556), np.complex128(0.008035087532948378+0j))\n",
      "Epoch 15400: <Test loss>: 0.0003306918079033494 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07433189), np.complex128(0.0009865800971177697+0j)) <f>: (np.float32(-0.0022036182), np.complex128(0.008009258195978594+0j))\n",
      "Epoch 15600: <Test loss>: 0.0003256355121266097 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07432329), np.complex128(0.0009673246793331409+0j)) <f>: (np.float32(-0.0021950891), np.complex128(0.008025649607597187+0j))\n",
      "Epoch 15800: <Test loss>: 0.0003303209668956697 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07428684), np.complex128(0.000998548511970819+0j)) <f>: (np.float32(-0.002158664), np.complex128(0.00799320068160615+0j))\n",
      "Epoch 16000: <Test loss>: 0.00032362749334424734 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074273735), np.complex128(0.0009729864069725436+0j)) <f>: (np.float32(-0.0021454648), np.complex128(0.00802330167057907+0j))\n",
      "Epoch 16200: <Test loss>: 0.00032456169719807804 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07433026), np.complex128(0.0009651823200077976+0j)) <f>: (np.float32(-0.0022020517), np.complex128(0.008026228599630399+0j))\n",
      "Epoch 16400: <Test loss>: 0.0003308544692117721 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07428624), np.complex128(0.0009838835470174292+0j)) <f>: (np.float32(-0.0021580376), np.complex128(0.008012282554013603+0j))\n",
      "Epoch 16600: <Test loss>: 0.0003174016601406038 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074347034), np.complex128(0.000935379326715425+0j)) <f>: (np.float32(-0.0022188523), np.complex128(0.008064398433756783+0j))\n",
      "Epoch 16800: <Test loss>: 0.0003180258790962398 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07445515), np.complex128(0.0009291871946739937+0j)) <f>: (np.float32(-0.0023268666), np.complex128(0.008068351411792471+0j))\n",
      "Epoch 17000: <Test loss>: 0.00032813232974149287 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07433688), np.complex128(0.000963153374108767+0j)) <f>: (np.float32(-0.0022086375), np.complex128(0.008032888784061517+0j))\n",
      "Epoch 17200: <Test loss>: 0.00032270807423628867 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07434514), np.complex128(0.0009449625709478706+0j)) <f>: (np.float32(-0.0022169496), np.complex128(0.008058073161457057+0j))\n",
      "Epoch 17400: <Test loss>: 0.0003406125761102885 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07427003), np.complex128(0.0010088322173144345+0j)) <f>: (np.float32(-0.002141801), np.complex128(0.007979126760448277+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc836950",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc2c0518",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.000980229233391583 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074056596), np.complex128(0.0013150979440146985+0j)) <f>: (np.float32(-0.0019283394), np.complex128(0.008227481358925695+0j))\n",
      "Epoch 800: <Test loss>: 0.0008975302334874868 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074253045), np.complex128(0.0011268835423437285+0j)) <f>: (np.float32(-0.0021247878), np.complex128(0.008338159162056134+0j))\n",
      "Epoch 1200: <Test loss>: 0.0008903857669793069 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073863536), np.complex128(0.0014051539132365536+0j)) <f>: (np.float32(-0.0017353408), np.complex128(0.007805078000189346+0j))\n",
      "Epoch 1600: <Test loss>: 0.0008743554353713989 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07383558), np.complex128(0.0013970429503453526+0j)) <f>: (np.float32(-0.001707412), np.complex128(0.0077714670305908985+0j))\n",
      "Epoch 2000: <Test loss>: 0.0008490732288919389 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07397903), np.complex128(0.0013961230837299232+0j)) <f>: (np.float32(-0.0018508093), np.complex128(0.007735556838466173+0j))\n",
      "Epoch 2400: <Test loss>: 0.0008753892034292221 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07396196), np.complex128(0.0015533034363043382+0j)) <f>: (np.float32(-0.0018337622), np.complex128(0.007555972386629408+0j))\n",
      "Epoch 2800: <Test loss>: 0.0008953174692578614 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07408904), np.complex128(0.0015918325398080353+0j)) <f>: (np.float32(-0.0019608214), np.complex128(0.007496705625438355+0j))\n",
      "Epoch 3200: <Test loss>: 0.0009249418508261442 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0741051), np.complex128(0.0016744216176883427+0j)) <f>: (np.float32(-0.0019768649), np.complex128(0.0073968683190699745+0j))\n",
      "Epoch 3600: <Test loss>: 0.0009384158183820546 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07410859), np.complex128(0.0016955486107284438+0j)) <f>: (np.float32(-0.0019803692), np.complex128(0.0073670251109497036+0j))\n",
      "Epoch 4000: <Test loss>: 0.0009882042650133371 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07415953), np.complex128(0.0018218959883457956+0j)) <f>: (np.float32(-0.0020313046), np.complex128(0.007213251220705658+0j))\n",
      "Epoch 4400: <Test loss>: 0.0009568643872626126 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07422051), np.complex128(0.001709636486558459+0j)) <f>: (np.float32(-0.0020923205), np.complex128(0.007346068745491695+0j))\n",
      "Epoch 4800: <Test loss>: 0.0009702804964035749 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07425278), np.complex128(0.001765365357779702+0j)) <f>: (np.float32(-0.0021245168), np.complex128(0.007286742106071089+0j))\n",
      "Epoch 5200: <Test loss>: 0.0009798683458939195 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07425813), np.complex128(0.001796989562647182+0j)) <f>: (np.float32(-0.0021299256), np.complex128(0.007251474336307491+0j))\n",
      "Epoch 5600: <Test loss>: 0.0009693559841252863 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074324265), np.complex128(0.0017634247703274574+0j)) <f>: (np.float32(-0.002196024), np.complex128(0.007293815348797594+0j))\n",
      "Epoch 6000: <Test loss>: 0.0010042126523330808 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07433374), np.complex128(0.0018356023941837275+0j)) <f>: (np.float32(-0.00220548), np.complex128(0.007205872497519914+0j))\n",
      "Epoch 6400: <Test loss>: 0.001004499732516706 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074281804), np.complex128(0.001849902888472915+0j)) <f>: (np.float32(-0.002153575), np.complex128(0.007186473980915507+0j))\n",
      "Epoch 6800: <Test loss>: 0.0009640576899982989 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07441614), np.complex128(0.00173386839415002+0j)) <f>: (np.float32(-0.002287893), np.complex128(0.007313404156385746+0j))\n",
      "Epoch 7200: <Test loss>: 0.0009326658910140395 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07438885), np.complex128(0.0016201335029353708+0j)) <f>: (np.float32(-0.0022606177), np.complex128(0.00743871508244496+0j))\n",
      "Epoch 7600: <Test loss>: 0.0009682769305072725 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07428176), np.complex128(0.0017583527544466845+0j)) <f>: (np.float32(-0.002153524), np.complex128(0.007278740243343903+0j))\n",
      "Epoch 8000: <Test loss>: 0.0009584439685568213 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074299894), np.complex128(0.0017334921254448259+0j)) <f>: (np.float32(-0.0021716785), np.complex128(0.007303571948116232+0j))\n",
      "Epoch 8400: <Test loss>: 0.0009827204048633575 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074300215), np.complex128(0.0017816496990047722+0j)) <f>: (np.float32(-0.0021720235), np.complex128(0.007249508503584736+0j))\n",
      "Epoch 8800: <Test loss>: 0.0009671509615145624 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074341975), np.complex128(0.0017442543491822354+0j)) <f>: (np.float32(-0.0022136632), np.complex128(0.007293495152502277+0j))\n",
      "Epoch 9200: <Test loss>: 0.0009697536588646472 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07426924), np.complex128(0.0017756569284839792+0j)) <f>: (np.float32(-0.002141036), np.complex128(0.007246409551484101+0j))\n",
      "Epoch 9600: <Test loss>: 0.00098122819326818 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0741817), np.complex128(0.0018079310404912807+0j)) <f>: (np.float32(-0.002053444), np.complex128(0.007215860490674787+0j))\n",
      "Epoch 10000: <Test loss>: 0.0009712244500406086 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07419447), np.complex128(0.0017923161429470296+0j)) <f>: (np.float32(-0.002066233), np.complex128(0.0072343872208523235+0j))\n",
      "Epoch 10400: <Test loss>: 0.0009906358318403363 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07417402), np.complex128(0.0018208407614105947+0j)) <f>: (np.float32(-0.002045828), np.complex128(0.007203375372370539+0j))\n",
      "Epoch 10800: <Test loss>: 0.000991092179901898 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0741348), np.complex128(0.0018389653433091473+0j)) <f>: (np.float32(-0.0020065922), np.complex128(0.007180606929304643+0j))\n",
      "Epoch 11200: <Test loss>: 0.001001802273094654 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07411162), np.complex128(0.0018793477541338773+0j)) <f>: (np.float32(-0.0019834244), np.complex128(0.00713435251930347+0j))\n",
      "Epoch 11600: <Test loss>: 0.0010840371251106262 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074073896), np.complex128(0.0020411468494657818+0j)) <f>: (np.float32(-0.0019457015), np.complex128(0.006960706668487459+0j))\n",
      "Epoch 12000: <Test loss>: 0.0010052734287455678 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07409763), np.complex128(0.001877791935050768+0j)) <f>: (np.float32(-0.0019693756), np.complex128(0.00714626524232861+0j))\n",
      "Epoch 12400: <Test loss>: 0.0009848986519500613 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074110135), np.complex128(0.001834037567994054+0j)) <f>: (np.float32(-0.00198185), np.complex128(0.007186569887571315+0j))\n",
      "Epoch 12800: <Test loss>: 0.001007741317152977 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074095175), np.complex128(0.0018730344067357995+0j)) <f>: (np.float32(-0.0019669714), np.complex128(0.0071466737336403855+0j))\n",
      "Epoch 13200: <Test loss>: 0.001008335966616869 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074124165), np.complex128(0.0018770548746403907+0j)) <f>: (np.float32(-0.0019959058), np.complex128(0.007144684558556958+0j))\n",
      "Epoch 13600: <Test loss>: 0.0010418317979201674 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07411251), np.complex128(0.001927096455800081+0j)) <f>: (np.float32(-0.0019842854), np.complex128(0.007091126019435669+0j))\n",
      "Epoch 14000: <Test loss>: 0.0010615165811032057 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07407161), np.complex128(0.00197704771217282+0j)) <f>: (np.float32(-0.0019433697), np.complex128(0.0070375766142816+0j))\n",
      "Epoch 14400: <Test loss>: 0.0009950068779289722 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0741251), np.complex128(0.0018317662548120587+0j)) <f>: (np.float32(-0.001996892), np.complex128(0.007194580884265721+0j))\n",
      "Epoch 14800: <Test loss>: 0.001030641607940197 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07410788), np.complex128(0.0018917996349462978+0j)) <f>: (np.float32(-0.0019797424), np.complex128(0.0071358261326816014+0j))\n",
      "Epoch 15200: <Test loss>: 0.0010425385553389788 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07405762), np.complex128(0.0019181526527033506+0j)) <f>: (np.float32(-0.0019294124), np.complex128(0.007105971253380994+0j))\n",
      "Epoch 15600: <Test loss>: 0.0010232278145849705 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074098736), np.complex128(0.001877142915935537+0j)) <f>: (np.float32(-0.001970526), np.complex128(0.007151442171971755+0j))\n",
      "Epoch 16000: <Test loss>: 0.0010446257656440139 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07414254), np.complex128(0.001904176033668499+0j)) <f>: (np.float32(-0.0020143706), np.complex128(0.007129071563922544+0j))\n",
      "Epoch 16400: <Test loss>: 0.0010647629387676716 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07409345), np.complex128(0.001957735460814379+0j)) <f>: (np.float32(-0.0019652306), np.complex128(0.00706844891604197+0j))\n",
      "Epoch 16800: <Test loss>: 0.0010246095480397344 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07417846), np.complex128(0.00184812633184866+0j)) <f>: (np.float32(-0.0020502491), np.complex128(0.007204051793387429+0j))\n",
      "Epoch 17200: <Test loss>: 0.0010876839514821768 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07411157), np.complex128(0.001994436248546242+0j)) <f>: (np.float32(-0.0019833492), np.complex128(0.007030383107653367+0j))\n",
      "Epoch 17600: <Test loss>: 0.0010747722117230296 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07410654), np.complex128(0.0019654046949019767+0j)) <f>: (np.float32(-0.0019782975), np.complex128(0.007067774524795574+0j))\n",
      "Epoch 18000: <Test loss>: 0.0010774116963148117 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074139975), np.complex128(0.0019466120647898188+0j)) <f>: (np.float32(-0.0020118053), np.complex128(0.00709002080940207+0j))\n",
      "Epoch 18400: <Test loss>: 0.001103451824747026 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074082516), np.complex128(0.0020149434004219533+0j)) <f>: (np.float32(-0.0019543462), np.complex128(0.007011578806360873+0j))\n",
      "Epoch 18800: <Test loss>: 0.0011339373886585236 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07404944), np.complex128(0.0020784748362800724+0j)) <f>: (np.float32(-0.0019212504), np.complex128(0.006944777029656091+0j))\n",
      "Epoch 19200: <Test loss>: 0.0011222079629078507 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07405566), np.complex128(0.002043573440090514+0j)) <f>: (np.float32(-0.0019275167), np.complex128(0.006984287019750672+0j))\n",
      "Epoch 19600: <Test loss>: 0.001116794184781611 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07406684), np.complex128(0.002029547725981856+0j)) <f>: (np.float32(-0.0019386647), np.complex128(0.006998409147957762+0j))\n",
      "Epoch 20000: <Test loss>: 0.001111453166231513 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0741233), np.complex128(0.0020086693798271834+0j)) <f>: (np.float32(-0.001995043), np.complex128(0.007024212097911132+0j))\n",
      "Epoch 20400: <Test loss>: 0.0011242053005844355 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07409189), np.complex128(0.0020392037248004205+0j)) <f>: (np.float32(-0.0019637162), np.complex128(0.006994167942512025+0j))\n",
      "Epoch 20800: <Test loss>: 0.0011374658206477761 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07410241), np.complex128(0.002049318832332437+0j)) <f>: (np.float32(-0.001974164), np.complex128(0.00698305139696288+0j))\n",
      "Epoch 21200: <Test loss>: 0.0011844964465126395 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07408559), np.complex128(0.002124216602371032+0j)) <f>: (np.float32(-0.0019573607), np.complex128(0.00690592062566035+0j))\n",
      "Epoch 21600: <Test loss>: 0.0011388311395421624 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074106365), np.complex128(0.0020413285139249315+0j)) <f>: (np.float32(-0.001978222), np.complex128(0.006992671494215844+0j))\n",
      "Epoch 22000: <Test loss>: 0.0011439796071499586 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074114054), np.complex128(0.0020387090951033363+0j)) <f>: (np.float32(-0.001985785), np.complex128(0.006998416252154489+0j))\n",
      "Epoch 22400: <Test loss>: 0.0011866430286318064 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07409825), np.complex128(0.0021279044416360345+0j)) <f>: (np.float32(-0.00197002), np.complex128(0.0069019930197558275+0j))\n",
      "Epoch 22800: <Test loss>: 0.0011650878004729748 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0740605), np.complex128(0.002086767463630423+0j)) <f>: (np.float32(-0.0019322779), np.complex128(0.006942492522965889+0j))\n",
      "Epoch 23200: <Test loss>: 0.0011950848856940866 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074076526), np.complex128(0.002133305534058033+0j)) <f>: (np.float32(-0.0019483502), np.complex128(0.006900304250705407+0j))\n",
      "Epoch 23600: <Test loss>: 0.0012171617709100246 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07401115), np.complex128(0.002158845121289898+0j)) <f>: (np.float32(-0.0018829376), np.complex128(0.00687331236268559+0j))\n",
      "Epoch 24000: <Test loss>: 0.0012202052166685462 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07406318), np.complex128(0.0021698738792652085+0j)) <f>: (np.float32(-0.0019349747), np.complex128(0.006861503157955609+0j))\n",
      "Epoch 24400: <Test loss>: 0.0012291587190702558 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07406816), np.complex128(0.002183965053472275+0j)) <f>: (np.float32(-0.0019399506), np.complex128(0.006840435662563093+0j))\n",
      "Epoch 24800: <Test loss>: 0.001238878583535552 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07403886), np.complex128(0.00219730927213781+0j)) <f>: (np.float32(-0.0019106), np.complex128(0.0068365694572159914+0j))\n",
      "Epoch 25200: <Test loss>: 0.0012644807575270534 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0740587), np.complex128(0.002238545582036937+0j)) <f>: (np.float32(-0.0019305401), np.complex128(0.006788505506820048+0j))\n",
      "Epoch 25600: <Test loss>: 0.0012635273160412908 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07406956), np.complex128(0.0022204293729416696+0j)) <f>: (np.float32(-0.0019413123), np.complex128(0.006809035113031842+0j))\n",
      "Epoch 26000: <Test loss>: 0.001259075477719307 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07407012), np.complex128(0.002215223519068995+0j)) <f>: (np.float32(-0.0019418792), np.complex128(0.006820395231040183+0j))\n",
      "Epoch 26400: <Test loss>: 0.001281091244891286 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074067704), np.complex128(0.002251004313324773+0j)) <f>: (np.float32(-0.0019395025), np.complex128(0.006782156384717027+0j))\n",
      "Epoch 26800: <Test loss>: 0.001287627499550581 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07409258), np.complex128(0.0022513772836529155+0j)) <f>: (np.float32(-0.0019644091), np.complex128(0.006781324686257399+0j))\n",
      "Epoch 27200: <Test loss>: 0.0013264348963275552 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07403063), np.complex128(0.0023298329876449424+0j)) <f>: (np.float32(-0.0019023797), np.complex128(0.006698013771245414+0j))\n",
      "Epoch 27600: <Test loss>: 0.0012249480932950974 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074217804), np.complex128(0.0020807740588063505+0j)) <f>: (np.float32(-0.002089592), np.complex128(0.006979997099813096+0j))\n",
      "Epoch 28000: <Test loss>: 0.0013291455106809735 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07411127), np.complex128(0.002311117996254153+0j)) <f>: (np.float32(-0.0019830398), np.complex128(0.006721314521623177+0j))\n",
      "Epoch 28400: <Test loss>: 0.001334578380919993 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07404899), np.complex128(0.00231466146809282+0j)) <f>: (np.float32(-0.0019208123), np.complex128(0.00671963082699899+0j))\n",
      "Epoch 28800: <Test loss>: 0.0013620512327179313 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07409667), np.complex128(0.0023629276343741577+0j)) <f>: (np.float32(-0.0019684234), np.complex128(0.006670714880438487+0j))\n",
      "Epoch 29200: <Test loss>: 0.0013297211844474077 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07403332), np.complex128(0.002284379575824686+0j)) <f>: (np.float32(-0.0019051658), np.complex128(0.006757429720567725+0j))\n",
      "Epoch 29600: <Test loss>: 0.001352418097667396 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07413347), np.complex128(0.0023196960600801227+0j)) <f>: (np.float32(-0.0020052777), np.complex128(0.006718989926965732+0j))\n",
      "Epoch 30000: <Test loss>: 0.0013863769127056003 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07409612), np.complex128(0.0023784480207298146+0j)) <f>: (np.float32(-0.001967935), np.complex128(0.00665085205383374+0j))\n",
      "Epoch 30400: <Test loss>: 0.0013466131640598178 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07408648), np.complex128(0.0022891690730249235+0j)) <f>: (np.float32(-0.0019583055), np.complex128(0.0067553223113530625+0j))\n",
      "Epoch 30800: <Test loss>: 0.001374249579384923 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07410978), np.complex128(0.002335737336288621+0j)) <f>: (np.float32(-0.0019815245), np.complex128(0.006702105788559894+0j))\n",
      "Epoch 31200: <Test loss>: 0.0014204704202711582 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074107945), np.complex128(0.0024202298314490145+0j)) <f>: (np.float32(-0.0019797357), np.complex128(0.0066142654406921245+0j))\n",
      "Epoch 31600: <Test loss>: 0.0014321153284981847 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07408588), np.complex128(0.0024328631229992735+0j)) <f>: (np.float32(-0.0019576482), np.complex128(0.006600440166419679+0j))\n",
      "Epoch 32000: <Test loss>: 0.0014031638856977224 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07409477), np.complex128(0.00237146484906895+0j)) <f>: (np.float32(-0.0019665859), np.complex128(0.0066684577756499445+0j))\n",
      "Epoch 32400: <Test loss>: 0.0014228351647034287 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0740759), np.complex128(0.0023939732277903935+0j)) <f>: (np.float32(-0.0019476647), np.complex128(0.0066488715052749085+0j))\n",
      "Epoch 32800: <Test loss>: 0.0013995921472087502 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074087046), np.complex128(0.0023686109917553794+0j)) <f>: (np.float32(-0.0019587583), np.complex128(0.006669150434830781+0j))\n",
      "Epoch 33200: <Test loss>: 0.0014660228043794632 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07406347), np.complex128(0.0024700551151905135+0j)) <f>: (np.float32(-0.0019352033), np.complex128(0.00656048007471821+0j))\n",
      "Epoch 33600: <Test loss>: 0.0014346910174936056 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07407894), np.complex128(0.0024169063359875584+0j)) <f>: (np.float32(-0.0019506892), np.complex128(0.006624293521814241+0j))\n",
      "Epoch 34000: <Test loss>: 0.0014341078931465745 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07407797), np.complex128(0.0024015594950090777+0j)) <f>: (np.float32(-0.0019497308), np.complex128(0.006648422925995891+0j))\n",
      "Epoch 34400: <Test loss>: 0.0014472610782831907 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07407297), np.complex128(0.0024071713029804108+0j)) <f>: (np.float32(-0.0019447278), np.complex128(0.006637161251856476+0j))\n",
      "Epoch 34800: <Test loss>: 0.0014329066034406424 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074086376), np.complex128(0.0023786811906152316+0j)) <f>: (np.float32(-0.0019581236), np.complex128(0.006667904663190522+0j))\n",
      "Epoch 35200: <Test loss>: 0.0014659992884844542 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0740473), np.complex128(0.0024450242391885297+0j)) <f>: (np.float32(-0.0019191128), np.complex128(0.0065904410095270925+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c51fe27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a1a44c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0016864414792507887 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07427836), np.complex128(0.0017982836681973125+0j)) <f>: (np.float32(-0.0021501712), np.complex128(0.007451005850224475+0j))\n",
      "Epoch 1600: <Test loss>: 0.001585963647812605 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07460783), np.complex128(0.0018388256697270775+0j)) <f>: (np.float32(-0.0024796294), np.complex128(0.007409623904292454+0j))\n",
      "Epoch 2400: <Test loss>: 0.001519438112154603 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07478791), np.complex128(0.00186587185410759+0j)) <f>: (np.float32(-0.0026596552), np.complex128(0.007348685627100174+0j))\n",
      "Epoch 3200: <Test loss>: 0.0015748465666547418 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07493006), np.complex128(0.002026401708577907+0j)) <f>: (np.float32(-0.002801739), np.complex128(0.007121217366998745+0j))\n",
      "Epoch 4000: <Test loss>: 0.0015214926097542048 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075091526), np.complex128(0.0019489393235210355+0j)) <f>: (np.float32(-0.0029633278), np.complex128(0.007171881453396597+0j))\n",
      "Epoch 4800: <Test loss>: 0.0016012199921533465 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074901715), np.complex128(0.0021300132463179106+0j)) <f>: (np.float32(-0.0027735326), np.complex128(0.006961219692979638+0j))\n",
      "Epoch 5600: <Test loss>: 0.0015220643253996968 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074844494), np.complex128(0.0020611605133903114+0j)) <f>: (np.float32(-0.002716307), np.complex128(0.007003367369830255+0j))\n",
      "Epoch 6400: <Test loss>: 0.0015031076036393642 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0748204), np.complex128(0.0020441720955253796+0j)) <f>: (np.float32(-0.002692208), np.complex128(0.007011843183967625+0j))\n",
      "Epoch 7200: <Test loss>: 0.0015230975113809109 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07478891), np.complex128(0.0021054237185375627+0j)) <f>: (np.float32(-0.0026606743), np.complex128(0.006933386972532975+0j))\n",
      "Epoch 8000: <Test loss>: 0.0014982377178967 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074805886), np.complex128(0.002048229987323441+0j)) <f>: (np.float32(-0.0026776914), np.complex128(0.006985911343587931+0j))\n",
      "Epoch 8800: <Test loss>: 0.0015306712593883276 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07475485), np.complex128(0.002144688994427046+0j)) <f>: (np.float32(-0.0026266298), np.complex128(0.0068871300253186855+0j))\n",
      "Epoch 9600: <Test loss>: 0.0015527232317253947 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07472812), np.complex128(0.002157413879370815+0j)) <f>: (np.float32(-0.0025999309), np.complex128(0.006860511107627012+0j))\n",
      "Epoch 10400: <Test loss>: 0.00153205799870193 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07477303), np.complex128(0.0021176190871038934+0j)) <f>: (np.float32(-0.002644855), np.complex128(0.006906038859791585+0j))\n",
      "Epoch 11200: <Test loss>: 0.0015323429834097624 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07481813), np.complex128(0.002159739235192194+0j)) <f>: (np.float32(-0.0026898482), np.complex128(0.006858260092149949+0j))\n",
      "Epoch 12000: <Test loss>: 0.0015403948491439223 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07477172), np.complex128(0.0021679263144768935+0j)) <f>: (np.float32(-0.0026434269), np.complex128(0.006849754846340426+0j))\n",
      "Epoch 12800: <Test loss>: 0.0015545489732176065 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074806586), np.complex128(0.0022087736696052435+0j)) <f>: (np.float32(-0.002678396), np.complex128(0.006806486228734888+0j))\n",
      "Epoch 13600: <Test loss>: 0.001575020025484264 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074824676), np.complex128(0.0022468879387643735+0j)) <f>: (np.float32(-0.0026965374), np.complex128(0.006771005333069495+0j))\n",
      "Epoch 14400: <Test loss>: 0.001582146156579256 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07488803), np.complex128(0.002264305653088637+0j)) <f>: (np.float32(-0.0027598417), np.complex128(0.0067485758616759785+0j))\n",
      "Epoch 15200: <Test loss>: 0.0015904580941423774 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0748489), np.complex128(0.0022979019068510072+0j)) <f>: (np.float32(-0.0027206894), np.complex128(0.006713305047256641+0j))\n",
      "Epoch 16000: <Test loss>: 0.0016070206183940172 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074797705), np.complex128(0.0023146799897485713+0j)) <f>: (np.float32(-0.002669482), np.complex128(0.006697515462589311+0j))\n",
      "Epoch 16800: <Test loss>: 0.0016432672273367643 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07484675), np.complex128(0.002388228216130438+0j)) <f>: (np.float32(-0.00271846), np.complex128(0.006626780498111149+0j))\n",
      "Epoch 17600: <Test loss>: 0.0017101629637181759 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07482628), np.complex128(0.002498533810096668+0j)) <f>: (np.float32(-0.0026980229), np.complex128(0.0065167263419647755+0j))\n",
      "Epoch 18400: <Test loss>: 0.0016912906430661678 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074826844), np.complex128(0.00244287445851482+0j)) <f>: (np.float32(-0.0026986303), np.complex128(0.0065753684412865184+0j))\n",
      "Epoch 19200: <Test loss>: 0.0016897158930078149 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07481058), np.complex128(0.0024543774216216263+0j)) <f>: (np.float32(-0.0026823403), np.complex128(0.006559813802553787+0j))\n",
      "Epoch 20000: <Test loss>: 0.0017054005293175578 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07493509), np.complex128(0.0024427836262852455+0j)) <f>: (np.float32(-0.0028069164), np.complex128(0.006583263741062806+0j))\n",
      "Epoch 20800: <Test loss>: 0.0016699614934623241 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07491583), np.complex128(0.0023668851793934567+0j)) <f>: (np.float32(-0.0027876499), np.complex128(0.006675188494605706+0j))\n",
      "Epoch 21600: <Test loss>: 0.0017710544634610415 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07488687), np.complex128(0.0025139638716653733+0j)) <f>: (np.float32(-0.0027586184), np.complex128(0.006510465514878212+0j))\n",
      "Epoch 22400: <Test loss>: 0.0017249628435820341 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074999616), np.complex128(0.002426090540027088+0j)) <f>: (np.float32(-0.0028713786), np.complex128(0.0066226565119114+0j))\n",
      "Epoch 23200: <Test loss>: 0.0017903655534610152 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07500647), np.complex128(0.0025267798425600285+0j)) <f>: (np.float32(-0.002878248), np.complex128(0.0065035191328075385+0j))\n",
      "Epoch 24000: <Test loss>: 0.0018372605554759502 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07501343), np.complex128(0.002575710251235296+0j)) <f>: (np.float32(-0.0028852518), np.complex128(0.006453936406639989+0j))\n",
      "Epoch 24800: <Test loss>: 0.0018003634177148342 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07510466), np.complex128(0.0025185161394392073+0j)) <f>: (np.float32(-0.0029764604), np.complex128(0.006523714334330562+0j))\n",
      "Epoch 25600: <Test loss>: 0.001854367321357131 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07504255), np.complex128(0.0025800120960745203+0j)) <f>: (np.float32(-0.0029143267), np.complex128(0.006460786374612231+0j))\n",
      "Epoch 26400: <Test loss>: 0.001888333703391254 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07503285), np.complex128(0.0026077740282752305+0j)) <f>: (np.float32(-0.0029046417), np.complex128(0.006431406969049677+0j))\n",
      "Epoch 27200: <Test loss>: 0.001910517574287951 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075078696), np.complex128(0.002625676857747391+0j)) <f>: (np.float32(-0.0029504756), np.complex128(0.006416497797333813+0j))\n",
      "Epoch 28000: <Test loss>: 0.0019333029631525278 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07507096), np.complex128(0.002627732254093162+0j)) <f>: (np.float32(-0.0029427067), np.complex128(0.006415773676710331+0j))\n",
      "Epoch 28800: <Test loss>: 0.0019711805507540703 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07511276), np.complex128(0.0026707661794854124+0j)) <f>: (np.float32(-0.002984586), np.complex128(0.006374015208351803+0j))\n",
      "Epoch 29600: <Test loss>: 0.0019973244052380323 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07513685), np.complex128(0.0026870439239563794+0j)) <f>: (np.float32(-0.0030086404), np.complex128(0.006359247105685224+0j))\n",
      "Epoch 30400: <Test loss>: 0.002027063863351941 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07512264), np.complex128(0.002706017711085064+0j)) <f>: (np.float32(-0.0029945252), np.complex128(0.006344773319740171+0j))\n",
      "Epoch 31200: <Test loss>: 0.002009019022807479 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07524654), np.complex128(0.0026459913082868955+0j)) <f>: (np.float32(-0.003118407), np.complex128(0.006414765895660411+0j))\n",
      "Epoch 32000: <Test loss>: 0.0020650227088481188 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07518452), np.complex128(0.0027232707602780587+0j)) <f>: (np.float32(-0.0030562375), np.complex128(0.006334045475240491+0j))\n",
      "Epoch 32800: <Test loss>: 0.002084538107737899 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07523098), np.complex128(0.002736132908451436+0j)) <f>: (np.float32(-0.003102773), np.complex128(0.006319575241393802+0j))\n",
      "Epoch 33600: <Test loss>: 0.0021171770058572292 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07516236), np.complex128(0.0027695748996563164+0j)) <f>: (np.float32(-0.0030341325), np.complex128(0.006284336903302357+0j))\n",
      "Epoch 34400: <Test loss>: 0.002136301714926958 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07516195), np.complex128(0.0027783328518922543+0j)) <f>: (np.float32(-0.003033745), np.complex128(0.006274141873557167+0j))\n",
      "Epoch 35200: <Test loss>: 0.002161368727684021 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075168796), np.complex128(0.0028062361068635602+0j)) <f>: (np.float32(-0.0030405917), np.complex128(0.006245397278716391+0j))\n",
      "Epoch 36000: <Test loss>: 0.0021756456699222326 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07517352), np.complex128(0.0028000554557114817+0j)) <f>: (np.float32(-0.0030453224), np.complex128(0.00625666402728204+0j))\n",
      "Epoch 36800: <Test loss>: 0.0021837123204022646 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075241156), np.complex128(0.002779919371254075+0j)) <f>: (np.float32(-0.0031129972), np.complex128(0.00627755645496951+0j))\n",
      "Epoch 37600: <Test loss>: 0.0022336626425385475 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075156435), np.complex128(0.002847279334844513+0j)) <f>: (np.float32(-0.0030282603), np.complex128(0.006205026665932653+0j))\n",
      "Epoch 38400: <Test loss>: 0.002245894866064191 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07512915), np.complex128(0.002849941632567779+0j)) <f>: (np.float32(-0.003000955), np.complex128(0.006199137286846362+0j))\n",
      "Epoch 39200: <Test loss>: 0.002295840298756957 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07512582), np.complex128(0.0028967867057824994+0j)) <f>: (np.float32(-0.0029975947), np.complex128(0.006148030203038349+0j))\n",
      "Epoch 40000: <Test loss>: 0.0022959152702242136 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07517505), np.complex128(0.0028862220040862187+0j)) <f>: (np.float32(-0.0030468288), np.complex128(0.00616222895508156+0j))\n",
      "Epoch 40800: <Test loss>: 0.002299749990925193 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07513914), np.complex128(0.0028809978822791045+0j)) <f>: (np.float32(-0.0030109417), np.complex128(0.006170089241316839+0j))\n",
      "Epoch 41600: <Test loss>: 0.002334634307771921 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07516993), np.complex128(0.0029117313984819964+0j)) <f>: (np.float32(-0.0030417016), np.complex128(0.00614066924034442+0j))\n",
      "Epoch 42400: <Test loss>: 0.002360007492825389 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07516917), np.complex128(0.00292927445113422+0j)) <f>: (np.float32(-0.0030409403), np.complex128(0.00612250685397043+0j))\n",
      "Epoch 43200: <Test loss>: 0.0023807412944734097 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07513595), np.complex128(0.002948634909541877+0j)) <f>: (np.float32(-0.003007786), np.complex128(0.006107274948746132+0j))\n",
      "Epoch 44000: <Test loss>: 0.0023796511813998222 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07518287), np.complex128(0.0029119780155969313+0j)) <f>: (np.float32(-0.0030546116), np.complex128(0.00615138947320475+0j))\n",
      "Epoch 44800: <Test loss>: 0.002451471984386444 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07510875), np.complex128(0.003029434998453658+0j)) <f>: (np.float32(-0.0029804772), np.complex128(0.006023487545110846+0j))\n",
      "Epoch 45600: <Test loss>: 0.002397294854745269 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07516844), np.complex128(0.002921680572276186+0j)) <f>: (np.float32(-0.0030401722), np.complex128(0.006138859699949649+0j))\n",
      "Epoch 46400: <Test loss>: 0.0024542221799492836 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07517766), np.complex128(0.002995222455625261+0j)) <f>: (np.float32(-0.0030495059), np.complex128(0.006061391479302609+0j))\n",
      "Epoch 47200: <Test loss>: 0.0025021969340741634 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07510838), np.complex128(0.0030474591067127924+0j)) <f>: (np.float32(-0.0029800802), np.complex128(0.006003728744243881+0j))\n",
      "Epoch 48000: <Test loss>: 0.0024991838727146387 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07517305), np.complex128(0.003023627825072345+0j)) <f>: (np.float32(-0.0030448125), np.complex128(0.006033734841446239+0j))\n",
      "Epoch 48800: <Test loss>: 0.002509430283680558 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075152256), np.complex128(0.0030294134321421666+0j)) <f>: (np.float32(-0.0030240174), np.complex128(0.0060256868014403305+0j))\n",
      "Epoch 49600: <Test loss>: 0.002525630174204707 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07516289), np.complex128(0.0030371897366232854+0j)) <f>: (np.float32(-0.003034629), np.complex128(0.006018419208189092+0j))\n",
      "Epoch 50400: <Test loss>: 0.0025843102484941483 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07508105), np.complex128(0.00312459875826123+0j)) <f>: (np.float32(-0.0029528136), np.complex128(0.0059299592728782454+0j))\n",
      "Epoch 51200: <Test loss>: 0.0025858881417661905 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0751367), np.complex128(0.0030911197237448473+0j)) <f>: (np.float32(-0.0030084325), np.complex128(0.005967194905135091+0j))\n",
      "Epoch 52000: <Test loss>: 0.0025897088926285505 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07515433), np.complex128(0.0030879532817753093+0j)) <f>: (np.float32(-0.003026071), np.complex128(0.005968424946054028+0j))\n",
      "Epoch 52800: <Test loss>: 0.0025689685717225075 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075177625), np.complex128(0.0030415928162658623+0j)) <f>: (np.float32(-0.0030493853), np.complex128(0.006018681556025351+0j))\n",
      "Epoch 53600: <Test loss>: 0.0026227731723338366 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07514059), np.complex128(0.0031049807727222367+0j)) <f>: (np.float32(-0.0030123454), np.complex128(0.005951266281188971+0j))\n",
      "Epoch 54400: <Test loss>: 0.002627949696034193 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07513421), np.complex128(0.003100558917702597+0j)) <f>: (np.float32(-0.0030060543), np.complex128(0.0059579863438496425+0j))\n",
      "Epoch 55200: <Test loss>: 0.00266960053704679 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07515418), np.complex128(0.003151951184264918+0j)) <f>: (np.float32(-0.0030259285), np.complex128(0.005902896850333787+0j))\n",
      "Epoch 56000: <Test loss>: 0.0026963171549141407 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07512041), np.complex128(0.003163307242732272+0j)) <f>: (np.float32(-0.0029921525), np.complex128(0.005896130610394393+0j))\n",
      "Epoch 56800: <Test loss>: 0.0025920583866536617 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07515145), np.complex128(0.0029871188506520982+0j)) <f>: (np.float32(-0.0030232475), np.complex128(0.006086347000075029+0j))\n",
      "Epoch 57600: <Test loss>: 0.0027426371816545725 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07514444), np.complex128(0.003206256171485116+0j)) <f>: (np.float32(-0.003016248), np.complex128(0.0058501233249507794+0j))\n",
      "Epoch 58400: <Test loss>: 0.002794253872707486 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07507591), np.complex128(0.003262790861592196+0j)) <f>: (np.float32(-0.0029476997), np.complex128(0.005788987652578026+0j))\n",
      "Epoch 59200: <Test loss>: 0.002717945957556367 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075132586), np.complex128(0.003146434521785458+0j)) <f>: (np.float32(-0.0030044182), np.complex128(0.005920719757592774+0j))\n",
      "Epoch 60000: <Test loss>: 0.0027583122719079256 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07508794), np.complex128(0.003201774184514612+0j)) <f>: (np.float32(-0.0029597653), np.complex128(0.00586144234010663+0j))\n",
      "Epoch 60800: <Test loss>: 0.0027706357650458813 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07509208), np.complex128(0.0032064822371738063+0j)) <f>: (np.float32(-0.0029638824), np.complex128(0.005858611825153732+0j))\n",
      "Epoch 61600: <Test loss>: 0.0027856603264808655 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075072676), np.complex128(0.00322087533974175+0j)) <f>: (np.float32(-0.002944436), np.complex128(0.00583958171189386+0j))\n",
      "Epoch 62400: <Test loss>: 0.002811807207763195 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07510946), np.complex128(0.003233263282783632+0j)) <f>: (np.float32(-0.0029812672), np.complex128(0.005831492061592838+0j))\n",
      "Epoch 63200: <Test loss>: 0.002792009385302663 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075058244), np.complex128(0.003205561228812474+0j)) <f>: (np.float32(-0.0029300132), np.complex128(0.00585719758256253+0j))\n",
      "Epoch 64000: <Test loss>: 0.002839987399056554 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07506044), np.complex128(0.0032514512950101004+0j)) <f>: (np.float32(-0.0029322763), np.complex128(0.005814935223679043+0j))\n",
      "Epoch 64800: <Test loss>: 0.0028273623902350664 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0751099), np.complex128(0.0032112428101445144+0j)) <f>: (np.float32(-0.0029816937), np.complex128(0.005859844910728409+0j))\n",
      "Epoch 65600: <Test loss>: 0.0028479830361902714 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0750669), np.complex128(0.003236690042818935+0j)) <f>: (np.float32(-0.0029386315), np.complex128(0.005831763035953693+0j))\n",
      "Epoch 66400: <Test loss>: 0.0029334446880966425 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07499395), np.complex128(0.003337273797286224+0j)) <f>: (np.float32(-0.0028657843), np.complex128(0.005723511316005364+0j))\n",
      "Epoch 67200: <Test loss>: 0.002884400077164173 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07505775), np.complex128(0.0032531304226506774+0j)) <f>: (np.float32(-0.0029295073), np.complex128(0.005818124500566631+0j))\n",
      "Epoch 68000: <Test loss>: 0.0029105311259627342 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07505433), np.complex128(0.0032842564457227098+0j)) <f>: (np.float32(-0.0029261182), np.complex128(0.005788904939430424+0j))\n",
      "Epoch 68800: <Test loss>: 0.0028927552048116922 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07508495), np.complex128(0.0032333518315214023+0j)) <f>: (np.float32(-0.0029567333), np.complex128(0.00584277454087981+0j))\n",
      "Epoch 69600: <Test loss>: 0.002951543079689145 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07505112), np.complex128(0.0033061959812634074+0j)) <f>: (np.float32(-0.002922965), np.complex128(0.005765179967019563+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce7dfa92",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7473dd90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0028407727368175983 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074505195), np.complex128(0.0026433094740226314+0j)) <f>: (np.float32(-0.0023770032), np.complex128(0.006533954019026605+0j))\n",
      "Epoch 3200: <Test loss>: 0.0025177982170134783 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07496588), np.complex128(0.0026430392608257115+0j)) <f>: (np.float32(-0.0028376605), np.complex128(0.0064953219046703725+0j))\n",
      "Epoch 4800: <Test loss>: 0.002548039657995105 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075357705), np.complex128(0.0026219451246954706+0j)) <f>: (np.float32(-0.0032295207), np.complex128(0.00652703656118546+0j))\n",
      "Epoch 6400: <Test loss>: 0.0026184082962572575 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07545028), np.complex128(0.002676345511128853+0j)) <f>: (np.float32(-0.0033221513), np.complex128(0.006461582044645602+0j))\n",
      "Epoch 8000: <Test loss>: 0.0026948205195367336 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07550982), np.complex128(0.0027637596071930304+0j)) <f>: (np.float32(-0.003381651), np.complex128(0.006387625834394583+0j))\n",
      "Epoch 9600: <Test loss>: 0.0027909581549465656 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.075397916), np.complex128(0.0028873528399722946+0j)) <f>: (np.float32(-0.0032697283), np.complex128(0.006275753511328842+0j))\n",
      "Epoch 11200: <Test loss>: 0.0028314164374023676 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07541035), np.complex128(0.002898679974210119+0j)) <f>: (np.float32(-0.0032821295), np.complex128(0.006282246747136888+0j))\n",
      "Epoch 12800: <Test loss>: 0.0029722596518695354 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07530992), np.complex128(0.0030563926340964+0j)) <f>: (np.float32(-0.0031817295), np.complex128(0.006119303368689389+0j))\n",
      "Epoch 14400: <Test loss>: 0.0029554811771959066 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07529192), np.complex128(0.0029824409908289916+0j)) <f>: (np.float32(-0.0031636665), np.complex128(0.006236953940906914+0j))\n",
      "Epoch 16000: <Test loss>: 0.0031107135582715273 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07503773), np.complex128(0.0031839875594031927+0j)) <f>: (np.float32(-0.0029095665), np.complex128(0.005991642475841565+0j))\n",
      "Epoch 17600: <Test loss>: 0.003179878229275346 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07495572), np.complex128(0.003262226331673749+0j)) <f>: (np.float32(-0.0028274842), np.complex128(0.005926556870088862+0j))\n",
      "Epoch 19200: <Test loss>: 0.00327622564509511 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07494406), np.complex128(0.0033453527912921557+0j)) <f>: (np.float32(-0.002815909), np.complex128(0.005836165608153646+0j))\n",
      "Epoch 20800: <Test loss>: 0.003364118281751871 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07487325), np.complex128(0.00340592520235302+0j)) <f>: (np.float32(-0.0027450041), np.complex128(0.00577855970666873+0j))\n",
      "Epoch 22400: <Test loss>: 0.0034519489854574203 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074814916), np.complex128(0.0035206993360602356+0j)) <f>: (np.float32(-0.0026866922), np.complex128(0.005652181121330044+0j))\n",
      "Epoch 24000: <Test loss>: 0.0034846009220927954 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074841805), np.complex128(0.0034735343202715097+0j)) <f>: (np.float32(-0.00271363), np.complex128(0.005708920818256948+0j))\n",
      "Epoch 25600: <Test loss>: 0.0035568641033023596 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074826844), np.complex128(0.0035745592960997785+0j)) <f>: (np.float32(-0.002698595), np.complex128(0.005605305601557925+0j))\n",
      "Epoch 27200: <Test loss>: 0.0036763856187462807 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074774615), np.complex128(0.0036331217269296593+0j)) <f>: (np.float32(-0.0026464355), np.complex128(0.005559234378343772+0j))\n",
      "Epoch 28800: <Test loss>: 0.003733245423063636 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074683495), np.complex128(0.0037046546449329967+0j)) <f>: (np.float32(-0.0025553326), np.complex128(0.005473813516904008+0j))\n",
      "Epoch 30400: <Test loss>: 0.0037486597429960966 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07472235), np.complex128(0.0036494088590891577+0j)) <f>: (np.float32(-0.0025941462), np.complex128(0.0055554199321442505+0j))\n",
      "Epoch 32000: <Test loss>: 0.003926289267838001 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074703164), np.complex128(0.003813011659225665+0j)) <f>: (np.float32(-0.0025749796), np.complex128(0.005373378950567977+0j))\n",
      "Epoch 33600: <Test loss>: 0.0039651477709412575 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074722186), np.complex128(0.003850083641236489+0j)) <f>: (np.float32(-0.0025939406), np.complex128(0.005337739225361482+0j))\n",
      "Epoch 35200: <Test loss>: 0.003978987690061331 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07468847), np.complex128(0.003876928370895542+0j)) <f>: (np.float32(-0.0025602812), np.complex128(0.005318373692527593+0j))\n",
      "Epoch 36800: <Test loss>: 0.004001815803349018 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0745285), np.complex128(0.003902499165848742+0j)) <f>: (np.float32(-0.0024002807), np.complex128(0.005295636203461719+0j))\n",
      "Epoch 38400: <Test loss>: 0.004137613344937563 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074662395), np.complex128(0.003965036141025803+0j)) <f>: (np.float32(-0.0025341483), np.complex128(0.0052319298267592095+0j))\n",
      "Epoch 40000: <Test loss>: 0.004235065542161465 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074565485), np.complex128(0.004041034807835698+0j)) <f>: (np.float32(-0.002437231), np.complex128(0.005148568674927515+0j))\n",
      "Epoch 41600: <Test loss>: 0.0042502013966441154 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074677594), np.complex128(0.004008011203073748+0j)) <f>: (np.float32(-0.0025493617), np.complex128(0.005209017269988289+0j))\n",
      "Epoch 43200: <Test loss>: 0.004372145049273968 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074614145), np.complex128(0.004137745760201783+0j)) <f>: (np.float32(-0.002485993), np.complex128(0.005059811378239401+0j))\n",
      "Epoch 44800: <Test loss>: 0.004575391765683889 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07447801), np.complex128(0.004332256383082294+0j)) <f>: (np.float32(-0.0023498011), np.complex128(0.004843049142604848+0j))\n",
      "Epoch 46400: <Test loss>: 0.004433361813426018 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07461557), np.complex128(0.004137419220873674+0j)) <f>: (np.float32(-0.0024873558), np.complex128(0.005073988817692433+0j))\n",
      "Epoch 48000: <Test loss>: 0.004548506811261177 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074540414), np.complex128(0.004212579846191151+0j)) <f>: (np.float32(-0.0024121725), np.complex128(0.005002196850229889+0j))\n",
      "Epoch 49600: <Test loss>: 0.004514085128903389 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07460479), np.complex128(0.004184223444957218+0j)) <f>: (np.float32(-0.0024765674), np.complex128(0.005035468848155956+0j))\n",
      "Epoch 51200: <Test loss>: 0.004590100608766079 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07457163), np.complex128(0.0042319012315168115+0j)) <f>: (np.float32(-0.0024433816), np.complex128(0.004985470526479904+0j))\n",
      "Epoch 52800: <Test loss>: 0.004659054335206747 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07447404), np.complex128(0.004270602611791127+0j)) <f>: (np.float32(-0.002345762), np.complex128(0.0049551833060642245+0j))\n",
      "Epoch 54400: <Test loss>: 0.004762559197843075 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07429642), np.complex128(0.004316504856611713+0j)) <f>: (np.float32(-0.0021682153), np.complex128(0.0049169312662328615+0j))\n",
      "Epoch 56000: <Test loss>: 0.00482951570302248 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0743932), np.complex128(0.004381774664036682+0j)) <f>: (np.float32(-0.0022650221), np.complex128(0.004842549819063498+0j))\n",
      "Epoch 57600: <Test loss>: 0.0048993793316185474 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074397214), np.complex128(0.004442250153556491+0j)) <f>: (np.float32(-0.0022690103), np.complex128(0.0047780066694751286+0j))\n",
      "Epoch 59200: <Test loss>: 0.0050108409486711025 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07423588), np.complex128(0.004495308861693807+0j)) <f>: (np.float32(-0.0021076498), np.complex128(0.004731545730326264+0j))\n",
      "Epoch 60800: <Test loss>: 0.005006346385926008 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0742369), np.complex128(0.004483070360504494+0j)) <f>: (np.float32(-0.0021087546), np.complex128(0.004748993637486615+0j))\n",
      "Epoch 62400: <Test loss>: 0.005035496782511473 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07423932), np.complex128(0.0044932283469381815+0j)) <f>: (np.float32(-0.002111082), np.complex128(0.004742023405612645+0j))\n",
      "Epoch 64000: <Test loss>: 0.004993909038603306 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07431756), np.complex128(0.0044312985268599265+0j)) <f>: (np.float32(-0.0021893522), np.complex128(0.004815016997207219+0j))\n",
      "Epoch 65600: <Test loss>: 0.005096133332699537 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074155144), np.complex128(0.004523955520108282+0j)) <f>: (np.float32(-0.0020269332), np.complex128(0.004723869138320628+0j))\n",
      "Epoch 67200: <Test loss>: 0.005156712606549263 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07428706), np.complex128(0.004546137866944239+0j)) <f>: (np.float32(-0.002158847), np.complex128(0.0046982402411870575+0j))\n",
      "Epoch 68800: <Test loss>: 0.005451064091175795 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07391165), np.complex128(0.0047371605829533365+0j)) <f>: (np.float32(-0.0017834448), np.complex128(0.004479534500305177+0j))\n",
      "Epoch 70400: <Test loss>: 0.00520443357527256 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07413761), np.complex128(0.004553074100162447+0j)) <f>: (np.float32(-0.002009379), np.complex128(0.0047054905813891015+0j))\n",
      "Epoch 72000: <Test loss>: 0.005250323098152876 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07418201), np.complex128(0.004587043324252959+0j)) <f>: (np.float32(-0.0020538128), np.complex128(0.004667281674180721+0j))\n",
      "Epoch 73600: <Test loss>: 0.005482695996761322 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073922835), np.complex128(0.004775877947670287+0j)) <f>: (np.float32(-0.0017946182), np.complex128(0.0044590104759622395+0j))\n",
      "Epoch 75200: <Test loss>: 0.005484676454216242 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07405743), np.complex128(0.00475498907208118+0j)) <f>: (np.float32(-0.0019292142), np.complex128(0.004481939270897106+0j))\n",
      "Epoch 76800: <Test loss>: 0.005470719188451767 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07401926), np.complex128(0.004735493633935719+0j)) <f>: (np.float32(-0.0018910426), np.complex128(0.004512530449444155+0j))\n",
      "Epoch 78400: <Test loss>: 0.005466256756335497 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07397015), np.complex128(0.004727969274717081+0j)) <f>: (np.float32(-0.0018418484), np.complex128(0.0045239590722066444+0j))\n",
      "Epoch 80000: <Test loss>: 0.0055145625956356525 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073947385), np.complex128(0.0047486013843387855+0j)) <f>: (np.float32(-0.0018191879), np.complex128(0.0045021973953054215+0j))\n",
      "Epoch 81600: <Test loss>: 0.005520009435713291 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388306), np.complex128(0.004756190188770586+0j)) <f>: (np.float32(-0.001754815), np.complex128(0.004503916610913241+0j))\n",
      "Epoch 83200: <Test loss>: 0.005594470538198948 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07392898), np.complex128(0.004785738065284083+0j)) <f>: (np.float32(-0.0018007876), np.complex128(0.004470611121774035+0j))\n",
      "Epoch 84800: <Test loss>: 0.005654850043356419 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388147), np.complex128(0.004831059795743587+0j)) <f>: (np.float32(-0.001753309), np.complex128(0.0044275190942014144+0j))\n",
      "Epoch 86400: <Test loss>: 0.005681989714503288 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389813), np.complex128(0.004864313018292591+0j)) <f>: (np.float32(-0.0017699244), np.complex128(0.004387543271778622+0j))\n",
      "Epoch 88000: <Test loss>: 0.005780923645943403 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07384555), np.complex128(0.004888207983982266+0j)) <f>: (np.float32(-0.0017172928), np.complex128(0.004368426893272784+0j))\n",
      "Epoch 89600: <Test loss>: 0.005746607203036547 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073852226), np.complex128(0.004870274961674017+0j)) <f>: (np.float32(-0.0017240302), np.complex128(0.004388608901287601+0j))\n",
      "Epoch 91200: <Test loss>: 0.005817893426865339 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07382258), np.complex128(0.0049125388428853745+0j)) <f>: (np.float32(-0.0016943441), np.complex128(0.004348887307619094+0j))\n",
      "Epoch 92800: <Test loss>: 0.005831331945955753 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073802315), np.complex128(0.004914624432067234+0j)) <f>: (np.float32(-0.0016741252), np.complex128(0.004350783620702453+0j))\n",
      "Epoch 94400: <Test loss>: 0.005867458879947662 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07378544), np.complex128(0.004935463070836631+0j)) <f>: (np.float32(-0.0016571793), np.complex128(0.004330558987507277+0j))\n",
      "Epoch 96000: <Test loss>: 0.005927018355578184 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07378423), np.complex128(0.004968390007778839+0j)) <f>: (np.float32(-0.0016560217), np.complex128(0.004297965947811217+0j))\n",
      "Epoch 97600: <Test loss>: 0.005925879348069429 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07377828), np.complex128(0.004934725249262319+0j)) <f>: (np.float32(-0.0016500959), np.complex128(0.004338701411841124+0j))\n",
      "Epoch 99200: <Test loss>: 0.005983393639326096 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07370375), np.complex128(0.004982383245559604+0j)) <f>: (np.float32(-0.0015755708), np.complex128(0.004283737256653229+0j))\n",
      "Epoch 100800: <Test loss>: 0.005957701709121466 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073631614), np.complex128(0.0049421790739563164+0j)) <f>: (np.float32(-0.001503402), np.complex128(0.004334271945182135+0j))\n",
      "Epoch 102400: <Test loss>: 0.006046829279512167 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07370167), np.complex128(0.0050237656989342485+0j)) <f>: (np.float32(-0.0015734566), np.complex128(0.004244685740968821+0j))\n",
      "Epoch 104000: <Test loss>: 0.006056865677237511 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07366906), np.complex128(0.005026673852608515+0j)) <f>: (np.float32(-0.0015408098), np.complex128(0.004245916543051692+0j))\n",
      "Epoch 105600: <Test loss>: 0.006151987239718437 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07361974), np.complex128(0.005095561218495156+0j)) <f>: (np.float32(-0.0014915065), np.complex128(0.004169085162896923+0j))\n",
      "Epoch 107200: <Test loss>: 0.0061279889196157455 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07368619), np.complex128(0.005052336748397847+0j)) <f>: (np.float32(-0.0015579152), np.complex128(0.004222356489493411+0j))\n",
      "Epoch 108800: <Test loss>: 0.006347439251840115 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0737136), np.complex128(0.0051705500744846375+0j)) <f>: (np.float32(-0.0015853937), np.complex128(0.004103282794438777+0j))\n",
      "Epoch 110400: <Test loss>: 0.006207624915987253 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07358768), np.complex128(0.005105365517420387+0j)) <f>: (np.float32(-0.0014594807), np.complex128(0.0041669526352724065+0j))\n",
      "Epoch 112000: <Test loss>: 0.006222602911293507 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07360472), np.complex128(0.0051077636912582125+0j)) <f>: (np.float32(-0.0014764502), np.complex128(0.004163216081515565+0j))\n",
      "Epoch 113600: <Test loss>: 0.006321781314909458 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07355347), np.complex128(0.005180169156852355+0j)) <f>: (np.float32(-0.0014253445), np.complex128(0.004081570846914639+0j))\n",
      "Epoch 115200: <Test loss>: 0.006276862230151892 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07360057), np.complex128(0.005079958880155832+0j)) <f>: (np.float32(-0.0014723517), np.complex128(0.004200527830165909+0j))\n",
      "Epoch 116800: <Test loss>: 0.006318958941847086 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073616296), np.complex128(0.0051315186027838505+0j)) <f>: (np.float32(-0.0014881149), np.complex128(0.004144891313502111+0j))\n",
      "Epoch 118400: <Test loss>: 0.006348226219415665 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07359427), np.complex128(0.0051449825779084895+0j)) <f>: (np.float32(-0.0014660364), np.complex128(0.004136218611626891+0j))\n",
      "Epoch 120000: <Test loss>: 0.006435954477638006 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07356641), np.complex128(0.005211261181268625+0j)) <f>: (np.float32(-0.0014381689), np.complex128(0.004060658375243548+0j))\n",
      "Epoch 121600: <Test loss>: 0.006379749625921249 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07361474), np.complex128(0.005139670161084917+0j)) <f>: (np.float32(-0.0014865617), np.complex128(0.00414748536019254+0j))\n",
      "Epoch 123200: <Test loss>: 0.006482807453721762 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073507704), np.complex128(0.005225874006492468+0j)) <f>: (np.float32(-0.0013794517), np.complex128(0.004052845788614861+0j))\n",
      "Epoch 124800: <Test loss>: 0.00646241195499897 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07355844), np.complex128(0.005196003396470538+0j)) <f>: (np.float32(-0.0014301989), np.complex128(0.004090662950118036+0j))\n",
      "Epoch 126400: <Test loss>: 0.00669939024373889 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073462285), np.complex128(0.005370584971483958+0j)) <f>: (np.float32(-0.0013340506), np.complex128(0.0038998942091419114+0j))\n",
      "Epoch 128000: <Test loss>: 0.006530201993882656 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073423006), np.complex128(0.005196937598340076+0j)) <f>: (np.float32(-0.0012947675), np.complex128(0.004107950759130728+0j))\n",
      "Epoch 129600: <Test loss>: 0.006549806334078312 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07351113), np.complex128(0.005246795358409467+0j)) <f>: (np.float32(-0.0013829379), np.complex128(0.004040581154130447+0j))\n",
      "Epoch 131200: <Test loss>: 0.006553685758262873 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07355415), np.complex128(0.005216162062124682+0j)) <f>: (np.float32(-0.0014259889), np.complex128(0.004076401021468221+0j))\n",
      "Epoch 132800: <Test loss>: 0.006580840330570936 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07354417), np.complex128(0.005235219069843591+0j)) <f>: (np.float32(-0.0014159799), np.complex128(0.004060262316276044+0j))\n",
      "Epoch 134400: <Test loss>: 0.006646160501986742 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073500834), np.complex128(0.005273888227511326+0j)) <f>: (np.float32(-0.0013726281), np.complex128(0.004020328865312299+0j))\n",
      "Epoch 136000: <Test loss>: 0.006586430594325066 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073545694), np.complex128(0.005214981243140209+0j)) <f>: (np.float32(-0.0014174742), np.complex128(0.004087368886328732+0j))\n",
      "Epoch 137600: <Test loss>: 0.006627829745411873 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07353991), np.complex128(0.005234096099318177+0j)) <f>: (np.float32(-0.001411691), np.complex128(0.004068324057232783+0j))\n",
      "Epoch 139200: <Test loss>: 0.006722608115524054 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07354555), np.complex128(0.005273993775576977+0j)) <f>: (np.float32(-0.001417335), np.complex128(0.004035379613520071+0j))\n",
      "Epoch 140800: <Test loss>: 0.0067448304034769535 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07352599), np.complex128(0.005291448279491431+0j)) <f>: (np.float32(-0.0013978496), np.complex128(0.004009016954353174+0j))\n",
      "Epoch 142400: <Test loss>: 0.006811575964093208 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07342583), np.complex128(0.005366706080071274+0j)) <f>: (np.float32(-0.0012976454), np.complex128(0.0039228877030112515+0j))\n",
      "Epoch 144000: <Test loss>: 0.006767380051314831 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07347413), np.complex128(0.005314376566983674+0j)) <f>: (np.float32(-0.0013458749), np.complex128(0.00398536175530302+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f633ff4",
   "metadata": {},
   "source": [
    "### l4, w64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "001efc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48012efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.000740960065741092 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07345032), np.complex128(0.0012784218941107226+0j)) <f>: (np.float32(-0.0013220906), np.complex128(0.008294565273729045+0j))\n",
      "Epoch 200: <Test loss>: 0.0005061859847046435 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07394504), np.complex128(0.0010191662229080871+0j)) <f>: (np.float32(-0.0018168312), np.complex128(0.008353114511050719+0j))\n",
      "Epoch 300: <Test loss>: 0.0003864647587761283 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07384706), np.complex128(0.001015466141589797+0j)) <f>: (np.float32(-0.0017188891), np.complex128(0.008123465262554247+0j))\n",
      "Epoch 400: <Test loss>: 0.0003342688432894647 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0737116), np.complex128(0.0009147543847207744+0j)) <f>: (np.float32(-0.0015833615), np.complex128(0.008190046809160507+0j))\n",
      "Epoch 500: <Test loss>: 0.0003128606185782701 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07379959), np.complex128(0.0010444884344061865+0j)) <f>: (np.float32(-0.0016713594), np.complex128(0.007987056566322951+0j))\n",
      "Epoch 600: <Test loss>: 0.0002240394096588716 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07374813), np.complex128(0.0007721278671652413+0j)) <f>: (np.float32(-0.0016199053), np.complex128(0.008295039732581853+0j))\n",
      "Epoch 700: <Test loss>: 0.00021132595429662615 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07370347), np.complex128(0.0007595813483035708+0j)) <f>: (np.float32(-0.0015752709), np.complex128(0.00828878195015103+0j))\n",
      "Epoch 800: <Test loss>: 0.00018657339387573302 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07378124), np.complex128(0.0007581359614213621+0j)) <f>: (np.float32(-0.0016529998), np.complex128(0.008260090129343079+0j))\n",
      "Epoch 900: <Test loss>: 0.0001706644252408296 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07372789), np.complex128(0.0007334194461245272+0j)) <f>: (np.float32(-0.0015996809), np.complex128(0.008283136143523932+0j))\n",
      "Epoch 1000: <Test loss>: 0.00015974311099853367 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07378893), np.complex128(0.0007445637107269714+0j)) <f>: (np.float32(-0.0016607312), np.complex128(0.008249098922121894+0j))\n",
      "Epoch 1100: <Test loss>: 0.0001422239001840353 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07386468), np.complex128(0.0006401145755068443+0j)) <f>: (np.float32(-0.0017364449), np.complex128(0.008368711267521187+0j))\n",
      "Epoch 1200: <Test loss>: 0.00013051657879259437 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07383722), np.complex128(0.0006280685219324266+0j)) <f>: (np.float32(-0.0017090513), np.complex128(0.008369945875423732+0j))\n",
      "Epoch 1300: <Test loss>: 0.00011545764573384076 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388003), np.complex128(0.0005612105459571114+0j)) <f>: (np.float32(-0.0017518396), np.complex128(0.00843975780177007+0j))\n",
      "Epoch 1400: <Test loss>: 0.00010339319851482287 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07387003), np.complex128(0.0005437661908951237+0j)) <f>: (np.float32(-0.0017418128), np.complex128(0.008458470763390366+0j))\n",
      "Epoch 1500: <Test loss>: 0.00010497448238311335 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07385412), np.complex128(0.0005692486274015374+0j)) <f>: (np.float32(-0.0017258814), np.complex128(0.00841455820109583+0j))\n",
      "Epoch 1600: <Test loss>: 9.786228474695235e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389161), np.complex128(0.0005568944292941065+0j)) <f>: (np.float32(-0.0017633883), np.complex128(0.008422226674019493+0j))\n",
      "Epoch 1700: <Test loss>: 8.75077021191828e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389422), np.complex128(0.0005066055650010379+0j)) <f>: (np.float32(-0.0017660345), np.complex128(0.008473229732089726+0j))\n",
      "Epoch 1800: <Test loss>: 9.316305659012869e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388935), np.complex128(0.0005547794718704225+0j)) <f>: (np.float32(-0.001761111), np.complex128(0.00841457342437453+0j))\n",
      "Epoch 1900: <Test loss>: 8.951675408752635e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07387716), np.complex128(0.0005652872131322355+0j)) <f>: (np.float32(-0.0017489508), np.complex128(0.0083914213546854+0j))\n",
      "Epoch 2000: <Test loss>: 7.47059821151197e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073909454), np.complex128(0.00048342019438975335+0j)) <f>: (np.float32(-0.00178122), np.complex128(0.008489973308888904+0j))\n",
      "Epoch 2100: <Test loss>: 6.331232725642622e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07391796), np.complex128(0.0004083827507690886+0j)) <f>: (np.float32(-0.0017897247), np.complex128(0.008567165988191478+0j))\n",
      "Epoch 2200: <Test loss>: 6.78646465530619e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073909216), np.complex128(0.00044738529824034644+0j)) <f>: (np.float32(-0.0017809622), np.complex128(0.008520501564550661+0j))\n",
      "Epoch 2300: <Test loss>: 6.724323611706495e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073869206), np.complex128(0.0005071801486264598+0j)) <f>: (np.float32(-0.0017410135), np.complex128(0.008439250359146746+0j))\n",
      "Epoch 2400: <Test loss>: 6.425996980397031e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07390338), np.complex128(0.0004576397622027931+0j)) <f>: (np.float32(-0.0017751905), np.complex128(0.008497696078173262+0j))\n",
      "Epoch 2500: <Test loss>: 5.605319529422559e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073969714), np.complex128(0.00042161095536486604+0j)) <f>: (np.float32(-0.0018414538), np.complex128(0.008541687294074412+0j))\n",
      "Epoch 2600: <Test loss>: 5.0046484830090776e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073939), np.complex128(0.00037730347584872983+0j)) <f>: (np.float32(-0.0018107735), np.complex128(0.008589956251290177+0j))\n",
      "Epoch 2700: <Test loss>: 5.9575435443548486e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07396206), np.complex128(0.00044536938727370246+0j)) <f>: (np.float32(-0.0018337889), np.complex128(0.008497293168730343+0j))\n",
      "Epoch 2800: <Test loss>: 4.6736240619793534e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0739574), np.complex128(0.00037213932741665977+0j)) <f>: (np.float32(-0.0018291892), np.complex128(0.008584676818237122+0j))\n",
      "Epoch 2900: <Test loss>: 3.8530615711351857e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073959984), np.complex128(0.0002671793243355019+0j)) <f>: (np.float32(-0.0018318816), np.complex128(0.008758509407938516+0j))\n",
      "Epoch 3000: <Test loss>: 4.7077835915843025e-05 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07393432), np.complex128(0.0004005835479395921+0j)) <f>: (np.float32(-0.0018060703), np.complex128(0.00854605231552024+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34a03f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87efe994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.001271231914870441 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0736407), np.complex128(0.0015485467960139607+0j)) <f>: (np.float32(-0.0015124929), np.complex128(0.00826481442016622+0j))\n",
      "Epoch 400: <Test loss>: 0.0009242826490662992 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07366915), np.complex128(0.0012925490967440453+0j)) <f>: (np.float32(-0.0015409387), np.complex128(0.00806675398241425+0j))\n",
      "Epoch 600: <Test loss>: 0.0006817739340476692 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0737398), np.complex128(0.0010100765934874794+0j)) <f>: (np.float32(-0.0016115489), np.complex128(0.008230767557354338+0j))\n",
      "Epoch 800: <Test loss>: 0.0005811892333440483 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07364411), np.complex128(0.0009808038143061517+0j)) <f>: (np.float32(-0.0015159276), np.complex128(0.008187427897781535+0j))\n",
      "Epoch 1000: <Test loss>: 0.00045827007852494717 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07374979), np.complex128(0.0007635855146138876+0j)) <f>: (np.float32(-0.0016216066), np.complex128(0.008442170184001348+0j))\n",
      "Epoch 1200: <Test loss>: 0.00046018767170608044 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073829874), np.complex128(0.0009274244658612241+0j)) <f>: (np.float32(-0.0017016751), np.complex128(0.008146661479751604+0j))\n",
      "Epoch 1400: <Test loss>: 0.00043215774348936975 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07382092), np.complex128(0.0009068507121412012+0j)) <f>: (np.float32(-0.0016927329), np.complex128(0.008136837390564065+0j))\n",
      "Epoch 1600: <Test loss>: 0.00036672057467512786 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07390765), np.complex128(0.0007139103070282368+0j)) <f>: (np.float32(-0.0017794762), np.complex128(0.008397165097738798+0j))\n",
      "Epoch 1800: <Test loss>: 0.0003790092596318573 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073935024), np.complex128(0.0008348425106907933+0j)) <f>: (np.float32(-0.0018067841), np.complex128(0.008221909131478982+0j))\n",
      "Epoch 2000: <Test loss>: 0.0003609601699281484 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073874615), np.complex128(0.0008211229747749829+0j)) <f>: (np.float32(-0.001746447), np.complex128(0.008223933827546041+0j))\n",
      "Epoch 2200: <Test loss>: 0.0003490432573016733 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388705), np.complex128(0.0008015614422278343+0j)) <f>: (np.float32(-0.0017588248), np.complex128(0.008238592830048607+0j))\n",
      "Epoch 2400: <Test loss>: 0.00034229742595925927 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07390593), np.complex128(0.0008409141251091853+0j)) <f>: (np.float32(-0.0017777674), np.complex128(0.008199952089167779+0j))\n",
      "Epoch 2600: <Test loss>: 0.0003275641065556556 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073929876), np.complex128(0.0007927703159303965+0j)) <f>: (np.float32(-0.0018016628), np.complex128(0.008243528724445674+0j))\n",
      "Epoch 2800: <Test loss>: 0.0003170062555000186 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388853), np.complex128(0.0008196701031140802+0j)) <f>: (np.float32(-0.0017602737), np.complex128(0.008205884600877053+0j))\n",
      "Epoch 3000: <Test loss>: 0.0003133499121759087 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389123), np.complex128(0.0008325307289595877+0j)) <f>: (np.float32(-0.0017630077), np.complex128(0.008186055772928068+0j))\n",
      "Epoch 3200: <Test loss>: 0.00029742319020442665 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07390447), np.complex128(0.0008027463207532945+0j)) <f>: (np.float32(-0.0017762567), np.complex128(0.008214436531407921+0j))\n",
      "Epoch 3400: <Test loss>: 0.0002678830933291465 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07394208), np.complex128(0.0007257841472623641+0j)) <f>: (np.float32(-0.0018138591), np.complex128(0.008311672686889147+0j))\n",
      "Epoch 3600: <Test loss>: 0.00027433218201622367 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0739052), np.complex128(0.0007769443222548429+0j)) <f>: (np.float32(-0.0017769729), np.complex128(0.008243355686511121+0j))\n",
      "Epoch 3800: <Test loss>: 0.00026596570387482643 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389791), np.complex128(0.0007526475254378244+0j)) <f>: (np.float32(-0.001769689), np.complex128(0.008271190436728278+0j))\n",
      "Epoch 4000: <Test loss>: 0.0002819703659042716 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07383894), np.complex128(0.00085475189515623+0j)) <f>: (np.float32(-0.001710745), np.complex128(0.008144162324831738+0j))\n",
      "Epoch 4200: <Test loss>: 0.00025390260270796716 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388452), np.complex128(0.000766956329099969+0j)) <f>: (np.float32(-0.0017563221), np.complex128(0.008255638335208662+0j))\n",
      "Epoch 4400: <Test loss>: 0.00023355324810836464 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07388781), np.complex128(0.0006974391634874555+0j)) <f>: (np.float32(-0.001759614), np.complex128(0.008337546678809782+0j))\n",
      "Epoch 4600: <Test loss>: 0.0002351565781282261 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073900834), np.complex128(0.0007334920104196625+0j)) <f>: (np.float32(-0.0017726733), np.complex128(0.00828432203693464+0j))\n",
      "Epoch 4800: <Test loss>: 0.0002426502906018868 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07387249), np.complex128(0.0007857898717736323+0j)) <f>: (np.float32(-0.0017442139), np.complex128(0.008218136295574573+0j))\n",
      "Epoch 5000: <Test loss>: 0.00021882665168959647 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073891945), np.complex128(0.0006976691618564768+0j)) <f>: (np.float32(-0.001763679), np.complex128(0.008322777053815332+0j))\n",
      "Epoch 5200: <Test loss>: 0.00021189762628637254 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07390183), np.complex128(0.0006768973152223741+0j)) <f>: (np.float32(-0.0017736136), np.complex128(0.008344267248913077+0j))\n",
      "Epoch 5400: <Test loss>: 0.0002000085514737293 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07392498), np.complex128(0.0006413915548684376+0j)) <f>: (np.float32(-0.001796788), np.complex128(0.008387388200715227+0j))\n",
      "Epoch 5600: <Test loss>: 0.0001988288713619113 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073878996), np.complex128(0.0006706723897014104+0j)) <f>: (np.float32(-0.0017508223), np.complex128(0.00834119265405836+0j))\n",
      "Epoch 5800: <Test loss>: 0.0002036744699580595 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07383369), np.complex128(0.000696074586843011+0j)) <f>: (np.float32(-0.0017054281), np.complex128(0.008306242543376963+0j))\n",
      "Epoch 6000: <Test loss>: 0.00020900524395983666 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07382537), np.complex128(0.0007396305438340053+0j)) <f>: (np.float32(-0.0016972113), np.complex128(0.008252003523697798+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb2f5b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e661ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.0021808315068483353 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07444818), np.complex128(0.002242353177761044+0j)) <f>: (np.float32(-0.0023199935), np.complex128(0.007223057041958759+0j))\n",
      "Epoch 800: <Test loss>: 0.001979802269488573 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074302524), np.complex128(0.0023518283418742054+0j)) <f>: (np.float32(-0.0021742415), np.complex128(0.006883201404528917+0j))\n",
      "Epoch 1200: <Test loss>: 0.001846162835136056 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073989004), np.complex128(0.002354436089515464+0j)) <f>: (np.float32(-0.0018607803), np.complex128(0.006818505007268303+0j))\n",
      "Epoch 1600: <Test loss>: 0.0017341254279017448 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074174605), np.complex128(0.0023206338140480246+0j)) <f>: (np.float32(-0.002046387), np.complex128(0.006852292059457043+0j))\n",
      "Epoch 2000: <Test loss>: 0.00164719857275486 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073934585), np.complex128(0.002306388631004779+0j)) <f>: (np.float32(-0.0018063657), np.complex128(0.006844139486270729+0j))\n",
      "Epoch 2400: <Test loss>: 0.001642104354687035 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0738391), np.complex128(0.0023836087122090133+0j)) <f>: (np.float32(-0.0017108748), np.complex128(0.006724789488707695+0j))\n",
      "Epoch 2800: <Test loss>: 0.0016315876273438334 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07378039), np.complex128(0.00240559543991368+0j)) <f>: (np.float32(-0.0016522259), np.complex128(0.0066970582567856965+0j))\n",
      "Epoch 3200: <Test loss>: 0.0015979479067027569 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07369605), np.complex128(0.0023766874485481944+0j)) <f>: (np.float32(-0.0015678421), np.complex128(0.006729444259891441+0j))\n",
      "Epoch 3600: <Test loss>: 0.0016655391082167625 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073662475), np.complex128(0.002493247780289508+0j)) <f>: (np.float32(-0.001534304), np.complex128(0.0065955798809734885+0j))\n",
      "Epoch 4000: <Test loss>: 0.001635706052184105 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07360923), np.complex128(0.0024506423901926545+0j)) <f>: (np.float32(-0.00148098), np.complex128(0.006643702187271114+0j))\n",
      "Epoch 4400: <Test loss>: 0.0016464804066345096 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07358364), np.complex128(0.0024733070612420814+0j)) <f>: (np.float32(-0.0014554422), np.complex128(0.00661313485852736+0j))\n",
      "Epoch 4800: <Test loss>: 0.0016790725057944655 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07353297), np.complex128(0.0025225972467372857+0j)) <f>: (np.float32(-0.0014047395), np.complex128(0.006557706900781748+0j))\n",
      "Epoch 5200: <Test loss>: 0.001659212401136756 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073498406), np.complex128(0.0025114807011881405+0j)) <f>: (np.float32(-0.0013701633), np.complex128(0.006567399054887225+0j))\n",
      "Epoch 5600: <Test loss>: 0.0016981536755338311 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07352945), np.complex128(0.0025742297873817504+0j)) <f>: (np.float32(-0.0014012364), np.complex128(0.006499202318410926+0j))\n",
      "Epoch 6000: <Test loss>: 0.0016760345315560699 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07353401), np.complex128(0.0025436530709494654+0j)) <f>: (np.float32(-0.0014057148), np.complex128(0.006533082740042358+0j))\n",
      "Epoch 6400: <Test loss>: 0.0016658841632306576 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073536664), np.complex128(0.002552945360267763+0j)) <f>: (np.float32(-0.0014085169), np.complex128(0.006521323264689462+0j))\n",
      "Epoch 6800: <Test loss>: 0.0017862434033304453 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073556244), np.complex128(0.0026954606210281328+0j)) <f>: (np.float32(-0.0014280326), np.complex128(0.006367390544904317+0j))\n",
      "Epoch 7200: <Test loss>: 0.0017090648179873824 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07348406), np.complex128(0.002598095828841895+0j)) <f>: (np.float32(-0.001355818), np.complex128(0.0064732912831787885+0j))\n",
      "Epoch 7600: <Test loss>: 0.0016746331239119172 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07353067), np.complex128(0.0025444444277205385+0j)) <f>: (np.float32(-0.0014024668), np.complex128(0.006528626378924332+0j))\n",
      "Epoch 8000: <Test loss>: 0.0017157132970169187 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073509075), np.complex128(0.0025936064839533536+0j)) <f>: (np.float32(-0.0013808587), np.complex128(0.0064701877640945425+0j))\n",
      "Epoch 8400: <Test loss>: 0.0017093588830903172 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073493145), np.complex128(0.0026010968445162297+0j)) <f>: (np.float32(-0.0013649504), np.complex128(0.006464632789697022+0j))\n",
      "Epoch 8800: <Test loss>: 0.0017190129728987813 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07352035), np.complex128(0.002601048637467014+0j)) <f>: (np.float32(-0.0013920808), np.complex128(0.006468733940978721+0j))\n",
      "Epoch 9200: <Test loss>: 0.00172374804969877 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07353404), np.complex128(0.002610411715031265+0j)) <f>: (np.float32(-0.0014058704), np.complex128(0.006455023348739147+0j))\n",
      "Epoch 9600: <Test loss>: 0.0017526510637253523 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07352562), np.complex128(0.0026448264737450565+0j)) <f>: (np.float32(-0.0013974332), np.complex128(0.006423140221273116+0j))\n",
      "Epoch 10000: <Test loss>: 0.0017140659037977457 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07356425), np.complex128(0.002585219979717688+0j)) <f>: (np.float32(-0.0014360228), np.complex128(0.0064910619238475725+0j))\n",
      "Epoch 10400: <Test loss>: 0.0017944182036444545 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073520415), np.complex128(0.0026926440607473764+0j)) <f>: (np.float32(-0.0013922061), np.complex128(0.006367422513789587+0j))\n",
      "Epoch 10800: <Test loss>: 0.0017718342132866383 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07353854), np.complex128(0.0026656514115636245+0j)) <f>: (np.float32(-0.0014102807), np.complex128(0.006395222250465734+0j))\n",
      "Epoch 11200: <Test loss>: 0.0017529912292957306 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07356113), np.complex128(0.002606331368897122+0j)) <f>: (np.float32(-0.0014329113), np.complex128(0.006465992228484905+0j))\n",
      "Epoch 11600: <Test loss>: 0.0017845932161435485 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07355401), np.complex128(0.0026578045725578633+0j)) <f>: (np.float32(-0.0014257905), np.complex128(0.0064126118017244025+0j))\n",
      "Epoch 12000: <Test loss>: 0.0018163514323532581 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07348731), np.complex128(0.0027207244131942218+0j)) <f>: (np.float32(-0.0013590393), np.complex128(0.006344485092330123+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61115e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3680e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0037145335227251053 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389492), np.complex128(0.002817664222183426+0j)) <f>: (np.float32(-0.0017666594), np.complex128(0.006444944015912075+0j))\n",
      "Epoch 1600: <Test loss>: 0.003679769579321146 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07389384), np.complex128(0.0030616444115264857+0j)) <f>: (np.float32(-0.0017656067), np.complex128(0.006056780855627093+0j))\n",
      "Epoch 2400: <Test loss>: 0.003751849289983511 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07338317), np.complex128(0.003393567959589849+0j)) <f>: (np.float32(-0.0012549388), np.complex128(0.005696822371231671+0j))\n",
      "Epoch 3200: <Test loss>: 0.0038125189021229744 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07311338), np.complex128(0.003502342852882824+0j)) <f>: (np.float32(-0.0009851949), np.complex128(0.005594213920697322+0j))\n",
      "Epoch 4000: <Test loss>: 0.003928776830434799 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073037416), np.complex128(0.003644006624921257+0j)) <f>: (np.float32(-0.0009092315), np.complex128(0.005441011410847139+0j))\n",
      "Epoch 4800: <Test loss>: 0.003945467993617058 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07298987), np.complex128(0.003642164861919905+0j)) <f>: (np.float32(-0.0008616331), np.complex128(0.005437858669828431+0j))\n",
      "Epoch 5600: <Test loss>: 0.004014953039586544 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0730138), np.complex128(0.0036807358294400263+0j)) <f>: (np.float32(-0.000885633), np.complex128(0.005390460991596911+0j))\n",
      "Epoch 6400: <Test loss>: 0.004138923715800047 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073040195), np.complex128(0.0037769758750516675+0j)) <f>: (np.float32(-0.0009119532), np.complex128(0.005292548415098796+0j))\n",
      "Epoch 7200: <Test loss>: 0.0040953741408884525 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.072994), np.complex128(0.0037169078619583864+0j)) <f>: (np.float32(-0.00086572726), np.complex128(0.005357100698654386+0j))\n",
      "Epoch 8000: <Test loss>: 0.004115953575819731 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07293814), np.complex128(0.003754455571428576+0j)) <f>: (np.float32(-0.00080992497), np.complex128(0.005314807893213499+0j))\n",
      "Epoch 8800: <Test loss>: 0.004191228188574314 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07283439), np.complex128(0.0038718729737606834+0j)) <f>: (np.float32(-0.00070621324), np.complex128(0.005189063103711344+0j))\n",
      "Epoch 9600: <Test loss>: 0.004221274051815271 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07288463), np.complex128(0.003874576881779062+0j)) <f>: (np.float32(-0.00075638224), np.complex128(0.005186542636201297+0j))\n",
      "Epoch 10400: <Test loss>: 0.004211402032524347 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073000886), np.complex128(0.0038594287045876125+0j)) <f>: (np.float32(-0.0008726166), np.complex128(0.005205345415165921+0j))\n",
      "Epoch 11200: <Test loss>: 0.004339384846389294 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.072958894), np.complex128(0.003966832234191056+0j)) <f>: (np.float32(-0.00083069946), np.complex128(0.005079822378090158+0j))\n",
      "Epoch 12000: <Test loss>: 0.004242134280502796 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07307343), np.complex128(0.003884797029934106+0j)) <f>: (np.float32(-0.0009452255), np.complex128(0.005180562424885431+0j))\n",
      "Epoch 12800: <Test loss>: 0.004256564658135176 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07304607), np.complex128(0.0039042665885057775+0j)) <f>: (np.float32(-0.0009178785), np.complex128(0.005161670843461725+0j))\n",
      "Epoch 13600: <Test loss>: 0.004257991909980774 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07302629), np.complex128(0.003891879914070454+0j)) <f>: (np.float32(-0.0008980588), np.complex128(0.0051859281231844525+0j))\n",
      "Epoch 14400: <Test loss>: 0.004311194643378258 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07305679), np.complex128(0.003943400817337787+0j)) <f>: (np.float32(-0.0009285657), np.complex128(0.005124508282942638+0j))\n",
      "Epoch 15200: <Test loss>: 0.004313599318265915 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073067464), np.complex128(0.003946014908011838+0j)) <f>: (np.float32(-0.0009392391), np.complex128(0.005126460922157186+0j))\n",
      "Epoch 16000: <Test loss>: 0.004334841854870319 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07307397), np.complex128(0.003950575548588957+0j)) <f>: (np.float32(-0.0009457516), np.complex128(0.005123070190548139+0j))\n",
      "Epoch 16800: <Test loss>: 0.0043432810343801975 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073046304), np.complex128(0.003970928057325211+0j)) <f>: (np.float32(-0.00091809634), np.complex128(0.005098824074563125+0j))\n",
      "Epoch 17600: <Test loss>: 0.004338712897151709 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07303832), np.complex128(0.003955942261773225+0j)) <f>: (np.float32(-0.00091008196), np.complex128(0.0051227611579905356+0j))\n",
      "Epoch 18400: <Test loss>: 0.0043930415995419025 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073052086), np.complex128(0.004017212913883782+0j)) <f>: (np.float32(-0.0009238951), np.complex128(0.005052808162594915+0j))\n",
      "Epoch 19200: <Test loss>: 0.0042757922783494 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07314927), np.complex128(0.0038676421708887247+0j)) <f>: (np.float32(-0.0010210605), np.complex128(0.005233306518596286+0j))\n",
      "Epoch 20000: <Test loss>: 0.004375832621008158 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07308145), np.complex128(0.003999125121575421+0j)) <f>: (np.float32(-0.00095325476), np.complex128(0.005078858237105844+0j))\n",
      "Epoch 20800: <Test loss>: 0.004407696891576052 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07309435), np.complex128(0.004049123189530161+0j)) <f>: (np.float32(-0.00096618105), np.complex128(0.0050234206379503885+0j))\n",
      "Epoch 21600: <Test loss>: 0.004303738474845886 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07315324), np.complex128(0.0039200381589599795+0j)) <f>: (np.float32(-0.0010250444), np.complex128(0.005159417290771546+0j))\n",
      "Epoch 22400: <Test loss>: 0.004422944504767656 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07310834), np.complex128(0.004029850772417651+0j)) <f>: (np.float32(-0.0009800831), np.complex128(0.005042860764849907+0j))\n",
      "Epoch 23200: <Test loss>: 0.004421515855938196 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073121294), np.complex128(0.004032787596600135+0j)) <f>: (np.float32(-0.0009930353), np.complex128(0.0050416520365211505+0j))\n",
      "Epoch 24000: <Test loss>: 0.0044607617892324924 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0731135), np.complex128(0.00407368924808918+0j)) <f>: (np.float32(-0.0009853325), np.complex128(0.00499767147691509+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9131645",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd93ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.016603002324700356 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.006800324656069279 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07433204), np.complex128(0.004631223308809986+0j)) <f>: (np.float32(-0.0022038682), np.complex128(0.004712801814705945+0j))\n",
      "Epoch 3200: <Test loss>: 0.006474036257714033 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07393667), np.complex128(0.00446761924006692+0j)) <f>: (np.float32(-0.0018084208), np.complex128(0.004806566047758391+0j))\n",
      "Epoch 4800: <Test loss>: 0.007625317666679621 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073716424), np.complex128(0.005230751544987852+0j)) <f>: (np.float32(-0.0015881923), np.complex128(0.004019842227836533+0j))\n",
      "Epoch 6400: <Test loss>: 0.007752508390694857 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073596), np.complex128(0.005362869306396326+0j)) <f>: (np.float32(-0.0014677945), np.complex128(0.0039020000960287037+0j))\n",
      "Epoch 8000: <Test loss>: 0.007866770029067993 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073700964), np.complex128(0.005441076870945548+0j)) <f>: (np.float32(-0.0015727325), np.complex128(0.003827636663072468+0j))\n",
      "Epoch 9600: <Test loss>: 0.00793053675442934 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07377288), np.complex128(0.005557035629456912+0j)) <f>: (np.float32(-0.0016446102), np.complex128(0.003689026173298572+0j))\n",
      "Epoch 11200: <Test loss>: 0.008019480854272842 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07381464), np.complex128(0.005565688541069822+0j)) <f>: (np.float32(-0.0016864631), np.complex128(0.00368444980200013+0j))\n",
      "Epoch 12800: <Test loss>: 0.008183267898857594 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07376878), np.complex128(0.005662664885927905+0j)) <f>: (np.float32(-0.0016405608), np.complex128(0.0035744923136735+0j))\n",
      "Epoch 14400: <Test loss>: 0.008447793312370777 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07373713), np.complex128(0.005840143958320502+0j)) <f>: (np.float32(-0.0016089161), np.complex128(0.003420464702117793+0j))\n",
      "Epoch 16000: <Test loss>: 0.008265442214906216 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07382221), np.complex128(0.005718302924919573+0j)) <f>: (np.float32(-0.0016939729), np.complex128(0.0035048744841305844+0j))\n",
      "Epoch 17600: <Test loss>: 0.00822680164128542 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073860504), np.complex128(0.005726669131450306+0j)) <f>: (np.float32(-0.0017322673), np.complex128(0.0034882803491839754+0j))\n",
      "Epoch 19200: <Test loss>: 0.00866567064076662 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073901646), np.complex128(0.005981321092883168+0j)) <f>: (np.float32(-0.0017734556), np.complex128(0.00326156234300113+0j))\n",
      "Epoch 20800: <Test loss>: 0.008022020570933819 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.073941566), np.complex128(0.005635310176432412+0j)) <f>: (np.float32(-0.0018133917), np.complex128(0.0035491668672286768+0j))\n",
      "Epoch 22400: <Test loss>: 0.008142092265188694 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07396857), np.complex128(0.005736972246474264+0j)) <f>: (np.float32(-0.0018403344), np.complex128(0.00344651756756315+0j))\n",
      "Epoch 24000: <Test loss>: 0.008138823322951794 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.0739766), np.complex128(0.005728424375484382+0j)) <f>: (np.float32(-0.001848378), np.complex128(0.0034625393072306504+0j))\n",
      "Epoch 25600: <Test loss>: 0.008064991794526577 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07396456), np.complex128(0.00569502957644347+0j)) <f>: (np.float32(-0.0018363423), np.complex128(0.003482791596048798+0j))\n",
      "Epoch 27200: <Test loss>: 0.008215359412133694 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07398938), np.complex128(0.005792805650875911+0j)) <f>: (np.float32(-0.0018611288), np.complex128(0.0033926903375728116+0j))\n",
      "Epoch 28800: <Test loss>: 0.008212323300540447 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07400341), np.complex128(0.005801871113341583+0j)) <f>: (np.float32(-0.001875175), np.complex128(0.0033805122220556738+0j))\n",
      "Epoch 30400: <Test loss>: 0.008289582096040249 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07399618), np.complex128(0.005847935232359009+0j)) <f>: (np.float32(-0.0018680254), np.complex128(0.003339726267484746+0j))\n",
      "Epoch 32000: <Test loss>: 0.008151749148964882 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074027516), np.complex128(0.005768612816366345+0j)) <f>: (np.float32(-0.0018992806), np.complex128(0.0034066252194518945+0j))\n",
      "Epoch 33600: <Test loss>: 0.008225027471780777 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07403974), np.complex128(0.005843895989077355+0j)) <f>: (np.float32(-0.0019115409), np.complex128(0.00332770875755789+0j))\n",
      "Epoch 35200: <Test loss>: 0.008709566667675972 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07399264), np.complex128(0.006147532909267492+0j)) <f>: (np.float32(-0.0018644168), np.complex128(0.0030498400275014255+0j))\n",
      "Epoch 36800: <Test loss>: 0.008298160508275032 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07404785), np.complex128(0.0058719311791307245+0j)) <f>: (np.float32(-0.0019196663), np.complex128(0.0033037901957862314+0j))\n",
      "Epoch 38400: <Test loss>: 0.008420951664447784 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074032925), np.complex128(0.005935480756062627+0j)) <f>: (np.float32(-0.0019047123), np.complex128(0.0032444582282180807+0j))\n",
      "Epoch 40000: <Test loss>: 0.008366704918444157 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07413129), np.complex128(0.005929170199598978+0j)) <f>: (np.float32(-0.0020030702), np.complex128(0.0032526952906011776+0j))\n",
      "Epoch 41600: <Test loss>: 0.008406905457377434 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07405806), np.complex128(0.005933718407831825+0j)) <f>: (np.float32(-0.001929866), np.complex128(0.0032435270710042823+0j))\n",
      "Epoch 43200: <Test loss>: 0.008483036421239376 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.074066095), np.complex128(0.005976185773535135+0j)) <f>: (np.float32(-0.0019378513), np.complex128(0.003203012851958144+0j))\n",
      "Epoch 44800: <Test loss>: 0.008533176966011524 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07406171), np.complex128(0.005999392647027583+0j)) <f>: (np.float32(-0.001933467), np.complex128(0.0031757020362495688+0j))\n",
      "Epoch 46400: <Test loss>: 0.008516010828316212 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07405464), np.complex128(0.005994343085482892+0j)) <f>: (np.float32(-0.0019264377), np.complex128(0.003175997875298966+0j))\n",
      "Epoch 48000: <Test loss>: 0.00858915876597166 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07403839), np.complex128(0.006034062649380906+0j)) <f>: (np.float32(-0.0019101384), np.complex128(0.003140818146830515+0j))\n",
      "Epoch 49600: <Test loss>: 0.008622496388852596 <O>: (np.float32(0.07212823), np.complex128(0.008887477980426646+0j)) <O-f>: (np.float32(0.07408338), np.complex128(0.006033780511282338+0j)) <f>: (np.float32(-0.0019551618), np.complex128(0.0031488103681478583+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_20min.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c78176",
   "metadata": {},
   "source": [
    "# 16x16x16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c1c0d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0009885071), np.complex128(7.283451065977387e-06+0j))\n",
      "bin size 1: (np.float32(0.0009885071), np.complex128(7.283384254504891e-06+0j))\n",
      "jack bin size 2: (np.float32(0.0009885071), np.complex128(1.0209129350987e-05+0j))\n",
      "bin size 2: (np.float32(0.0009885071), np.complex128(1.0209121806089082e-05+0j))\n",
      "jack bin size 4: (np.float32(0.0009885071), np.complex128(1.4261771870204315e-05+0j))\n",
      "bin size 4: (np.float32(0.0009885071), np.complex128(1.4261821373384561e-05+0j))\n",
      "jack bin size 5: (np.float32(0.0009885071), np.complex128(1.5859950990796923e-05+0j))\n",
      "bin size 5: (np.float32(0.0009885071), np.complex128(1.5860005352826588e-05+0j))\n",
      "jack bin size 10: (np.float32(0.0009885071), np.complex128(2.1866609613658744e-05+0j))\n",
      "bin size 10: (np.float32(0.0009885071), np.complex128(2.186659628832664e-05+0j))\n",
      "jack bin size 20: (np.float32(0.0009885071), np.complex128(2.9584116660447023e-05+0j))\n",
      "bin size 20: (np.float32(0.0009885071), np.complex128(2.9584096262579525e-05+0j))\n",
      "jack bin size 50: (np.float32(0.0009885071), np.complex128(4.2272243286890414e-05+0j))\n",
      "bin size 50: (np.float32(0.0009885071), np.complex128(4.227222973911447e-05+0j))\n",
      "jack bin size 100: (np.float32(0.0009885071), np.complex128(5.2660751315820905e-05+0j))\n",
      "bin size 100: (np.float32(0.0009885071), np.complex128(5.266078417702958e-05+0j))\n",
      "jack bin size 200: (np.float32(0.0009885071), np.complex128(5.777838232509734e-05+0j))\n",
      "bin size 200: (np.float32(0.0009885071), np.complex128(5.7778361479235184e-05+0j))\n",
      "jack bin size 500: (np.float32(0.0009885071), np.complex128(6.525889237520093e-05+0j))\n",
      "bin size 500: (np.float32(0.0009885071), np.complex128(6.525889991846821e-05+0j))\n",
      "jack bin size 1000: (np.float32(0.0009885071), np.complex128(6.0894392527533653e-05+0j))\n",
      "bin size 1000: (np.float32(0.0009885071), np.complex128(6.08944471436086e-05+0j))\n",
      "jack bin size 2000: (np.float32(0.0009885071), np.complex128(5.530933412956074e-05+0j))\n",
      "bin size 2000: (np.float32(0.0009885071), np.complex128(5.53093892189541e-05+0j))\n",
      "jack bin size 5000: (np.float32(0.0009885071), np.complex128(5.4545289053109566e-05+0j))\n",
      "bin size 5000: (np.float32(0.0009885071), np.complex128(5.454524794858785e-05+0j))\n",
      "jack bin size 10000: (np.float32(0.0009885071), np.complex128(4.791818355442956e-05+0j))\n",
      "bin size 10000: (np.float32(0.0009885071), np.complex128(4.791812292144944e-05+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_16x16x16_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(16,16,16), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 4))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb671b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j, -k) for i, j, k in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1, 2)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1, 2)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fab952d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 3.590295364119811e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003889202), np.complex128(4.8217142566650044e-05+0j)) <f>: (np.float32(0.00027653226), np.complex128(0.00014569055126065157+0j))\n",
      "Epoch 200: <Test loss>: 3.8985737660368613e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00041970698), np.complex128(2.0516674353689973e-05+0j)) <f>: (np.float32(0.00024574588), np.complex128(0.00011639112341280855+0j))\n",
      "Epoch 300: <Test loss>: 2.461279109411407e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039732733), np.complex128(1.535205911405037e-05+0j)) <f>: (np.float32(0.00026812495), np.complex128(0.00012199707200760056+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159333e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 7.602173354825936e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00028160293), np.complex128(8.816316859290384e-05+0j)) <f>: (np.float32(0.00038384987), np.complex128(0.0001485459943324201+0j))\n",
      "Epoch 400: <Test loss>: 4.9922709877137095e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039678364), np.complex128(7.206073761950389e-05+0j)) <f>: (np.float32(0.00026866907), np.complex128(0.00011404102976354218+0j))\n",
      "Epoch 600: <Test loss>: 1.1297646551611251e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036836867), np.complex128(3.4013621355658584e-05+0j)) <f>: (np.float32(0.00029708364), np.complex128(0.00012080726970293605+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d39955fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 6.5578606154304e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036388563), np.complex128(7.269913215480865e-05+0j)) <f>: (np.float32(0.00030156688), np.complex128(0.00012964651583288874+0j))\n",
      "Epoch 800: <Test loss>: 3.5577484140958404e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004212738), np.complex128(6.855018612102487e-05+0j)) <f>: (np.float32(0.0002441787), np.complex128(0.00014096867078997174+0j))\n",
      "Epoch 1200: <Test loss>: 1.2122477528464515e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003814782), np.complex128(3.431845561843362e-05+0j)) <f>: (np.float32(0.0002839744), np.complex128(0.00011336068399511235+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449c4d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 2.2732658180757426e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045447773), np.complex128(0.00016665054810493341+0j)) <f>: (np.float32(0.00021097442), np.complex128(0.00016322220231143277+0j))\n",
      "Epoch 1600: <Test loss>: 1.0734629540820606e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00035748119), np.complex128(0.00011486224636148117+0j)) <f>: (np.float32(0.0003079713), np.complex128(0.00015077767941705037+0j))\n",
      "Epoch 2400: <Test loss>: 3.8703077734680846e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00012630192), np.complex128(0.00021796481122150074+0j)) <f>: (np.float32(0.0005391507), np.complex128(0.00024299770113639577+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1b9d05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.00012857909314334393 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0007518187), np.complex128(0.0004364989730663703+0j)) <f>: (np.float32(-8.6365835e-05), np.complex128(0.00048140780380630867+0j))\n",
      "Epoch 3200: <Test loss>: 5.758496990893036e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00056306843), np.complex128(0.00027886093896088156+0j)) <f>: (np.float32(0.00010238407), np.complex128(0.0002073117986511795+0j))\n",
      "Epoch 4800: <Test loss>: 1.1397541129554156e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00049124606), np.complex128(0.00010449486848655407+0j)) <f>: (np.float32(0.00017420664), np.complex128(7.394396819773078e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d90c6",
   "metadata": {},
   "source": [
    "## 2 hours run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b845d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j, -k) for i, j, k in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1, 2)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1, 2)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46f98115",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 3.590295364119811e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003889202), np.complex128(4.8217142566650044e-05+0j)) <f>: (np.float32(0.00027653226), np.complex128(0.00014569055126065157+0j))\n",
      "Epoch 200: <Test loss>: 3.8985737660368613e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00041970698), np.complex128(2.0516674353689973e-05+0j)) <f>: (np.float32(0.00024574588), np.complex128(0.00011639112341280855+0j))\n",
      "Epoch 300: <Test loss>: 2.461279109411407e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039732733), np.complex128(1.535205911405037e-05+0j)) <f>: (np.float32(0.00026812495), np.complex128(0.00012199707200760056+0j))\n",
      "Epoch 400: <Test loss>: 2.4408322474300803e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.000395445), np.complex128(1.6142560566795187e-05+0j)) <f>: (np.float32(0.00027000761), np.complex128(0.00012282706577716485+0j))\n",
      "Epoch 500: <Test loss>: 2.9539899060182506e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039551107), np.complex128(1.676036006750491e-05+0j)) <f>: (np.float32(0.00026994164), np.complex128(0.00011987362284876697+0j))\n",
      "Epoch 600: <Test loss>: 3.4072758126058034e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004022845), np.complex128(1.7790470575049106e-05+0j)) <f>: (np.float32(0.00026316784), np.complex128(0.00012297129841405352+0j))\n",
      "Epoch 700: <Test loss>: 3.7125420249140006e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037619183), np.complex128(1.7702568033744815e-05+0j)) <f>: (np.float32(0.00028926082), np.complex128(0.00012875340095825765+0j))\n",
      "Epoch 800: <Test loss>: 3.208668317711272e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00040181613), np.complex128(1.803729582074774e-05+0j)) <f>: (np.float32(0.00026363632), np.complex128(0.00011824535047359611+0j))\n",
      "Epoch 900: <Test loss>: 3.7788080931022705e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039073825), np.complex128(1.8488511422769585e-05+0j)) <f>: (np.float32(0.0002747144), np.complex128(0.00012413386532166934+0j))\n",
      "Epoch 1000: <Test loss>: 9.449845492781606e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004207011), np.complex128(3.596588395263493e-05+0j)) <f>: (np.float32(0.00024475105), np.complex128(0.00012133997345435174+0j))\n",
      "Epoch 1100: <Test loss>: 4.263843607077433e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003985191), np.complex128(1.9428810532578786e-05+0j)) <f>: (np.float32(0.00026693376), np.complex128(0.00012325415010381037+0j))\n",
      "Epoch 1200: <Test loss>: 4.660931267608248e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00038406785), np.complex128(1.896374333269841e-05+0j)) <f>: (np.float32(0.0002813844), np.complex128(0.00012693318841081473+0j))\n",
      "Epoch 1300: <Test loss>: 5.378101946007519e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00040411), np.complex128(1.7390201419529826e-05+0j)) <f>: (np.float32(0.00026134262), np.complex128(0.00011948182957081539+0j))\n",
      "Epoch 1400: <Test loss>: 5.173691306481487e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039298754), np.complex128(1.8712697987995655e-05+0j)) <f>: (np.float32(0.00027246514), np.complex128(0.00012495139503179825+0j))\n",
      "Epoch 1500: <Test loss>: 5.787706527371483e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003832591), np.complex128(2.1825973449553897e-05+0j)) <f>: (np.float32(0.00028219388), np.complex128(0.00012977910900460496+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76707fc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 7.602173354825936e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00028160293), np.complex128(8.816316859290384e-05+0j)) <f>: (np.float32(0.00038384987), np.complex128(0.0001485459943324201+0j))\n",
      "Epoch 400: <Test loss>: 4.9922709877137095e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039678364), np.complex128(7.206073761950389e-05+0j)) <f>: (np.float32(0.00026866907), np.complex128(0.00011404102976354218+0j))\n",
      "Epoch 600: <Test loss>: 1.1297646551611251e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036836867), np.complex128(3.4013621355658584e-05+0j)) <f>: (np.float32(0.00029708364), np.complex128(0.00012080726970293605+0j))\n",
      "Epoch 800: <Test loss>: 3.319978816307412e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00038415988), np.complex128(1.987000216059225e-05+0j)) <f>: (np.float32(0.0002812926), np.complex128(0.00011956167249607893+0j))\n",
      "Epoch 1000: <Test loss>: 3.9822290887059353e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037623348), np.complex128(1.8407326549631087e-05+0j)) <f>: (np.float32(0.00028921885), np.complex128(0.00012042844792704316+0j))\n",
      "Epoch 1200: <Test loss>: 4.901924057776341e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003693057), np.complex128(2.1071465734604462e-05+0j)) <f>: (np.float32(0.000296147), np.complex128(0.00012372079116870215+0j))\n",
      "Epoch 1400: <Test loss>: 6.360498332469433e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003687631), np.complex128(2.4865033445253352e-05+0j)) <f>: (np.float32(0.0002966895), np.complex128(0.0001243564106271606+0j))\n",
      "Epoch 1600: <Test loss>: 7.620371320626873e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003602642), np.complex128(2.5530915116720714e-05+0j)) <f>: (np.float32(0.0003051885), np.complex128(0.00012626755054967028+0j))\n",
      "Epoch 1800: <Test loss>: 7.786425726408197e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037109185), np.complex128(2.4520935809498674e-05+0j)) <f>: (np.float32(0.0002943605), np.complex128(0.00012197105764436425+0j))\n",
      "Epoch 2000: <Test loss>: 8.554243322578259e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037473592), np.complex128(2.6268316465110458e-05+0j)) <f>: (np.float32(0.0002907165), np.complex128(0.00012049316271909887+0j))\n",
      "Epoch 2200: <Test loss>: 9.329179988526448e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036939516), np.complex128(2.6586550384657625e-05+0j)) <f>: (np.float32(0.0002960574), np.complex128(0.00012114002520318034+0j))\n",
      "Epoch 2400: <Test loss>: 9.994433867177577e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036840435), np.complex128(2.74631218368893e-05+0j)) <f>: (np.float32(0.0002970483), np.complex128(0.00012022165712924791+0j))\n",
      "Epoch 2600: <Test loss>: 1.0606423757053562e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037684332), np.complex128(2.9171362015781775e-05+0j)) <f>: (np.float32(0.00028860924), np.complex128(0.00011935390645699197+0j))\n",
      "Epoch 2800: <Test loss>: 1.1968012358920532e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00035864316), np.complex128(2.9583458945259534e-05+0j)) <f>: (np.float32(0.0003068096), np.complex128(0.00012460813388349295+0j))\n",
      "Epoch 3000: <Test loss>: 3.6404501315701054e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.000266052), np.complex128(5.5835402456976826e-05+0j)) <f>: (np.float32(0.00039940054), np.complex128(0.00015896758135134191+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf5226ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 6.5578606154304e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036388563), np.complex128(7.269913215480865e-05+0j)) <f>: (np.float32(0.00030156688), np.complex128(0.00012964651583288874+0j))\n",
      "Epoch 800: <Test loss>: 3.5577484140958404e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004212738), np.complex128(6.855018612102487e-05+0j)) <f>: (np.float32(0.0002441787), np.complex128(0.00014096867078997174+0j))\n",
      "Epoch 1200: <Test loss>: 1.2122477528464515e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003814782), np.complex128(3.431845561843362e-05+0j)) <f>: (np.float32(0.0002839744), np.complex128(0.00011336068399511235+0j))\n",
      "Epoch 1600: <Test loss>: 9.312977340414363e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037800724), np.complex128(3.17914448736224e-05+0j)) <f>: (np.float32(0.00028744544), np.complex128(0.00011452715979668597+0j))\n",
      "Epoch 2000: <Test loss>: 1.0020970648838556e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036896745), np.complex128(3.4085785246848864e-05+0j)) <f>: (np.float32(0.00029648526), np.complex128(0.00011167955829920598+0j))\n",
      "Epoch 2400: <Test loss>: 1.4562564274456236e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00038088261), np.complex128(3.691121784472242e-05+0j)) <f>: (np.float32(0.0002845696), np.complex128(0.00011469095276094557+0j))\n",
      "Epoch 2800: <Test loss>: 1.502034592704149e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003894117), np.complex128(3.916367239734948e-05+0j)) <f>: (np.float32(0.0002760407), np.complex128(0.00011580451181145574+0j))\n",
      "Epoch 3200: <Test loss>: 1.772960899870668e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003893382), np.complex128(4.3846111097252396e-05+0j)) <f>: (np.float32(0.0002761146), np.complex128(0.00011458231246680843+0j))\n",
      "Epoch 3600: <Test loss>: 1.943191819009371e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039714493), np.complex128(4.44616033197842e-05+0j)) <f>: (np.float32(0.00026830757), np.complex128(0.0001149074404701209+0j))\n",
      "Epoch 4000: <Test loss>: 2.1723292320530163e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045660388), np.complex128(5.024780901332535e-05+0j)) <f>: (np.float32(0.00020884856), np.complex128(0.00010305596735537471+0j))\n",
      "Epoch 4400: <Test loss>: 2.168341097785742e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00041019203), np.complex128(4.769577698634924e-05+0j)) <f>: (np.float32(0.00025526056), np.complex128(0.00011301331573307454+0j))\n",
      "Epoch 4800: <Test loss>: 2.3276277261174982e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004194495), np.complex128(5.03409881650331e-05+0j)) <f>: (np.float32(0.00024600333), np.complex128(0.00010991082739165741+0j))\n",
      "Epoch 5200: <Test loss>: 2.418078565824544e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00042475117), np.complex128(5.0688633934755547e-05+0j)) <f>: (np.float32(0.00024070163), np.complex128(0.00010858239810533374+0j))\n",
      "Epoch 5600: <Test loss>: 2.5192784960381687e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004219232), np.complex128(5.1163140360308836e-05+0j)) <f>: (np.float32(0.00024352969), np.complex128(0.00010995109772109271+0j))\n",
      "Epoch 6000: <Test loss>: 2.628187075970345e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00043146743), np.complex128(5.3122637979063035e-05+0j)) <f>: (np.float32(0.0002339847), np.complex128(0.00010723653340762432+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9965cbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 2.2732658180757426e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045447773), np.complex128(0.00016665054810493341+0j)) <f>: (np.float32(0.00021097442), np.complex128(0.00016322220231143277+0j))\n",
      "Epoch 1600: <Test loss>: 1.0734629540820606e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00035748119), np.complex128(0.00011486224636148117+0j)) <f>: (np.float32(0.0003079713), np.complex128(0.00015077767941705037+0j))\n",
      "Epoch 2400: <Test loss>: 3.8703077734680846e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00012630192), np.complex128(0.00021796481122150074+0j)) <f>: (np.float32(0.0005391507), np.complex128(0.00024299770113639577+0j))\n",
      "Epoch 3200: <Test loss>: 4.486460511543555e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039797643), np.complex128(7.710791259810697e-05+0j)) <f>: (np.float32(0.00026747576), np.complex128(9.799721634192707e-05+0j))\n",
      "Epoch 4000: <Test loss>: 3.0007484383531846e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00046392856), np.complex128(0.0001799355862745231+0j)) <f>: (np.float32(0.0002015238), np.complex128(0.00014473027952634+0j))\n",
      "Epoch 4800: <Test loss>: 3.3840947253338527e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036206425), np.complex128(6.256011138916807e-05+0j)) <f>: (np.float32(0.00030338828), np.complex128(0.00010593453078166842+0j))\n",
      "Epoch 5600: <Test loss>: 4.4885900933877565e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037392508), np.complex128(7.210604273121748e-05+0j)) <f>: (np.float32(0.00029152748), np.complex128(9.781467971576846e-05+0j))\n",
      "Epoch 6400: <Test loss>: 4.649161382985767e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00038208312), np.complex128(7.662856168125913e-05+0j)) <f>: (np.float32(0.00028336933), np.complex128(0.00010653463718528524+0j))\n",
      "Epoch 7200: <Test loss>: 5.211763891566079e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00041553107), np.complex128(8.559887072626708e-05+0j)) <f>: (np.float32(0.00024992143), np.complex128(0.00010035171839898357+0j))\n",
      "Epoch 8000: <Test loss>: 5.370559847506229e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004200213), np.complex128(8.580056331145614e-05+0j)) <f>: (np.float32(0.00024543123), np.complex128(9.603617260892993e-05+0j))\n",
      "Epoch 8800: <Test loss>: 4.958828412782168e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004253398), np.complex128(8.159694826380027e-05+0j)) <f>: (np.float32(0.00024011303), np.complex128(9.836689622180914e-05+0j))\n",
      "Epoch 9600: <Test loss>: 5.514758868230274e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045694262), np.complex128(8.669133126395439e-05+0j)) <f>: (np.float32(0.00020850984), np.complex128(9.464445571292865e-05+0j))\n",
      "Epoch 10400: <Test loss>: 5.269806933938526e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045639972), np.complex128(8.660036424493268e-05+0j)) <f>: (np.float32(0.00020905267), np.complex128(9.480759058753613e-05+0j))\n",
      "Epoch 11200: <Test loss>: 5.8117748267250136e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00047038047), np.complex128(8.830916935018315e-05+0j)) <f>: (np.float32(0.00019507195), np.complex128(9.299276961796603e-05+0j))\n",
      "Epoch 12000: <Test loss>: 5.8251084738003556e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004554351), np.complex128(9.005259155689347e-05+0j)) <f>: (np.float32(0.00021001762), np.complex128(9.051082017454547e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbd215e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.00012857909314334393 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0007518187), np.complex128(0.0004364989730663703+0j)) <f>: (np.float32(-8.6365835e-05), np.complex128(0.00048140780380630867+0j))\n",
      "Epoch 3200: <Test loss>: 5.758496990893036e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00056306843), np.complex128(0.00027886093896088156+0j)) <f>: (np.float32(0.00010238407), np.complex128(0.0002073117986511795+0j))\n",
      "Epoch 4800: <Test loss>: 1.1397541129554156e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00049124606), np.complex128(0.00010449486848655407+0j)) <f>: (np.float32(0.00017420664), np.complex128(7.394396819773078e-05+0j))\n",
      "Epoch 6400: <Test loss>: 1.1787054063461255e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00046481218), np.complex128(0.00011905326256025484+0j)) <f>: (np.float32(0.00020064031), np.complex128(0.0001482259883280868+0j))\n",
      "Epoch 8000: <Test loss>: 8.151234396791551e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0005280488), np.complex128(7.602796369260098e-05+0j)) <f>: (np.float32(0.00013740387), np.complex128(8.144177389534617e-05+0j))\n",
      "Epoch 9600: <Test loss>: 5.748750481870957e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00047469445), np.complex128(6.411497108856781e-05+0j)) <f>: (np.float32(0.0001907582), np.complex128(9.133918854195691e-05+0j))\n",
      "Epoch 11200: <Test loss>: 7.215774076030357e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0005044337), np.complex128(8.083236701989875e-05+0j)) <f>: (np.float32(0.0001610189), np.complex128(8.393441927781147e-05+0j))\n",
      "Epoch 12800: <Test loss>: 8.126176908263005e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004990891), np.complex128(0.00010035073522890087+0j)) <f>: (np.float32(0.00016636333), np.complex128(7.326905365112871e-05+0j))\n",
      "Epoch 14400: <Test loss>: 7.71171380620217e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00049922284), np.complex128(0.00011016171812493586+0j)) <f>: (np.float32(0.00016622938), np.complex128(8.265333280248568e-05+0j))\n",
      "Epoch 16000: <Test loss>: 8.41259225126123e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0005234546), np.complex128(0.0001079485705536389+0j)) <f>: (np.float32(0.0001419983), np.complex128(7.702354233318846e-05+0j))\n",
      "Epoch 17600: <Test loss>: 9.845800377661362e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0005464888), np.complex128(0.00012275996441902132+0j)) <f>: (np.float32(0.000118963304), np.complex128(8.179058519734406e-05+0j))\n",
      "Epoch 19200: <Test loss>: 9.833193871600088e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0005478015), np.complex128(0.00012139004376944997+0j)) <f>: (np.float32(0.0001176516), np.complex128(7.586705680826153e-05+0j))\n",
      "Epoch 20800: <Test loss>: 1.0669084986147936e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00057020894), np.complex128(0.0001258276770831588+0j)) <f>: (np.float32(9.524355e-05), np.complex128(7.2613366422676e-05+0j))\n",
      "Epoch 22400: <Test loss>: 1.2013283594569657e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00056098663), np.complex128(0.00013826964690684263+0j)) <f>: (np.float32(0.00010446581), np.complex128(7.543450954462429e-05+0j))\n",
      "Epoch 24000: <Test loss>: 1.0730598660302348e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0005813724), np.complex128(0.00012784683885410844+0j)) <f>: (np.float32(8.408047e-05), np.complex128(7.49902197415317e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b045e1-38dd-4d39-aa6f-39c7560dce33",
   "metadata": {},
   "source": [
    "## 2 hours run with recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d33ee6ee-1947-4e9f-8eed-af015a5b9870",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0003759962), np.complex128(8.907993781250059e-06+0j))\n",
      "bin size 1: (np.float32(0.0003759962), np.complex128(8.907984216894209e-06+0j))\n",
      "jack bin size 2: (np.float32(0.0003759962), np.complex128(1.2461337281434044e-05+0j))\n",
      "bin size 2: (np.float32(0.0003759962), np.complex128(1.2461309360375226e-05+0j))\n",
      "jack bin size 4: (np.float32(0.0003759962), np.complex128(1.735763368210416e-05+0j))\n",
      "bin size 4: (np.float32(0.0003759962), np.complex128(1.7357527093629767e-05+0j))\n",
      "jack bin size 5: (np.float32(0.0003759962), np.complex128(1.9278563237351494e-05+0j))\n",
      "bin size 5: (np.float32(0.0003759962), np.complex128(1.927853938190075e-05+0j))\n",
      "jack bin size 10: (np.float32(0.0003759962), np.complex128(2.644778388334708e-05+0j))\n",
      "bin size 10: (np.float32(0.0003759962), np.complex128(2.6447758952850747e-05+0j))\n",
      "jack bin size 20: (np.float32(0.0003759962), np.complex128(3.547637725987774e-05+0j))\n",
      "bin size 20: (np.float32(0.0003759962), np.complex128(3.5476307855754687e-05+0j))\n",
      "jack bin size 50: (np.float32(0.0003759962), np.complex128(4.944866431568171e-05+0j))\n",
      "bin size 50: (np.float32(0.0003759962), np.complex128(4.9448626446494937e-05+0j))\n",
      "jack bin size 100: (np.float32(0.0003759962), np.complex128(6.0060514520853695e-05+0j))\n",
      "bin size 100: (np.float32(0.0003759962), np.complex128(6.0060475519749754e-05+0j))\n",
      "jack bin size 200: (np.float32(0.0003759962), np.complex128(6.652642424951726e-05+0j))\n",
      "bin size 200: (np.float32(0.0003759962), np.complex128(6.65264786645461e-05+0j))\n",
      "jack bin size 500: (np.float32(0.0003759962), np.complex128(7.46526475007012e-05+0j))\n",
      "bin size 500: (np.float32(0.0003759962), np.complex128(7.465262532220591e-05+0j))\n",
      "jack bin size 1000: (np.float32(0.0003759962), np.complex128(6.969908305843537e-05+0j))\n",
      "bin size 1000: (np.float32(0.0003759962), np.complex128(6.969905495058091e-05+0j))\n",
      "jack bin size 2000: (np.float32(0.0003759962), np.complex128(6.629731433349662e-05+0j))\n",
      "bin size 2000: (np.float32(0.0003759962), np.complex128(6.629728263110986e-05+0j))\n",
      "jack bin size 5000: (np.float32(0.0003759962), np.complex128(6.319480714347626e-05+0j))\n",
      "bin size 5000: (np.float32(0.0003759962), np.complex128(6.319476103129707e-05+0j))\n",
      "jack bin size 10000: (np.float32(0.0003759962), np.complex128(5.022869663662277e-05+0j))\n",
      "bin size 10000: (np.float32(0.0003759962), np.complex128(5.022868087204794e-05+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_16x16x16_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(16,16,16), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec052edc-0a24-42e7-9092-b24a713aace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be333eb5-2785-4841-aa50-d2b3f561ede8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 3.590295364119811e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003889202), np.complex128(4.8217142566650044e-05+0j)) <f>: (np.float32(0.00027653226), np.complex128(0.00014569055126065157+0j))\n",
      "Epoch 200: <Test loss>: 3.8985737660368613e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00041970698), np.complex128(2.0516674353689973e-05+0j)) <f>: (np.float32(0.00024574588), np.complex128(0.00011639112341280855+0j))\n",
      "Epoch 300: <Test loss>: 2.461279109411407e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039732733), np.complex128(1.535205911405037e-05+0j)) <f>: (np.float32(0.00026812495), np.complex128(0.00012199707200760056+0j))\n",
      "Epoch 400: <Test loss>: 2.4408322474300803e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.000395445), np.complex128(1.6142560566795187e-05+0j)) <f>: (np.float32(0.00027000761), np.complex128(0.00012282706577716485+0j))\n",
      "Epoch 500: <Test loss>: 2.9539899060182506e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039551107), np.complex128(1.676036006750491e-05+0j)) <f>: (np.float32(0.00026994164), np.complex128(0.00011987362284876697+0j))\n",
      "Epoch 600: <Test loss>: 3.4072758126058034e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004022845), np.complex128(1.7790470575049106e-05+0j)) <f>: (np.float32(0.00026316784), np.complex128(0.00012297129841405352+0j))\n",
      "Epoch 700: <Test loss>: 3.7125420249140006e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037619183), np.complex128(1.7702568033744815e-05+0j)) <f>: (np.float32(0.00028926082), np.complex128(0.00012875340095825765+0j))\n",
      "Epoch 800: <Test loss>: 3.208668317711272e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00040181613), np.complex128(1.803729582074774e-05+0j)) <f>: (np.float32(0.00026363632), np.complex128(0.00011824535047359611+0j))\n",
      "Epoch 900: <Test loss>: 3.7788080931022705e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039073825), np.complex128(1.8488511422769585e-05+0j)) <f>: (np.float32(0.0002747144), np.complex128(0.00012413386532166934+0j))\n",
      "Epoch 1000: <Test loss>: 9.449845492781606e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004207011), np.complex128(3.596588395263493e-05+0j)) <f>: (np.float32(0.00024475105), np.complex128(0.00012133997345435174+0j))\n",
      "Epoch 1100: <Test loss>: 4.263843607077433e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003985191), np.complex128(1.9428810532578786e-05+0j)) <f>: (np.float32(0.00026693376), np.complex128(0.00012325415010381037+0j))\n",
      "Epoch 1200: <Test loss>: 4.660931267608248e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00038406785), np.complex128(1.896374333269841e-05+0j)) <f>: (np.float32(0.0002813844), np.complex128(0.00012693318841081473+0j))\n",
      "Epoch 1300: <Test loss>: 5.378101946007519e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00040411), np.complex128(1.7390201419529826e-05+0j)) <f>: (np.float32(0.00026134262), np.complex128(0.00011948182957081539+0j))\n",
      "Epoch 1400: <Test loss>: 5.173691306481487e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039298754), np.complex128(1.8712697987995655e-05+0j)) <f>: (np.float32(0.00027246514), np.complex128(0.00012495139503179825+0j))\n",
      "Epoch 1500: <Test loss>: 5.787706527371483e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003832591), np.complex128(2.1825973449553897e-05+0j)) <f>: (np.float32(0.00028219388), np.complex128(0.00012977910900460496+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cede3d5-d0da-44f9-92f3-7a6948e0dcdf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 7.602173354825936e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00028160293), np.complex128(8.816316859290384e-05+0j)) <f>: (np.float32(0.00038384987), np.complex128(0.0001485459943324201+0j))\n",
      "Epoch 400: <Test loss>: 4.9922709877137095e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039678364), np.complex128(7.206073761950389e-05+0j)) <f>: (np.float32(0.00026866907), np.complex128(0.00011404102976354218+0j))\n",
      "Epoch 600: <Test loss>: 1.1297646551611251e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036836867), np.complex128(3.4013621355658584e-05+0j)) <f>: (np.float32(0.00029708364), np.complex128(0.00012080726970293605+0j))\n",
      "Epoch 800: <Test loss>: 3.319978816307412e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00038415988), np.complex128(1.987000216059225e-05+0j)) <f>: (np.float32(0.0002812926), np.complex128(0.00011956167249607893+0j))\n",
      "Epoch 1000: <Test loss>: 3.9822290887059353e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037623348), np.complex128(1.8407326549631087e-05+0j)) <f>: (np.float32(0.00028921885), np.complex128(0.00012042844792704316+0j))\n",
      "Epoch 1200: <Test loss>: 4.901924057776341e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003693057), np.complex128(2.1071465734604462e-05+0j)) <f>: (np.float32(0.000296147), np.complex128(0.00012372079116870215+0j))\n",
      "Epoch 1400: <Test loss>: 6.360498332469433e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003687631), np.complex128(2.4865033445253352e-05+0j)) <f>: (np.float32(0.0002966895), np.complex128(0.0001243564106271606+0j))\n",
      "Epoch 1600: <Test loss>: 7.620371320626873e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003602642), np.complex128(2.5530915116720714e-05+0j)) <f>: (np.float32(0.0003051885), np.complex128(0.00012626755054967028+0j))\n",
      "Epoch 1800: <Test loss>: 7.786425726408197e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037109185), np.complex128(2.4520935809498674e-05+0j)) <f>: (np.float32(0.0002943605), np.complex128(0.00012197105764436425+0j))\n",
      "Epoch 2000: <Test loss>: 8.554243322578259e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037473592), np.complex128(2.6268316465110458e-05+0j)) <f>: (np.float32(0.0002907165), np.complex128(0.00012049316271909887+0j))\n",
      "Epoch 2200: <Test loss>: 9.329179988526448e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036939516), np.complex128(2.6586550384657625e-05+0j)) <f>: (np.float32(0.0002960574), np.complex128(0.00012114002520318034+0j))\n",
      "Epoch 2400: <Test loss>: 9.994433867177577e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036840435), np.complex128(2.74631218368893e-05+0j)) <f>: (np.float32(0.0002970483), np.complex128(0.00012022165712924791+0j))\n",
      "Epoch 2600: <Test loss>: 1.0606423757053562e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037684332), np.complex128(2.9171362015781775e-05+0j)) <f>: (np.float32(0.00028860924), np.complex128(0.00011935390645699197+0j))\n",
      "Epoch 2800: <Test loss>: 1.1968012358920532e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00035864316), np.complex128(2.9583458945259534e-05+0j)) <f>: (np.float32(0.0003068096), np.complex128(0.00012460813388349295+0j))\n",
      "Epoch 3000: <Test loss>: 3.6404501315701054e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.000266052), np.complex128(5.5835402456976826e-05+0j)) <f>: (np.float32(0.00039940054), np.complex128(0.00015896758135134191+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "496a5d5d-52a4-4941-bcf8-13e8dca84ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "738e58e4-ba10-4b8c-9213-3736c2e8255d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 6.5578606154304e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036388563), np.complex128(7.269913215480865e-05+0j)) <f>: (np.float32(0.00030156688), np.complex128(0.00012964651583288874+0j))\n",
      "Epoch 800: <Test loss>: 3.5577484140958404e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004212738), np.complex128(6.855018612102487e-05+0j)) <f>: (np.float32(0.0002441787), np.complex128(0.00014096867078997174+0j))\n",
      "Epoch 1200: <Test loss>: 1.2122477528464515e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003814782), np.complex128(3.431845561843362e-05+0j)) <f>: (np.float32(0.0002839744), np.complex128(0.00011336068399511235+0j))\n",
      "Epoch 1600: <Test loss>: 9.312977340414363e-07 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037800724), np.complex128(3.17914448736224e-05+0j)) <f>: (np.float32(0.00028744544), np.complex128(0.00011452715979668597+0j))\n",
      "Epoch 2000: <Test loss>: 1.0020970648838556e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036896745), np.complex128(3.4085785246848864e-05+0j)) <f>: (np.float32(0.00029648526), np.complex128(0.00011167955829920598+0j))\n",
      "Epoch 2400: <Test loss>: 1.4562564274456236e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00038088261), np.complex128(3.691121784472242e-05+0j)) <f>: (np.float32(0.0002845696), np.complex128(0.00011469095276094557+0j))\n",
      "Epoch 2800: <Test loss>: 1.502034592704149e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003894117), np.complex128(3.916367239734948e-05+0j)) <f>: (np.float32(0.0002760407), np.complex128(0.00011580451181145574+0j))\n",
      "Epoch 3200: <Test loss>: 1.772960899870668e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0003893382), np.complex128(4.3846111097252396e-05+0j)) <f>: (np.float32(0.0002761146), np.complex128(0.00011458231246680843+0j))\n",
      "Epoch 3600: <Test loss>: 1.943191819009371e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039714493), np.complex128(4.44616033197842e-05+0j)) <f>: (np.float32(0.00026830757), np.complex128(0.0001149074404701209+0j))\n",
      "Epoch 4000: <Test loss>: 2.1723292320530163e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045660388), np.complex128(5.024780901332535e-05+0j)) <f>: (np.float32(0.00020884856), np.complex128(0.00010305596735537471+0j))\n",
      "Epoch 4400: <Test loss>: 2.168341097785742e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00041019203), np.complex128(4.769577698634924e-05+0j)) <f>: (np.float32(0.00025526056), np.complex128(0.00011301331573307454+0j))\n",
      "Epoch 4800: <Test loss>: 2.3276277261174982e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004194495), np.complex128(5.03409881650331e-05+0j)) <f>: (np.float32(0.00024600333), np.complex128(0.00010991082739165741+0j))\n",
      "Epoch 5200: <Test loss>: 2.418078565824544e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00042475117), np.complex128(5.0688633934755547e-05+0j)) <f>: (np.float32(0.00024070163), np.complex128(0.00010858239810533374+0j))\n",
      "Epoch 5600: <Test loss>: 2.5192784960381687e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004219232), np.complex128(5.1163140360308836e-05+0j)) <f>: (np.float32(0.00024352969), np.complex128(0.00010995109772109271+0j))\n",
      "Epoch 6000: <Test loss>: 2.628187075970345e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00043146743), np.complex128(5.3122637979063035e-05+0j)) <f>: (np.float32(0.0002339847), np.complex128(0.00010723653340762432+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3aebb85a-671d-4a17-8a63-4c872730d2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eec2a61f-c132-4ed9-ad35-9c270012ba9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 2.2732658180757426e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045447773), np.complex128(0.00016665054810493341+0j)) <f>: (np.float32(0.00021097442), np.complex128(0.00016322220231143277+0j))\n",
      "Epoch 1600: <Test loss>: 1.0734629540820606e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00035748119), np.complex128(0.00011486224636148117+0j)) <f>: (np.float32(0.0003079713), np.complex128(0.00015077767941705037+0j))\n",
      "Epoch 2400: <Test loss>: 3.8703077734680846e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00012630192), np.complex128(0.00021796481122150074+0j)) <f>: (np.float32(0.0005391507), np.complex128(0.00024299770113639577+0j))\n",
      "Epoch 3200: <Test loss>: 4.486460511543555e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00039797643), np.complex128(7.710791259810697e-05+0j)) <f>: (np.float32(0.00026747576), np.complex128(9.799721634192707e-05+0j))\n",
      "Epoch 4000: <Test loss>: 3.0007484383531846e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00046392856), np.complex128(0.0001799355862745231+0j)) <f>: (np.float32(0.0002015238), np.complex128(0.00014473027952634+0j))\n",
      "Epoch 4800: <Test loss>: 3.3840947253338527e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00036206425), np.complex128(6.256011138916807e-05+0j)) <f>: (np.float32(0.00030338828), np.complex128(0.00010593453078166842+0j))\n",
      "Epoch 5600: <Test loss>: 4.4885900933877565e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00037392508), np.complex128(7.210604273121748e-05+0j)) <f>: (np.float32(0.00029152748), np.complex128(9.781467971576846e-05+0j))\n",
      "Epoch 6400: <Test loss>: 4.649161382985767e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00038208312), np.complex128(7.662856168125913e-05+0j)) <f>: (np.float32(0.00028336933), np.complex128(0.00010653463718528524+0j))\n",
      "Epoch 7200: <Test loss>: 5.211763891566079e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00041553107), np.complex128(8.559887072626708e-05+0j)) <f>: (np.float32(0.00024992143), np.complex128(0.00010035171839898357+0j))\n",
      "Epoch 8000: <Test loss>: 5.370559847506229e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004200213), np.complex128(8.580056331145614e-05+0j)) <f>: (np.float32(0.00024543123), np.complex128(9.603617260892993e-05+0j))\n",
      "Epoch 8800: <Test loss>: 4.958828412782168e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004253398), np.complex128(8.159694826380027e-05+0j)) <f>: (np.float32(0.00024011303), np.complex128(9.836689622180914e-05+0j))\n",
      "Epoch 9600: <Test loss>: 5.514758868230274e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045694262), np.complex128(8.669133126395439e-05+0j)) <f>: (np.float32(0.00020850984), np.complex128(9.464445571292865e-05+0j))\n",
      "Epoch 10400: <Test loss>: 5.269806933938526e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00045639972), np.complex128(8.660036424493268e-05+0j)) <f>: (np.float32(0.00020905267), np.complex128(9.480759058753613e-05+0j))\n",
      "Epoch 11200: <Test loss>: 5.8117748267250136e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00047038047), np.complex128(8.830916935018315e-05+0j)) <f>: (np.float32(0.00019507195), np.complex128(9.299276961796603e-05+0j))\n",
      "Epoch 12000: <Test loss>: 5.8251084738003556e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0004554351), np.complex128(9.005259155689347e-05+0j)) <f>: (np.float32(0.00021001762), np.complex128(9.051082017454547e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1503117-7a53-4cbb-a056-f28b099c0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73113aca-d842-4c17-abd2-43e1d4eea5a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 9.809627044887748e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.00012857909314334393 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0007518187), np.complex128(0.0004364989730663703+0j)) <f>: (np.float32(-8.6365835e-05), np.complex128(0.00048140780380630867+0j))\n",
      "Epoch 3200: <Test loss>: 5.758496990893036e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00056306843), np.complex128(0.00027886093896088156+0j)) <f>: (np.float32(0.00010238407), np.complex128(0.0002073117986511795+0j))\n",
      "Epoch 4800: <Test loss>: 1.1397541129554156e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00049124606), np.complex128(0.00010449486848655407+0j)) <f>: (np.float32(0.00017420664), np.complex128(7.394396819773078e-05+0j))\n",
      "Epoch 6400: <Test loss>: 1.1787054063461255e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00046481218), np.complex128(0.00011905326256025484+0j)) <f>: (np.float32(0.00020064031), np.complex128(0.0001482259883280868+0j))\n",
      "Epoch 8000: <Test loss>: 8.151234396791551e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0005280488), np.complex128(7.602796369260098e-05+0j)) <f>: (np.float32(0.00013740387), np.complex128(8.144177389534617e-05+0j))\n",
      "Epoch 9600: <Test loss>: 5.748750481870957e-06 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.00047469445), np.complex128(6.411497108856781e-05+0j)) <f>: (np.float32(0.0001907582), np.complex128(9.133918854195691e-05+0j))\n",
      "Epoch 24000: <Test loss>: 1.0730598660302348e-05 <O>: (np.float32(0.0006654522), np.complex128(0.00012305537531370541+0j)) <O-f>: (np.float32(0.0005813724), np.complex128(0.00012784683885410844+0j)) <f>: (np.float32(8.408047e-05), np.complex128(7.49902197415317e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f1740f",
   "metadata": {},
   "source": [
    "## m^2=0.01, lambda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56defcd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.007444006), np.complex128(4.5619301570591136e-05+0j))\n",
      "bin size 1: (np.float32(0.007444006), np.complex128(4.562226862707872e-05+0j))\n",
      "jack bin size 2: (np.float32(0.007444006), np.complex128(6.443344777906431e-05+0j))\n",
      "bin size 2: (np.float32(0.007444006), np.complex128(6.443512421528273e-05+0j))\n",
      "jack bin size 4: (np.float32(0.007444006), np.complex128(9.094828467955732e-05+0j))\n",
      "bin size 4: (np.float32(0.007444006), np.complex128(9.094879238845219e-05+0j))\n",
      "jack bin size 5: (np.float32(0.007444006), np.complex128(0.00010159508122324533+0j))\n",
      "bin size 5: (np.float32(0.007444006), np.complex128(0.0001015970112132691+0j))\n",
      "jack bin size 10: (np.float32(0.007444006), np.complex128(0.00014307882663009357+0j))\n",
      "bin size 10: (np.float32(0.007444006), np.complex128(0.00014308070929640738+0j))\n",
      "jack bin size 20: (np.float32(0.007444006), np.complex128(0.00020071713742780584+0j))\n",
      "bin size 20: (np.float32(0.007444006), np.complex128(0.00020071693791195576+0j))\n",
      "jack bin size 50: (np.float32(0.007444006), np.complex128(0.0003104745199822295+0j))\n",
      "bin size 50: (np.float32(0.007444006), np.complex128(0.00031047459056899+0j))\n",
      "jack bin size 100: (np.float32(0.007444006), np.complex128(0.00042359805182094317+0j))\n",
      "bin size 100: (np.float32(0.007444006), np.complex128(0.0004235975957062158+0j))\n",
      "jack bin size 200: (np.float32(0.007444006), np.complex128(0.0005628647950643595+0j))\n",
      "bin size 200: (np.float32(0.007444006), np.complex128(0.0005628650025458312+0j))\n",
      "jack bin size 500: (np.float32(0.007444006), np.complex128(0.000723948238133583+0j))\n",
      "bin size 500: (np.float32(0.007444006), np.complex128(0.0007239480086119457+0j))\n",
      "jack bin size 1000: (np.float32(0.007444006), np.complex128(0.0007864980217483121+0j))\n",
      "bin size 1000: (np.float32(0.007444006), np.complex128(0.0007864989277810064+0j))\n",
      "jack bin size 2000: (np.float32(0.007444006), np.complex128(0.0008181844532373361+0j))\n",
      "bin size 2000: (np.float32(0.007444006), np.complex128(0.0008181845769286156+0j))\n",
      "jack bin size 5000: (np.float32(0.007444006), np.complex128(0.0010244718526865727+0j))\n",
      "bin size 5000: (np.float32(0.007444006), np.complex128(0.0010244718526865725+0j))\n",
      "jack bin size 10000: (np.float32(0.007444006), np.complex128(0.0009127301455009729+0j))\n",
      "bin size 10000: (np.float32(0.007444006), np.complex128(0.0009127296507358551+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_16x16x16_0.01_0.1_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(16,16,16), m2=0.01, lamda=0.1)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56ff82b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5438f9c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 4.5303720980882645e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008352184), np.complex128(0.00020107239515786916+0j)) <f>: (np.float32(0.0018827346), np.complex128(0.0011557771984551054+0j))\n",
      "Epoch 200: <Test loss>: 4.408280074130744e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008295201), np.complex128(0.00024268486275911693+0j)) <f>: (np.float32(0.0019397268), np.complex128(0.001169209331555131+0j))\n",
      "Epoch 300: <Test loss>: 4.3505664507392794e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0086211385), np.complex128(0.00023262061049250662+0j)) <f>: (np.float32(0.0016137856), np.complex128(0.00099140663762951+0j))\n",
      "Epoch 400: <Test loss>: 4.0301092667505145e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008465082), np.complex128(0.0002146404277354535+0j)) <f>: (np.float32(0.001769843), np.complex128(0.0010306229453058426+0j))\n",
      "Epoch 500: <Test loss>: 4.256656757206656e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00867225), np.complex128(0.0002492633172127172+0j)) <f>: (np.float32(0.0015626701), np.complex128(0.0009777369579514411+0j))\n",
      "Epoch 600: <Test loss>: 4.274473758414388e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0086085405), np.complex128(0.00026187378970250833+0j)) <f>: (np.float32(0.0016263805), np.complex128(0.0009783141105051435+0j))\n",
      "Epoch 700: <Test loss>: 4.44127363152802e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008655931), np.complex128(0.00031371603101106163+0j)) <f>: (np.float32(0.0015789856), np.complex128(0.0008984595774915979+0j))\n",
      "Epoch 800: <Test loss>: 4.70158556709066e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008753385), np.complex128(0.0002959239191762777+0j)) <f>: (np.float32(0.0014815432), np.complex128(0.0009173639084112149+0j))\n",
      "Epoch 900: <Test loss>: 4.85083473904524e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008722523), np.complex128(0.00030223396819730386+0j)) <f>: (np.float32(0.0015123915), np.complex128(0.0009182188858011868+0j))\n",
      "Epoch 1000: <Test loss>: 5.1783950766548514e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008823782), np.complex128(0.0003577680131663545+0j)) <f>: (np.float32(0.0014111429), np.complex128(0.0008405953242810824+0j))\n",
      "Epoch 1100: <Test loss>: 5.18490924150683e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008656267), np.complex128(0.0003025730033000118+0j)) <f>: (np.float32(0.0015786536), np.complex128(0.0009241272939858525+0j))\n",
      "Epoch 1200: <Test loss>: 5.511924246093258e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008757601), np.complex128(0.0003462301316640312+0j)) <f>: (np.float32(0.0014773224), np.complex128(0.0008545374372175479+0j))\n",
      "Epoch 1300: <Test loss>: 5.578600394073874e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00865413), np.complex128(0.0003196771180830611+0j)) <f>: (np.float32(0.0015807964), np.complex128(0.0009033296946386161+0j))\n",
      "Epoch 1400: <Test loss>: 5.629120641970076e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008658046), np.complex128(0.00032627698027233283+0j)) <f>: (np.float32(0.0015768745), np.complex128(0.0009005513194152648+0j))\n",
      "Epoch 1500: <Test loss>: 5.843145845574327e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008612994), np.complex128(0.0003141631831077014+0j)) <f>: (np.float32(0.0016219247), np.complex128(0.0009257568825403276+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "162830ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28fd5aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 6.877733540022746e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008850507), np.complex128(0.00035265473585727256+0j)) <f>: (np.float32(0.0013844173), np.complex128(0.001021198023171874+0j))\n",
      "Epoch 400: <Test loss>: 6.569199467776343e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008866093), np.complex128(0.0003016480622583491+0j)) <f>: (np.float32(0.0013688244), np.complex128(0.0010224215941973625+0j))\n",
      "Epoch 600: <Test loss>: 5.624838377116248e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008752231), np.complex128(0.00023388090767785814+0j)) <f>: (np.float32(0.0014826899), np.complex128(0.0010545305970611001+0j))\n",
      "Epoch 800: <Test loss>: 8.016419451450929e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008703681), np.complex128(0.0004074413495573319+0j)) <f>: (np.float32(0.0015312361), np.complex128(0.0009725707480337138+0j))\n",
      "Epoch 1000: <Test loss>: 5.636016794596799e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008695928), np.complex128(0.00026121971201862644+0j)) <f>: (np.float32(0.0015389966), np.complex128(0.0010241651670511015+0j))\n",
      "Epoch 1200: <Test loss>: 5.4069521866040304e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008776228), np.complex128(0.0002800324336871515+0j)) <f>: (np.float32(0.0014586914), np.complex128(0.0009726468644272123+0j))\n",
      "Epoch 1400: <Test loss>: 6.179678894113749e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008779615), np.complex128(0.00031341239003133053+0j)) <f>: (np.float32(0.0014553092), np.complex128(0.0009231933458376259+0j))\n",
      "Epoch 1600: <Test loss>: 6.113834388088435e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0086770365), np.complex128(0.0003011108390960695+0j)) <f>: (np.float32(0.0015578875), np.complex128(0.0009672302315748749+0j))\n",
      "Epoch 1800: <Test loss>: 6.58948119962588e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008759383), np.complex128(0.0003473863396812735+0j)) <f>: (np.float32(0.0014755358), np.complex128(0.00090304628793349+0j))\n",
      "Epoch 2000: <Test loss>: 6.803884025430307e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008737512), np.complex128(0.0003684075626495757+0j)) <f>: (np.float32(0.0014974078), np.complex128(0.0008743696269739116+0j))\n",
      "Epoch 2200: <Test loss>: 7.12824912625365e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008797542), np.complex128(0.0004020470124652567+0j)) <f>: (np.float32(0.0014373774), np.complex128(0.0008429085649098299+0j))\n",
      "Epoch 2400: <Test loss>: 7.696557440795004e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008807613), np.complex128(0.00041098193531607886+0j)) <f>: (np.float32(0.0014273118), np.complex128(0.000841510623912902+0j))\n",
      "Epoch 2600: <Test loss>: 7.713164086453617e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0087546185), np.complex128(0.000409409148620252+0j)) <f>: (np.float32(0.0014803013), np.complex128(0.0008533373988437161+0j))\n",
      "Epoch 2800: <Test loss>: 8.898695523384959e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008741503), np.complex128(0.0004467369768587229+0j)) <f>: (np.float32(0.0014934222), np.complex128(0.000828778507911751+0j))\n",
      "Epoch 3000: <Test loss>: 8.720507321413606e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008762413), np.complex128(0.00043999465015295315+0j)) <f>: (np.float32(0.0014725123), np.complex128(0.0008336033992349654+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4bdbfc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "847ddd04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.00019962988153565675 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009228558), np.complex128(0.0007087948413728417+0j)) <f>: (np.float32(0.0010063561), np.complex128(0.000717516765182524+0j))\n",
      "Epoch 800: <Test loss>: 9.914916154230013e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0087753115), np.complex128(0.0004148110339213491+0j)) <f>: (np.float32(0.001459613), np.complex128(0.0010086882304700666+0j))\n",
      "Epoch 1200: <Test loss>: 0.00010969154391204938 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008680885), np.complex128(0.0004260593242615746+0j)) <f>: (np.float32(0.0015540319), np.complex128(0.000948932611741769+0j))\n",
      "Epoch 1600: <Test loss>: 0.0001259643759112805 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00877034), np.complex128(0.0004944277032052119+0j)) <f>: (np.float32(0.0014645776), np.complex128(0.0008449429023867334+0j))\n",
      "Epoch 2000: <Test loss>: 0.0001423196226824075 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00883004), np.complex128(0.0005191233816393265+0j)) <f>: (np.float32(0.0014048764), np.complex128(0.0008654682587665568+0j))\n",
      "Epoch 2400: <Test loss>: 0.00012949079973623157 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008969537), np.complex128(0.0005413680804876253+0j)) <f>: (np.float32(0.0012653786), np.complex128(0.0007754538998398143+0j))\n",
      "Epoch 2800: <Test loss>: 0.00017097170348279178 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009172668), np.complex128(0.000662419406306007+0j)) <f>: (np.float32(0.0010622557), np.complex128(0.000688332027296343+0j))\n",
      "Epoch 3200: <Test loss>: 0.00014620256843045354 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009127806), np.complex128(0.0006347228073430497+0j)) <f>: (np.float32(0.0011071143), np.complex128(0.0006892007056371447+0j))\n",
      "Epoch 3600: <Test loss>: 0.00015471142251044512 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009117758), np.complex128(0.0006432068674230491+0j)) <f>: (np.float32(0.0011171629), np.complex128(0.0006935106695583418+0j))\n",
      "Epoch 4000: <Test loss>: 0.0001956493651960045 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009279128), np.complex128(0.0007375426711603413+0j)) <f>: (np.float32(0.00095579267), np.complex128(0.0006500042516534494+0j))\n",
      "Epoch 4400: <Test loss>: 0.00017776437744032592 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009237229), np.complex128(0.000699467411652879+0j)) <f>: (np.float32(0.0009976913), np.complex128(0.0006726226819938259+0j))\n",
      "Epoch 4800: <Test loss>: 0.0001979184744413942 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009394646), np.complex128(0.000779009740592425+0j)) <f>: (np.float32(0.0008402776), np.complex128(0.0006021652886306221+0j))\n",
      "Epoch 5200: <Test loss>: 0.0001943177339853719 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009360402), np.complex128(0.0007526049002574653+0j)) <f>: (np.float32(0.00087452086), np.complex128(0.0006304795086964918+0j))\n",
      "Epoch 5600: <Test loss>: 0.00018881425785366446 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009373538), np.complex128(0.0007549649524682143+0j)) <f>: (np.float32(0.0008613865), np.complex128(0.000617507245473853+0j))\n",
      "Epoch 6000: <Test loss>: 0.00019865330250468105 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009421476), np.complex128(0.0007728406337600268+0j)) <f>: (np.float32(0.00081343914), np.complex128(0.0006183990758843439+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "523ca48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd508d3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0001907278929138556 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0095587745), np.complex128(0.0006394761492563756+0j)) <f>: (np.float32(0.00067614176), np.complex128(0.0006426004100578498+0j))\n",
      "Epoch 1600: <Test loss>: 0.00025385120534338057 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009820936), np.complex128(0.0008071006859040323+0j)) <f>: (np.float32(0.0004139753), np.complex128(0.0005661799315673171+0j))\n",
      "Epoch 2400: <Test loss>: 0.00022304791491478682 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0094449865), np.complex128(0.0007787503105512509+0j)) <f>: (np.float32(0.0007899378), np.complex128(0.0005177519545194673+0j))\n",
      "Epoch 3200: <Test loss>: 0.00020008212595712394 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008866807), np.complex128(0.0005836645648875311+0j)) <f>: (np.float32(0.0013681084), np.complex128(0.000753749500524699+0j))\n",
      "Epoch 4000: <Test loss>: 0.00023513336782343686 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009513442), np.complex128(0.000862362519620834+0j)) <f>: (np.float32(0.0007214779), np.complex128(0.0005042931806817173+0j))\n",
      "Epoch 4800: <Test loss>: 0.00033804558916017413 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009999744), np.complex128(0.0010322043268111022+0j)) <f>: (np.float32(0.00023517432), np.complex128(0.0005085927420291361+0j))\n",
      "Epoch 5600: <Test loss>: 0.0003710131859406829 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010135715), np.complex128(0.0011097146017655496+0j)) <f>: (np.float32(9.920153e-05), np.complex128(0.0004765156446203399+0j))\n",
      "Epoch 6400: <Test loss>: 0.0004189874161966145 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.01029949), np.complex128(0.0011903436825132705+0j)) <f>: (np.float32(-6.456237e-05), np.complex128(0.0004813236000460009+0j))\n",
      "Epoch 7200: <Test loss>: 0.0004566111892927438 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010422627), np.complex128(0.0012541208474617337+0j)) <f>: (np.float32(-0.00018770972), np.complex128(0.0004930531997144487+0j))\n",
      "Epoch 8000: <Test loss>: 0.00041847783722914755 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010337484), np.complex128(0.0012218580260528011+0j)) <f>: (np.float32(-0.000102561076), np.complex128(0.00047177752598571374+0j))\n",
      "Epoch 8800: <Test loss>: 0.00044430888374336064 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010439069), np.complex128(0.0012716214017942542+0j)) <f>: (np.float32(-0.00020414403), np.complex128(0.00048350493730784836+0j))\n",
      "Epoch 9600: <Test loss>: 0.00043969694525003433 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010449267), np.complex128(0.0012768821863309036+0j)) <f>: (np.float32(-0.00021434738), np.complex128(0.0004829980655574762+0j))\n",
      "Epoch 10400: <Test loss>: 0.0004555644409265369 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010493316), np.complex128(0.0013042880206706965+0j)) <f>: (np.float32(-0.00025839318), np.complex128(0.0004990740381553444+0j))\n",
      "Epoch 11200: <Test loss>: 0.00044963197433389723 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010474238), np.complex128(0.0012980806019202375+0j)) <f>: (np.float32(-0.00023931469), np.complex128(0.0004972153709716028+0j))\n",
      "Epoch 12000: <Test loss>: 0.00044416351011022925 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010471032), np.complex128(0.0012950025818278138+0j)) <f>: (np.float32(-0.00023611287), np.complex128(0.0005020316040550568+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0797a520",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ca1d275e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0009262169478461146 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009753372), np.complex128(0.0009283871479479964+0j)) <f>: (np.float32(0.00048154933), np.complex128(0.0012229571467749195+0j))\n",
      "Epoch 3200: <Test loss>: 0.00021296419436112046 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009824973), np.complex128(0.0007264589190907285+0j)) <f>: (np.float32(0.0004099414), np.complex128(0.00046959901137345856+0j))\n",
      "Epoch 4800: <Test loss>: 0.00023160748241934925 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008814889), np.complex128(0.0005690384827251535+0j)) <f>: (np.float32(0.0014200358), np.complex128(0.0008559958273169793+0j))\n",
      "Epoch 6400: <Test loss>: 0.00023798123584128916 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010033965), np.complex128(0.0007744302612079152+0j)) <f>: (np.float32(0.00020096195), np.complex128(0.0004577098209999757+0j))\n",
      "Epoch 8000: <Test loss>: 0.000176313886186108 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009122319), np.complex128(0.000523763817568963+0j)) <f>: (np.float32(0.0011126044), np.complex128(0.0007150921408979569+0j))\n",
      "Epoch 9600: <Test loss>: 0.0002967610489577055 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010292878), np.complex128(0.0009149550782782988+0j)) <f>: (np.float32(-5.7950907e-05), np.complex128(0.0003373531524177598+0j))\n",
      "Epoch 11200: <Test loss>: 0.0002471832849550992 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009710072), np.complex128(0.0006957179181091426+0j)) <f>: (np.float32(0.00052484876), np.complex128(0.0006943082425015502+0j))\n",
      "Epoch 12800: <Test loss>: 0.00027439589030109346 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010238482), np.complex128(0.0008842476319401802+0j)) <f>: (np.float32(-3.5569387e-06), np.complex128(0.0003609027082567707+0j))\n",
      "Epoch 14400: <Test loss>: 0.0003129684482701123 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010343494), np.complex128(0.0009780641950131567+0j)) <f>: (np.float32(-0.00010856467), np.complex128(0.0003236037408175013+0j))\n",
      "Epoch 16000: <Test loss>: 0.00031493286951445043 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010391671), np.complex128(0.0009957253549359594+0j)) <f>: (np.float32(-0.00015675029), np.complex128(0.00031085700412576586+0j))\n",
      "Epoch 17600: <Test loss>: 0.0002806794655043632 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010238403), np.complex128(0.0009676101158087603+0j)) <f>: (np.float32(-3.4781845e-06), np.complex128(0.00031554609111691343+0j))\n",
      "Epoch 19200: <Test loss>: 0.0003324356221128255 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010483254), np.complex128(0.0010307708014002136+0j)) <f>: (np.float32(-0.00024833373), np.complex128(0.0003320126042437569+0j))\n",
      "Epoch 20800: <Test loss>: 0.0003337061498314142 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010477478), np.complex128(0.0010441360155042884+0j)) <f>: (np.float32(-0.00024255534), np.complex128(0.00033112702172056574+0j))\n",
      "Epoch 22400: <Test loss>: 0.0003372257051523775 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010499263), np.complex128(0.001045126289783704+0j)) <f>: (np.float32(-0.00026433868), np.complex128(0.00032173657396981903+0j))\n",
      "Epoch 24000: <Test loss>: 0.00033980514854192734 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010556266), np.complex128(0.0010467650123053987+0j)) <f>: (np.float32(-0.0003213416), np.complex128(0.0003346984346186796+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5e93df",
   "metadata": {},
   "source": [
    "### l4, w64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4b5af55",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b709a229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 5.5634136515436694e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008333485), np.complex128(0.0002643032822647428+0j)) <f>: (np.float32(0.0019014435), np.complex128(0.0009890495666441729+0j))\n",
      "Epoch 200: <Test loss>: 5.1761144277406856e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008738483), np.complex128(0.00037280950054625016+0j)) <f>: (np.float32(0.0014964319), np.complex128(0.0008139581386549685+0j))\n",
      "Epoch 300: <Test loss>: 5.891251203138381e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008768608), np.complex128(0.0003775544379411272+0j)) <f>: (np.float32(0.0014663159), np.complex128(0.0008205367516843886+0j))\n",
      "Epoch 400: <Test loss>: 6.250135629670694e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008780248), np.complex128(0.00042164089447964214+0j)) <f>: (np.float32(0.00145467), np.complex128(0.0007823924799801544+0j))\n",
      "Epoch 500: <Test loss>: 7.698262197664008e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009024264), np.complex128(0.00048808051572719126+0j)) <f>: (np.float32(0.0012106557), np.complex128(0.0007050969166857007+0j))\n",
      "Epoch 600: <Test loss>: 7.581755198771134e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008981677), np.complex128(0.0004945254176253656+0j)) <f>: (np.float32(0.0012532487), np.complex128(0.000712003718231755+0j))\n",
      "Epoch 700: <Test loss>: 7.991984602995217e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00900068), np.complex128(0.0005205529743698843+0j)) <f>: (np.float32(0.0012342412), np.complex128(0.000681894356455551+0j))\n",
      "Epoch 800: <Test loss>: 8.450826135231182e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009046963), np.complex128(0.0005437289572926374+0j)) <f>: (np.float32(0.0011879633), np.complex128(0.0006620964190762617+0j))\n",
      "Epoch 900: <Test loss>: 8.816087211016566e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009095847), np.complex128(0.0005639460422787919+0j)) <f>: (np.float32(0.0011390727), np.complex128(0.0006370540621849252+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51e564b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e992775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 7.150630699470639e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008475396), np.complex128(0.00031418481284952056+0j)) <f>: (np.float32(0.0017595271), np.complex128(0.0009418412913718086+0j))\n",
      "Epoch 400: <Test loss>: 7.406713120872155e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008680234), np.complex128(0.00034937706880773506+0j)) <f>: (np.float32(0.0015546886), np.complex128(0.0008745429186297766+0j))\n",
      "Epoch 600: <Test loss>: 7.311881199711934e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008647981), np.complex128(0.0003641859888903288+0j)) <f>: (np.float32(0.0015869391), np.complex128(0.0008357796303554157+0j))\n",
      "Epoch 800: <Test loss>: 7.524067768827081e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008887905), np.complex128(0.0004328096749038256+0j)) <f>: (np.float32(0.0013470169), np.complex128(0.0007534899436228692+0j))\n",
      "Epoch 1000: <Test loss>: 8.886029536370188e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008983041), np.complex128(0.0004940288215881158+0j)) <f>: (np.float32(0.0012518727), np.complex128(0.0006963110551054797+0j))\n",
      "Epoch 1200: <Test loss>: 9.383707219967619e-05 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008997998), np.complex128(0.0005036011558041599+0j)) <f>: (np.float32(0.0012369176), np.complex128(0.0006801133597083418+0j))\n",
      "Epoch 1400: <Test loss>: 0.00010548521822784096 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009231838), np.complex128(0.0005349702438927642+0j)) <f>: (np.float32(0.0010030906), np.complex128(0.0006385700470221039+0j))\n",
      "Epoch 1600: <Test loss>: 0.00010572134488029405 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009123577), np.complex128(0.0005415155560000287+0j)) <f>: (np.float32(0.0011113406), np.complex128(0.0006444600604116741+0j))\n",
      "Epoch 1800: <Test loss>: 0.00010711394861573353 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0091219535), np.complex128(0.0005536328418326954+0j)) <f>: (np.float32(0.0011129659), np.complex128(0.0006379732310667477+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2470376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca5659bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.00011572870425879955 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008661634), np.complex128(0.0004930113356980245+0j)) <f>: (np.float32(0.0015732892), np.complex128(0.0010917327381048066+0j))\n",
      "Epoch 800: <Test loss>: 0.00010280424612574279 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008665296), np.complex128(0.0004968936840635797+0j)) <f>: (np.float32(0.0015696243), np.complex128(0.000824586207248837+0j))\n",
      "Epoch 1200: <Test loss>: 0.00011533466749824584 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008826893), np.complex128(0.0005897231760887005+0j)) <f>: (np.float32(0.0014080298), np.complex128(0.0006758779264224453+0j))\n",
      "Epoch 1600: <Test loss>: 0.00013456650776788592 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008989891), np.complex128(0.0006679925217773117+0j)) <f>: (np.float32(0.0012450264), np.complex128(0.0006362849060286229+0j))\n",
      "Epoch 2000: <Test loss>: 0.00013240407861303538 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00893659), np.complex128(0.0006505302159325241+0j)) <f>: (np.float32(0.0012983311), np.complex128(0.0006573631211465571+0j))\n",
      "Epoch 2400: <Test loss>: 0.00014253509289119393 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009091674), np.complex128(0.000701718807711909+0j)) <f>: (np.float32(0.0011432562), np.complex128(0.0005730917441092769+0j))\n",
      "Epoch 2800: <Test loss>: 0.00015527733194176108 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00914239), np.complex128(0.0007183065361953989+0j)) <f>: (np.float32(0.0010925374), np.complex128(0.0005499994892193722+0j))\n",
      "Epoch 3200: <Test loss>: 0.00015461500152014196 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009183277), np.complex128(0.0007265975777875516+0j)) <f>: (np.float32(0.0010516413), np.complex128(0.0005407776709953886+0j))\n",
      "Epoch 3600: <Test loss>: 0.00015669880667701364 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009254982), np.complex128(0.0007397719934652567+0j)) <f>: (np.float32(0.0009799455), np.complex128(0.0005285194429740938+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "195877ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4b03653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.00019040335610043257 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.008957177), np.complex128(0.0006188387749174536+0j)) <f>: (np.float32(0.0012777519), np.complex128(0.0008306486242696814+0j))\n",
      "Epoch 1600: <Test loss>: 0.00016969762509688735 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00956008), np.complex128(0.0007230811272685764+0j)) <f>: (np.float32(0.00067484274), np.complex128(0.0006298622681755468+0j))\n",
      "Epoch 2400: <Test loss>: 0.00017893874610308558 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009652951), np.complex128(0.0007974027596387155+0j)) <f>: (np.float32(0.0005819738), np.complex128(0.0005283060633509863+0j))\n",
      "Epoch 3200: <Test loss>: 0.0001917261106427759 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00989397), np.complex128(0.0008818192018359383+0j)) <f>: (np.float32(0.00034095056), np.complex128(0.0004741885127497789+0j))\n",
      "Epoch 4000: <Test loss>: 0.0001903054362628609 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009775167), np.complex128(0.0008815588203398455+0j)) <f>: (np.float32(0.0004597565), np.complex128(0.0004457518404358685+0j))\n",
      "Epoch 4800: <Test loss>: 0.00020407605916261673 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009858206), np.complex128(0.0009049800249103196+0j)) <f>: (np.float32(0.0003767088), np.complex128(0.0004306626217342165+0j))\n",
      "Epoch 5600: <Test loss>: 0.00020222760213073343 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009833255), np.complex128(0.0009072073174447417+0j)) <f>: (np.float32(0.000401664), np.complex128(0.0004055647950211181+0j))\n",
      "Epoch 6400: <Test loss>: 0.0002098457916872576 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0098863635), np.complex128(0.0009366919539213068+0j)) <f>: (np.float32(0.0003485585), np.complex128(0.00038036960275466945+0j))\n",
      "Epoch 7200: <Test loss>: 0.00021197914611548185 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010008562), np.complex128(0.0009528257749782338+0j)) <f>: (np.float32(0.00022635279), np.complex128(0.00037051056345644757+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1eace16",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c882ee6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00036045044544152915 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.00027325365226715803 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.00978941), np.complex128(0.0008118308123173411+0j)) <f>: (np.float32(0.0004455066), np.complex128(0.0006331277883172885+0j))\n",
      "Epoch 3200: <Test loss>: 0.00024527471396140754 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.009881605), np.complex128(0.0008435555542545672+0j)) <f>: (np.float32(0.0003533157), np.complex128(0.0004331819474983612+0j))\n",
      "Epoch 4800: <Test loss>: 0.00025671286857686937 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010265084), np.complex128(0.0009200354669623566+0j)) <f>: (np.float32(-3.0164458e-05), np.complex128(0.0003291315670346745+0j))\n",
      "Epoch 6400: <Test loss>: 0.00024440247216261923 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010129898), np.complex128(0.0009016392129693422+0j)) <f>: (np.float32(0.00010502159), np.complex128(0.00034535472970847043+0j))\n",
      "Epoch 8000: <Test loss>: 0.0002543663722462952 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010298928), np.complex128(0.0009175829333335068+0j)) <f>: (np.float32(-6.399962e-05), np.complex128(0.00031549613973368005+0j))\n",
      "Epoch 9600: <Test loss>: 0.00025780382566154003 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010275585), np.complex128(0.0009419439850727036+0j)) <f>: (np.float32(-4.0665618e-05), np.complex128(0.0002742474609206092+0j))\n",
      "Epoch 11200: <Test loss>: 0.00024809440947137773 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.0101975575), np.complex128(0.0009373093847332355+0j)) <f>: (np.float32(3.7357895e-05), np.complex128(0.0002631568266604175+0j))\n",
      "Epoch 12800: <Test loss>: 0.00025119600468315184 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010198796), np.complex128(0.0009567295310794606+0j)) <f>: (np.float32(3.6121826e-05), np.complex128(0.0002526731254928845+0j))\n",
      "Epoch 14400: <Test loss>: 0.00025430027744732797 <O>: (np.float32(0.010234929), np.complex128(0.0011329745030121347+0j)) <O-f>: (np.float32(0.010272896), np.complex128(0.0009610659454473983+0j)) <f>: (np.float32(-3.7973587e-05), np.complex128(0.0002451156080703278+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_16x16x16_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cde36e",
   "metadata": {},
   "source": [
    "# 32x8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6aa768cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0042313966), np.complex128(2.0295486416161257e-05+0j))\n",
      "bin size 1: (np.float32(0.0042313966), np.complex128(2.029609420273659e-05+0j))\n",
      "jack bin size 2: (np.float32(0.0042313966), np.complex128(2.8444631597870962e-05+0j))\n",
      "bin size 2: (np.float32(0.0042313966), np.complex128(2.8444916299736605e-05+0j))\n",
      "jack bin size 4: (np.float32(0.0042313966), np.complex128(3.9715187412483375e-05+0j))\n",
      "bin size 4: (np.float32(0.0042313966), np.complex128(3.9715843677386486e-05+0j))\n",
      "jack bin size 5: (np.float32(0.0042313966), np.complex128(4.4168955841685635e-05+0j))\n",
      "bin size 5: (np.float32(0.0042313966), np.complex128(4.4169888560215895e-05+0j))\n",
      "jack bin size 10: (np.float32(0.0042313966), np.complex128(6.094574516365895e-05+0j))\n",
      "bin size 10: (np.float32(0.0042313966), np.complex128(6.0945876956599474e-05+0j))\n",
      "jack bin size 20: (np.float32(0.0042313966), np.complex128(8.229443837684097e-05+0j))\n",
      "bin size 20: (np.float32(0.0042313966), np.complex128(8.229468324129544e-05+0j))\n",
      "jack bin size 50: (np.float32(0.0042313966), np.complex128(0.00011704488402616336+0j))\n",
      "bin size 50: (np.float32(0.0042313966), np.complex128(0.00011704519153626684+0j))\n",
      "jack bin size 100: (np.float32(0.0042313966), np.complex128(0.00014302971782946048+0j))\n",
      "bin size 100: (np.float32(0.0042313966), np.complex128(0.0001430302047048114+0j))\n",
      "jack bin size 200: (np.float32(0.0042313966), np.complex128(0.0001612933822871836+0j))\n",
      "bin size 200: (np.float32(0.0042313966), np.complex128(0.00016129297186391514+0j))\n",
      "jack bin size 500: (np.float32(0.0042313966), np.complex128(0.00016809229205233764+0j))\n",
      "bin size 500: (np.float32(0.0042313966), np.complex128(0.00016809257766493678+0j))\n",
      "jack bin size 1000: (np.float32(0.0042313966), np.complex128(0.00016436984243544783+0j))\n",
      "bin size 1000: (np.float32(0.0042313966), np.complex128(0.00016436981592722735+0j))\n",
      "jack bin size 2000: (np.float32(0.0042313966), np.complex128(0.00016839051204442512+0j))\n",
      "bin size 2000: (np.float32(0.0042313966), np.complex128(0.00016839117077844482+0j))\n",
      "jack bin size 5000: (np.float32(0.0042313966), np.complex128(0.0001768351752763452+0j))\n",
      "bin size 5000: (np.float32(0.0042313966), np.complex128(0.00017683526124113167+0j))\n",
      "jack bin size 10000: (np.float32(0.0042313966), np.complex128(0.00019189609884051606+0j))\n",
      "bin size 10000: (np.float32(0.0042313966), np.complex128(0.00019189657177776098+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_32x8x8_0.1_0.5_sweep1.pkl\", \"rb\") as aa:\n",
    "    conf = pickle.load(aa)\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(32,8,8), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 4))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32695833",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array(\n",
    "    [(-i, -j, -k) for i, j, k in product(*list(map(lambda y: range(y), model.shape)))])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        return g1.apply(p, jnp.roll(x.reshape(model.shape), ind, axis=(0, 1, 2)).reshape(V))[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), ind, axis=(0, 1, 2)).reshape(V)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "56dc6a9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.9863032220164314e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 1.7299636283496511e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(4.20531e-05), np.complex128(4.5931454486590795e-05+0j)) <f>: (np.float32(0.00035040366), np.complex128(0.0003136184117363998+0j))\n",
      "Epoch 200: <Test loss>: 1.255454321835714e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(3.22713e-05), np.complex128(4.060924560614645e-05+0j)) <f>: (np.float32(0.00036018505), np.complex128(0.00030188323019909554+0j))\n",
      "Epoch 300: <Test loss>: 9.4732030220257e-07 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(3.5717978e-05), np.complex128(3.717293930649244e-05+0j)) <f>: (np.float32(0.00035673805), np.complex128(0.0003038283529197859+0j))\n",
      "Epoch 400: <Test loss>: 1.0264753882438526e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(7.545391e-05), np.complex128(3.4395634469924704e-05+0j)) <f>: (np.float32(0.00031700247), np.complex128(0.00030702467057377195+0j))\n",
      "Epoch 500: <Test loss>: 1.6467523664687178e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(5.612688e-05), np.complex128(4.996374421854712e-05+0j)) <f>: (np.float32(0.00033632934), np.complex128(0.00031210721588897884+0j))\n",
      "Epoch 600: <Test loss>: 8.817248158266011e-07 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(4.917667e-05), np.complex128(3.3856766083514736e-05+0j)) <f>: (np.float32(0.00034327974), np.complex128(0.0003015164443279246+0j))\n",
      "Epoch 700: <Test loss>: 1.137890649260953e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(5.3492622e-05), np.complex128(3.9372992479470374e-05+0j)) <f>: (np.float32(0.0003389638), np.complex128(0.0002881771415128033+0j))\n",
      "Epoch 800: <Test loss>: 1.0160432566408417e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(5.0622442e-05), np.complex128(3.377603513366038e-05+0j)) <f>: (np.float32(0.0003418341), np.complex128(0.0003021401864574809+0j))\n",
      "Epoch 900: <Test loss>: 1.1119216196675552e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(7.1284936e-05), np.complex128(2.799317142552128e-05+0j)) <f>: (np.float32(0.00032117145), np.complex128(0.0003005529059317254+0j))\n",
      "Epoch 1000: <Test loss>: 1.1749865507226787e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(6.277222e-05), np.complex128(3.4420185971223466e-05+0j)) <f>: (np.float32(0.0003296844), np.complex128(0.0002942784413246598+0j))\n",
      "Epoch 1100: <Test loss>: 1.229575559591467e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(7.140318e-05), np.complex128(3.601352012889941e-05+0j)) <f>: (np.float32(0.00032105308), np.complex128(0.0002934574308752865+0j))\n",
      "Epoch 1200: <Test loss>: 1.2242005595908267e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(7.264031e-05), np.complex128(3.5741760817736776e-05+0j)) <f>: (np.float32(0.00031981608), np.complex128(0.00029208200764476854+0j))\n",
      "Epoch 1300: <Test loss>: 1.1562185591174057e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(5.9359176e-05), np.complex128(3.454302673002267e-05+0j)) <f>: (np.float32(0.00033309727), np.complex128(0.0003071184523135949+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6e1a3067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.9863032220164314e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 3.455754949754919e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(8.553902e-05), np.complex128(5.300069713804113e-05+0j)) <f>: (np.float32(0.00030691753), np.complex128(0.00030326480617142137+0j))\n",
      "Epoch 400: <Test loss>: 2.6039858767035184e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(6.46386e-05), np.complex128(5.685058503328568e-05+0j)) <f>: (np.float32(0.00032781763), np.complex128(0.0003049590936603702+0j))\n",
      "Epoch 600: <Test loss>: 3.7530289773712866e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.000118298565), np.complex128(6.253320107254995e-05+0j)) <f>: (np.float32(0.0002741578), np.complex128(0.0003120635123930451+0j))\n",
      "Epoch 800: <Test loss>: 1.5950664646879886e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(5.0549883e-05), np.complex128(4.4908259866987125e-05+0j)) <f>: (np.float32(0.00034190647), np.complex128(0.0002930710133176258+0j))\n",
      "Epoch 1000: <Test loss>: 1.3648058256876539e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(6.64218e-05), np.complex128(4.335521189632847e-05+0j)) <f>: (np.float32(0.00032603453), np.complex128(0.0002991119274573068+0j))\n",
      "Epoch 1200: <Test loss>: 1.3322903669177322e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(6.640641e-05), np.complex128(3.86614865624521e-05+0j)) <f>: (np.float32(0.00032604998), np.complex128(0.0002965977395497232+0j))\n",
      "Epoch 1400: <Test loss>: 1.320047545050329e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(6.447567e-05), np.complex128(3.781124266070082e-05+0j)) <f>: (np.float32(0.00032798067), np.complex128(0.00030442586660874916+0j))\n",
      "Epoch 1600: <Test loss>: 1.6173553376575e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(7.1315e-05), np.complex128(3.8037538284330075e-05+0j)) <f>: (np.float32(0.00032114124), np.complex128(0.00030011520495394503+0j))\n",
      "Epoch 1800: <Test loss>: 1.6573227412663982e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(7.040412e-05), np.complex128(3.849253591965388e-05+0j)) <f>: (np.float32(0.0003220525), np.complex128(0.0002996737615868177+0j))\n",
      "Epoch 2000: <Test loss>: 1.8003821651291219e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(7.2111274e-05), np.complex128(3.71222703676745e-05+0j)) <f>: (np.float32(0.00032034516), np.complex128(0.00030431885964555584+0j))\n",
      "Epoch 2200: <Test loss>: 1.731857764752931e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(8.2312254e-05), np.complex128(3.6564896183095206e-05+0j)) <f>: (np.float32(0.0003101441), np.complex128(0.000299910483570598+0j))\n",
      "Epoch 2400: <Test loss>: 1.8672603800951038e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(8.6598164e-05), np.complex128(3.8146551231643715e-05+0j)) <f>: (np.float32(0.00030585832), np.complex128(0.0003014695058852672+0j))\n",
      "Epoch 2600: <Test loss>: 2.6155939849559218e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(9.371389e-05), np.complex128(4.835063169174805e-05+0j)) <f>: (np.float32(0.00029874235), np.complex128(0.0002992083732709022+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c79f1f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.9863032220164314e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 7.320024451473728e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(5.9348233e-05), np.complex128(0.00010126482175569904+0j)) <f>: (np.float32(0.0003331087), np.complex128(0.0002962038054982045+0j))\n",
      "Epoch 800: <Test loss>: 1.060730755853001e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00023885135), np.complex128(0.00012234362360416645+0j)) <f>: (np.float32(0.00015360485), np.complex128(0.0002564249659587538+0j))\n",
      "Epoch 1200: <Test loss>: 3.6504943636828102e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00012538182), np.complex128(5.6043335000674565e-05+0j)) <f>: (np.float32(0.00026707485), np.complex128(0.0003002637904970869+0j))\n",
      "Epoch 1600: <Test loss>: 2.7569953999773134e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00013419714), np.complex128(5.426287741125264e-05+0j)) <f>: (np.float32(0.00025825945), np.complex128(0.0002992567706111017+0j))\n",
      "Epoch 2000: <Test loss>: 2.8589081466634525e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00014367746), np.complex128(5.3057237346586745e-05+0j)) <f>: (np.float32(0.00024877882), np.complex128(0.00029862202331965205+0j))\n",
      "Epoch 2400: <Test loss>: 2.66945198745816e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00014779683), np.complex128(4.602730167646649e-05+0j)) <f>: (np.float32(0.00024465978), np.complex128(0.00029959304649454526+0j))\n",
      "Epoch 2800: <Test loss>: 3.0707201403856743e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00014594394), np.complex128(4.892839872675215e-05+0j)) <f>: (np.float32(0.0002465125), np.complex128(0.00029422198833281506+0j))\n",
      "Epoch 3200: <Test loss>: 3.0017019980732584e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00013002307), np.complex128(4.965075519423946e-05+0j)) <f>: (np.float32(0.00026243355), np.complex128(0.00030758076425860644+0j))\n",
      "Epoch 3600: <Test loss>: 3.0800006243225653e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00012302483), np.complex128(4.725916225293442e-05+0j)) <f>: (np.float32(0.00026943182), np.complex128(0.0003010291408337144+0j))\n",
      "Epoch 4000: <Test loss>: 3.2560803902015323e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00011824192), np.complex128(4.8957227810789715e-05+0j)) <f>: (np.float32(0.00027421452), np.complex128(0.0003017059741477359+0j))\n",
      "Epoch 4400: <Test loss>: 3.4104018595826346e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00012741555), np.complex128(5.037604134999736e-05+0j)) <f>: (np.float32(0.00026504052), np.complex128(0.0003017409242584173+0j))\n",
      "Epoch 4800: <Test loss>: 3.604688799896394e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00010463421), np.complex128(5.0688875762880725e-05+0j)) <f>: (np.float32(0.00028782233), np.complex128(0.0003082802104845298+0j))\n",
      "Epoch 5200: <Test loss>: 3.6274263948143926e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00012637611), np.complex128(5.121227111367483e-05+0j)) <f>: (np.float32(0.00026608005), np.complex128(0.0003050270275415676+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "787e30c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.9863032220164314e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 2.412071989965625e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00015720901), np.complex128(0.0001477078418369277+0j)) <f>: (np.float32(0.00023524756), np.complex128(0.000245148385692278+0j))\n",
      "Epoch 1600: <Test loss>: 2.110319837811403e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.0002847602), np.complex128(0.00013191819373926968+0j)) <f>: (np.float32(0.00010769626), np.complex128(0.0002564211759966609+0j))\n",
      "Epoch 2400: <Test loss>: 1.930551661644131e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00026213904), np.complex128(0.00012296960958157278+0j)) <f>: (np.float32(0.00013031719), np.complex128(0.0002454070704270991+0j))\n",
      "Epoch 3200: <Test loss>: 3.4201624657725915e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00019232256), np.complex128(0.00016353492968563778+0j)) <f>: (np.float32(0.00020013427), np.complex128(0.00026095812534630326+0j))\n",
      "Epoch 4000: <Test loss>: 1.2172861715953331e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00030308415), np.complex128(9.408259779704484e-05+0j)) <f>: (np.float32(8.937258e-05), np.complex128(0.00026198739341980486+0j))\n",
      "Epoch 4800: <Test loss>: 1.1192620149813592e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.0003391169), np.complex128(8.444513649181348e-05+0j)) <f>: (np.float32(5.3339718e-05), np.complex128(0.000274015115629455+0j))\n",
      "Epoch 5600: <Test loss>: 9.006705113279168e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00027687024), np.complex128(8.859026084834035e-05+0j)) <f>: (np.float32(0.000115586314), np.complex128(0.00025611382434274666+0j))\n",
      "Epoch 6400: <Test loss>: 8.810671715764329e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00028058898), np.complex128(9.156121054724288e-05+0j)) <f>: (np.float32(0.00011186716), np.complex128(0.00025911044746687297+0j))\n",
      "Epoch 7200: <Test loss>: 7.878095857449807e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00025538803), np.complex128(8.55574586509293e-05+0j)) <f>: (np.float32(0.00013706855), np.complex128(0.0002794484306580342+0j))\n",
      "Epoch 8000: <Test loss>: 7.816605830157641e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00024596052), np.complex128(8.042050597195294e-05+0j)) <f>: (np.float32(0.00014649615), np.complex128(0.00025951665528684334+0j))\n",
      "Epoch 8800: <Test loss>: 7.13658027962083e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00024406039), np.complex128(7.378697020734913e-05+0j)) <f>: (np.float32(0.00014839605), np.complex128(0.00027119173658844773+0j))\n",
      "Epoch 9600: <Test loss>: 1.1359790732967667e-05 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00023199027), np.complex128(9.613302279086579e-05+0j)) <f>: (np.float32(0.0001604663), np.complex128(0.0002597442274458219+0j))\n",
      "Epoch 10400: <Test loss>: 7.034331702016061e-06 <O>: (np.float32(0.00039245636), np.complex128(0.0003076857097361425+0j)) <O-f>: (np.float32(0.00022366142), np.complex128(7.291576258222179e-05+0j)) <f>: (np.float32(0.00016879497), np.complex128(0.0002787220899730747+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4c9f51",
   "metadata": {},
   "source": [
    "## sweep 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d593cfe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(9.8053504e-05), np.complex128(2.521083766003614e-05+0j))\n",
      "bin size 1: (np.float32(9.8053504e-05), np.complex128(2.5210775971631163e-05+0j))\n",
      "jack bin size 2: (np.float32(9.8053504e-05), np.complex128(3.526782874255484e-05+0j))\n",
      "bin size 2: (np.float32(9.8053504e-05), np.complex128(3.5267799980714864e-05+0j))\n",
      "jack bin size 4: (np.float32(9.8053504e-05), np.complex128(4.91185145676044e-05+0j))\n",
      "bin size 4: (np.float32(9.8053504e-05), np.complex128(4.9118462629868035e-05+0j))\n",
      "jack bin size 5: (np.float32(9.8053504e-05), np.complex128(5.455441014502263e-05+0j))\n",
      "bin size 5: (np.float32(9.8053504e-05), np.complex128(5.4554446764212234e-05+0j))\n",
      "jack bin size 10: (np.float32(9.8053504e-05), np.complex128(7.484604565115395e-05+0j))\n",
      "bin size 10: (np.float32(9.8053504e-05), np.complex128(7.484604918130677e-05+0j))\n",
      "jack bin size 20: (np.float32(9.8053504e-05), np.complex128(0.00010060286172095932+0j))\n",
      "bin size 20: (np.float32(9.8053504e-05), np.complex128(0.00010060289902345453+0j))\n",
      "jack bin size 50: (np.float32(9.8053504e-05), np.complex128(0.00014002786822619895+0j))\n",
      "bin size 50: (np.float32(9.8053504e-05), np.complex128(0.00014002786452395387+0j))\n",
      "jack bin size 100: (np.float32(9.8053504e-05), np.complex128(0.00017078660304477075+0j))\n",
      "bin size 100: (np.float32(9.8053504e-05), np.complex128(0.00017078662453979954+0j))\n",
      "jack bin size 200: (np.float32(9.8053504e-05), np.complex128(0.0001862182310965717+0j))\n",
      "bin size 200: (np.float32(9.8053504e-05), np.complex128(0.00018621823374301904+0j))\n",
      "jack bin size 500: (np.float32(9.8053504e-05), np.complex128(0.00020791723296067582+0j))\n",
      "bin size 500: (np.float32(9.8053504e-05), np.complex128(0.00020791718331695087+0j))\n",
      "jack bin size 1000: (np.float32(9.8053504e-05), np.complex128(0.00020750482006982258+0j))\n",
      "bin size 1000: (np.float32(9.8053504e-05), np.complex128(0.0002075048229948676+0j))\n",
      "jack bin size 2000: (np.float32(9.8053504e-05), np.complex128(0.00021365668726502918+0j))\n",
      "bin size 2000: (np.float32(9.8053504e-05), np.complex128(0.0002136566742722477+0j))\n",
      "jack bin size 5000: (np.float32(9.8053504e-05), np.complex128(0.00017396728400272324+0j))\n",
      "bin size 5000: (np.float32(9.8053504e-05), np.complex128(0.00017396727565662743+0j))\n",
      "jack bin size 10000: (np.float32(9.8053504e-05), np.complex128(0.00018157672093366273+0j))\n",
      "bin size 10000: (np.float32(9.8053504e-05), np.complex128(0.0001815767221463223+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_32x8x8_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(32,8,8), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "957c6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e767489",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.536474418477155e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 4.372862349555362e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.919664e-05), np.complex128(5.9883827278596636e-05+0j)) <f>: (np.float32(5.7978827e-06), np.complex128(0.00019238727679595993+0j))\n",
      "Epoch 200: <Test loss>: 8.837379937176593e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-8.963316e-05), np.complex128(8.577114749688537e-05+0j)) <f>: (np.float32(0.00013462781), np.complex128(0.00017653042426587554+0j))\n",
      "Epoch 300: <Test loss>: 1.0741470077846316e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.4242693e-05), np.complex128(2.9466822465409555e-05+0j)) <f>: (np.float32(-9.248152e-06), np.complex128(0.00017984724368531887+0j))\n",
      "Epoch 400: <Test loss>: 1.268556388822617e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.5888126e-05), np.complex128(3.662810450486292e-05+0j)) <f>: (np.float32(-8.9356024e-07), np.complex128(0.00018338746471968032+0j))\n",
      "Epoch 500: <Test loss>: 1.2456074500732939e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.199965e-05), np.complex128(3.652035619971209e-05+0j)) <f>: (np.float32(-3.7005117e-05), np.complex128(0.00016910910761493516+0j))\n",
      "Epoch 600: <Test loss>: 9.427961913388572e-07 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.437415e-05), np.complex128(2.7157098255465854e-05+0j)) <f>: (np.float32(-3.937953e-05), np.complex128(0.00018528709203767337+0j))\n",
      "Epoch 700: <Test loss>: 8.482983275825973e-07 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.0431907e-05), np.complex128(2.5006136191898956e-05+0j)) <f>: (np.float32(4.5626653e-06), np.complex128(0.00017777001144916266+0j))\n",
      "Epoch 800: <Test loss>: 8.923960308493406e-07 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.5344022e-05), np.complex128(2.640525857868436e-05+0j)) <f>: (np.float32(-1.0349456e-05), np.complex128(0.00017867088068138144+0j))\n",
      "Epoch 900: <Test loss>: 9.924692676577251e-07 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.5608573e-05), np.complex128(2.5901629703826902e-05+0j)) <f>: (np.float32(-6.1406473e-07), np.complex128(0.00017465079335875786+0j))\n",
      "Epoch 1000: <Test loss>: 9.9826218047383e-07 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.3409127e-05), np.complex128(2.5668689753348505e-05+0j)) <f>: (np.float32(1.5854012e-06), np.complex128(0.00017828869709810905+0j))\n",
      "Epoch 1100: <Test loss>: 2.069735046461574e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.099415e-05), np.complex128(3.3698162512957716e-05+0j)) <f>: (np.float32(-1.5999578e-05), np.complex128(0.0001610272909594115+0j))\n",
      "Epoch 1200: <Test loss>: 1.1924081491088145e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.6433295e-05), np.complex128(2.8634212194818764e-05+0j)) <f>: (np.float32(8.561285e-06), np.complex128(0.00018204204403426632+0j))\n",
      "Epoch 1300: <Test loss>: 1.1353070021868916e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.4166365e-05), np.complex128(3.221266784152898e-05+0j)) <f>: (np.float32(-1.9171823e-05), np.complex128(0.0001748473798025497+0j))\n",
      "Epoch 1400: <Test loss>: 1.1139204616483767e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.2225095e-05), np.complex128(2.8380364022501246e-05+0j)) <f>: (np.float32(-1.7230563e-05), np.complex128(0.00018389282999976448+0j))\n",
      "Epoch 1500: <Test loss>: 1.139014557338669e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.098565e-05), np.complex128(2.757671762539483e-05+0j)) <f>: (np.float32(-1.5991096e-05), np.complex128(0.00017700017341683517+0j))\n",
      "Epoch 1600: <Test loss>: 1.2085087064406252e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.772247e-05), np.complex128(2.6299736281603815e-05+0j)) <f>: (np.float32(-2.7278923e-06), np.complex128(0.00017840957944553388+0j))\n",
      "Epoch 1700: <Test loss>: 1.3749428262599395e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.440684e-05), np.complex128(3.088393726114772e-05+0j)) <f>: (np.float32(-9.412242e-06), np.complex128(0.00016711907622208067+0j))\n",
      "Epoch 1800: <Test loss>: 1.2503555808507372e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.385088e-05), np.complex128(2.7943182380530684e-05+0j)) <f>: (np.float32(-1.885636e-05), np.complex128(0.0001729507337099687+0j))\n",
      "Epoch 1900: <Test loss>: 1.2279206202947535e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.140814e-05), np.complex128(2.5607927463601024e-05+0j)) <f>: (np.float32(-1.6413625e-05), np.complex128(0.00017619031084759306+0j))\n",
      "Epoch 2000: <Test loss>: 1.3016092452744488e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.310809e-05), np.complex128(2.74523228235617e-05+0j)) <f>: (np.float32(-1.8113471e-05), np.complex128(0.00017350919017451806+0j))\n",
      "Epoch 2100: <Test loss>: 1.2283321666473057e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.2785e-05), np.complex128(2.633730685770722e-05+0j)) <f>: (np.float32(-1.7790448e-05), np.complex128(0.00017728091604818884+0j))\n",
      "Epoch 2200: <Test loss>: 1.2492315590861836e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.0442155e-05), np.complex128(2.5464646281631075e-05+0j)) <f>: (np.float32(-1.544759e-05), np.complex128(0.00017655351290523676+0j))\n",
      "Epoch 2300: <Test loss>: 1.312716335633013e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.9869606e-05), np.complex128(2.5658301054954607e-05+0j)) <f>: (np.float32(-1.487507e-05), np.complex128(0.00017517447414602756+0j))\n",
      "Epoch 2400: <Test loss>: 1.2855562090408057e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.1742e-05), np.complex128(2.662669187123935e-05+0j)) <f>: (np.float32(-1.674744e-05), np.complex128(0.00017487901567859754+0j))\n",
      "Epoch 2500: <Test loss>: 1.306634658249095e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.108265e-05), np.complex128(2.66403393027299e-05+0j)) <f>: (np.float32(-1.6088141e-05), np.complex128(0.00017837827657870761+0j))\n",
      "Epoch 2600: <Test loss>: 1.3574772310676053e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.592885e-05), np.complex128(2.8349687530163148e-05+0j)) <f>: (np.float32(-1.0934262e-05), np.complex128(0.0001738526495425981+0j))\n",
      "Epoch 2700: <Test loss>: 1.3432548939817934e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.171153e-05), np.complex128(2.916338565204641e-05+0j)) <f>: (np.float32(-1.6717015e-05), np.complex128(0.00017550080732557045+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fd3052c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.536474418477155e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 3.6237652238924056e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00011544553), np.complex128(0.00018782259253543652+0j)) <f>: (np.float32(-7.0450944e-05), np.complex128(0.00021332118791788656+0j))\n",
      "Epoch 400: <Test loss>: 1.9584651909099193e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-8.886349e-06), np.complex128(4.499882648206386e-05+0j)) <f>: (np.float32(5.3880907e-05), np.complex128(0.00017002234576109744+0j))\n",
      "Epoch 600: <Test loss>: 1.6574776964262128e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-3.373525e-05), np.complex128(3.8623293576256024e-05+0j)) <f>: (np.float32(7.8729805e-05), np.complex128(0.00019110027544255604+0j))\n",
      "Epoch 800: <Test loss>: 3.4806757867045235e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-5.5799977e-05), np.complex128(6.184134269960347e-05+0j)) <f>: (np.float32(0.00010079465), np.complex128(0.00017713155348353+0j))\n",
      "Epoch 1000: <Test loss>: 1.1542191487023956e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.357219e-05), np.complex128(3.548749241949682e-05+0j)) <f>: (np.float32(2.1422315e-05), np.complex128(0.00018755107901679454+0j))\n",
      "Epoch 1200: <Test loss>: 1.2579255326272687e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.654391e-06), np.complex128(3.813740537123741e-05+0j)) <f>: (np.float32(3.6340138e-05), np.complex128(0.00018566432805536834+0j))\n",
      "Epoch 1400: <Test loss>: 1.274335772905033e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.3320235e-06), np.complex128(3.47991187501902e-05+0j)) <f>: (np.float32(4.26626e-05), np.complex128(0.00017993726717821284+0j))\n",
      "Epoch 1600: <Test loss>: 1.3431495062832255e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-1.7453704e-05), np.complex128(4.1357588686101606e-05+0j)) <f>: (np.float32(6.2448235e-05), np.complex128(0.00018495736533558703+0j))\n",
      "Epoch 1800: <Test loss>: 1.384232064083335e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.147418e-06), np.complex128(4.1381799249387826e-05+0j)) <f>: (np.float32(3.684711e-05), np.complex128(0.00017981690813099332+0j))\n",
      "Epoch 2000: <Test loss>: 1.531633643025998e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(1.2326785e-05), np.complex128(4.0298361675846484e-05+0j)) <f>: (np.float32(3.2667762e-05), np.complex128(0.0001862605889953553+0j))\n",
      "Epoch 2200: <Test loss>: 1.4641541383753065e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-9.635368e-06), np.complex128(4.17343014035913e-05+0j)) <f>: (np.float32(5.4629825e-05), np.complex128(0.0001785129074497081+0j))\n",
      "Epoch 2400: <Test loss>: 1.7231459423783235e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-6.679865e-06), np.complex128(4.690213275907598e-05+0j)) <f>: (np.float32(5.1674484e-05), np.complex128(0.00018118979414839473+0j))\n",
      "Epoch 2600: <Test loss>: 1.8838910591512104e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-5.156249e-06), np.complex128(4.7103975991293846e-05+0j)) <f>: (np.float32(5.0150782e-05), np.complex128(0.00018365594944016434+0j))\n",
      "Epoch 2800: <Test loss>: 1.736804847496387e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.727834e-06), np.complex128(4.1821137522507506e-05+0j)) <f>: (np.float32(4.2266773e-05), np.complex128(0.00017960232333165546+0j))\n",
      "Epoch 3000: <Test loss>: 1.6738856629672227e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-8.6129734e-07), np.complex128(4.196584985125105e-05+0j)) <f>: (np.float32(4.5855857e-05), np.complex128(0.00017914367448808102+0j))\n",
      "Epoch 3200: <Test loss>: 1.7622649011173053e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-3.9561954e-07), np.complex128(4.3739131884827545e-05+0j)) <f>: (np.float32(4.5390112e-05), np.complex128(0.00017897429965496486+0j))\n",
      "Epoch 3400: <Test loss>: 1.8191412891610526e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(9.28757e-06), np.complex128(4.4742611565636e-05+0j)) <f>: (np.float32(3.5707002e-05), np.complex128(0.00017770390118989281+0j))\n",
      "Epoch 3600: <Test loss>: 1.970591029021307e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-4.82739e-06), np.complex128(4.623276836616652e-05+0j)) <f>: (np.float32(4.982194e-05), np.complex128(0.0001844465133321382+0j))\n",
      "Epoch 3800: <Test loss>: 1.896529397527047e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.766551e-06), np.complex128(4.559921039336531e-05+0j)) <f>: (np.float32(4.0228046e-05), np.complex128(0.00017983008578161775+0j))\n",
      "Epoch 4000: <Test loss>: 1.8830737644748297e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(7.789431e-06), np.complex128(4.475641559074859e-05+0j)) <f>: (np.float32(3.720511e-05), np.complex128(0.0001769244851780501+0j))\n",
      "Epoch 4200: <Test loss>: 1.9982937828899594e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.485639e-06), np.complex128(4.5850477744215766e-05+0j)) <f>: (np.float32(3.6508936e-05), np.complex128(0.00018020026517533215+0j))\n",
      "Epoch 4400: <Test loss>: 1.980716888283496e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.41257e-06), np.complex128(4.578559248315378e-05+0j)) <f>: (np.float32(4.0581926e-05), np.complex128(0.00017743828669174698+0j))\n",
      "Epoch 4600: <Test loss>: 2.2766694200981874e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-1.476928e-05), np.complex128(5.318223473653507e-05+0j)) <f>: (np.float32(5.976376e-05), np.complex128(0.00017037338505336336+0j))\n",
      "Epoch 4800: <Test loss>: 2.0552090518322075e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-2.9589523e-06), np.complex128(4.676012414805984e-05+0j)) <f>: (np.float32(4.7953396e-05), np.complex128(0.00018057967368175824+0j))\n",
      "Epoch 5000: <Test loss>: 2.1159933112357976e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.0622548e-06), np.complex128(4.6720389012016325e-05+0j)) <f>: (np.float32(4.1932366e-05), np.complex128(0.00017894572429223897+0j))\n",
      "Epoch 5200: <Test loss>: 2.110627065121662e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.5939314e-06), np.complex128(4.649261863326303e-05+0j)) <f>: (np.float32(4.2400643e-05), np.complex128(0.00017838049664018464+0j))\n",
      "Epoch 5400: <Test loss>: 2.144287464034278e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.790231e-06), np.complex128(4.678265777205179e-05+0j)) <f>: (np.float32(4.020435e-05), np.complex128(0.00018165025075631474+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "536a40f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1eecd3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.536474418477155e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 7.942108823044691e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(7.8766054e-05), np.complex128(8.63885624512321e-05+0j)) <f>: (np.float32(-3.3771492e-05), np.complex128(0.0002076883527928494+0j))\n",
      "Epoch 800: <Test loss>: 6.713363745802781e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00012490043), np.complex128(8.217359337487936e-05+0j)) <f>: (np.float32(-7.990597e-05), np.complex128(0.00019555758801550064+0j))\n",
      "Epoch 1200: <Test loss>: 3.709053544298513e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00010270253), np.complex128(4.9745262418437944e-05+0j)) <f>: (np.float32(-5.770802e-05), np.complex128(0.0001719202287450728+0j))\n",
      "Epoch 1600: <Test loss>: 2.9636883027706062e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00011126055), np.complex128(4.497791033143375e-05+0j)) <f>: (np.float32(-6.626599e-05), np.complex128(0.00017154935161775133+0j))\n",
      "Epoch 2000: <Test loss>: 2.5105166514549637e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00010362012), np.complex128(3.678630767147497e-05+0j)) <f>: (np.float32(-5.8625476e-05), np.complex128(0.00017290718878985476+0j))\n",
      "Epoch 2400: <Test loss>: 2.3889472231530817e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.000104735824), np.complex128(3.985085278361219e-05+0j)) <f>: (np.float32(-5.9741255e-05), np.complex128(0.0001662406930411079+0j))\n",
      "Epoch 2800: <Test loss>: 2.2318597530102124e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.696873e-05), np.complex128(4.737151322125461e-05+0j)) <f>: (np.float32(-4.197415e-05), np.complex128(0.00016440302129590626+0j))\n",
      "Epoch 3200: <Test loss>: 2.538955413911026e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.08333e-05), np.complex128(5.333825748562503e-05+0j)) <f>: (np.float32(-3.58387e-05), np.complex128(0.00015746320426417187+0j))\n",
      "Epoch 3600: <Test loss>: 2.311733169335639e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.2082144e-05), np.complex128(4.9570773515133606e-05+0j)) <f>: (np.float32(-3.7087582e-05), np.complex128(0.00016187887482683997+0j))\n",
      "Epoch 4000: <Test loss>: 2.4608282274130033e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.4751446e-05), np.complex128(5.025882606840515e-05+0j)) <f>: (np.float32(-3.9756902e-05), np.complex128(0.0001585241399288852+0j))\n",
      "Epoch 4400: <Test loss>: 2.4659366317791864e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(7.033678e-05), np.complex128(5.1259237307100706e-05+0j)) <f>: (np.float32(-2.5342244e-05), np.complex128(0.00016108834265003008+0j))\n",
      "Epoch 4800: <Test loss>: 2.5363476652273675e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.1497034e-05), np.complex128(5.316724535716955e-05+0j)) <f>: (np.float32(-1.6502465e-05), np.complex128(0.00015767550557170478+0j))\n",
      "Epoch 5200: <Test loss>: 2.585627271400881e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.597945e-05), np.complex128(5.282442822115967e-05+0j)) <f>: (np.float32(-2.0984962e-05), np.complex128(0.00015945323565702633+0j))\n",
      "Epoch 5600: <Test loss>: 2.713216645133798e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.193313e-05), np.complex128(5.481531592344101e-05+0j)) <f>: (np.float32(-1.6938475e-05), np.complex128(0.00016085767826256564+0j))\n",
      "Epoch 6000: <Test loss>: 2.783069930956117e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.832152e-05), np.complex128(5.4002273909012126e-05+0j)) <f>: (np.float32(-1.3327002e-05), np.complex128(0.00016138711535209369+0j))\n",
      "Epoch 6400: <Test loss>: 2.757034735623165e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.7992824e-05), np.complex128(5.468437590464611e-05+0j)) <f>: (np.float32(-1.2998294e-05), np.complex128(0.00016028112829697842+0j))\n",
      "Epoch 6800: <Test loss>: 2.8222905257280217e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.652876e-05), np.complex128(5.376024359966435e-05+0j)) <f>: (np.float32(-1.1534218e-05), np.complex128(0.00016080591911498668+0j))\n",
      "Epoch 7200: <Test loss>: 2.892219526984263e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.5310855e-05), np.complex128(5.571162213644537e-05+0j)) <f>: (np.float32(-3.1633581e-07), np.complex128(0.00016098396804544525+0j))\n",
      "Epoch 7600: <Test loss>: 2.9185462153691333e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.8174468e-05), np.complex128(5.3670414362149626e-05+0j)) <f>: (np.float32(-1.3179916e-05), np.complex128(0.00016175253747121443+0j))\n",
      "Epoch 8000: <Test loss>: 3.1926624615152832e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.6642402e-05), np.complex128(5.2821613500358427e-05+0j)) <f>: (np.float32(-1.1647964e-05), np.complex128(0.00015773214885453325+0j))\n",
      "Epoch 8400: <Test loss>: 3.068520300075761e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.4731358e-05), np.complex128(5.366740142157365e-05+0j)) <f>: (np.float32(2.6317966e-07), np.complex128(0.0001682442350938084+0j))\n",
      "Epoch 8800: <Test loss>: 3.0731234801351093e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.3435117e-05), np.complex128(5.6039802724288777e-05+0j)) <f>: (np.float32(1.559407e-06), np.complex128(0.00016378194324012242+0j))\n",
      "Epoch 9200: <Test loss>: 3.1468330234929454e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.888411e-05), np.complex128(5.5774172368560965e-05+0j)) <f>: (np.float32(6.110444e-06), np.complex128(0.00016154420055917624+0j))\n",
      "Epoch 9600: <Test loss>: 3.4117313134629512e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.7208312e-05), np.complex128(5.828302816420415e-05+0j)) <f>: (np.float32(7.786291e-06), np.complex128(0.00016168228838304812+0j))\n",
      "Epoch 10000: <Test loss>: 3.4527706702647265e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.5601603e-05), np.complex128(5.42752820047556e-05+0j)) <f>: (np.float32(9.3929575e-06), np.complex128(0.0001701043453175101+0j))\n",
      "Epoch 10400: <Test loss>: 3.294614543847274e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.0708663e-05), np.complex128(5.774770394736645e-05+0j)) <f>: (np.float32(1.42859335e-05), np.complex128(0.00016608654148669147+0j))\n",
      "Epoch 10800: <Test loss>: 3.815082436631201e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.201783e-05), np.complex128(5.811798245096822e-05+0j)) <f>: (np.float32(1.2976686e-05), np.complex128(0.00016409860329465818+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ceac8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fa02438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.536474418477155e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 3.1620867957826704e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-6.062931e-06), np.complex128(0.00016070443059032198+0j)) <f>: (np.float32(5.105746e-05), np.complex128(0.00019631052186543866+0j))\n",
      "Epoch 1600: <Test loss>: 2.3767621314618737e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.6188134e-05), np.complex128(0.00013158282173799887+0j)) <f>: (np.float32(-4.1193653e-05), np.complex128(0.00016929828856794292+0j))\n",
      "Epoch 2400: <Test loss>: 1.6979554857243784e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.0695296e-05), np.complex128(0.0001003500374952938+0j)) <f>: (np.float32(-1.5700753e-05), np.complex128(0.00016495438942131107+0j))\n",
      "Epoch 3200: <Test loss>: 1.6540390788577497e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.8061807e-05), np.complex128(9.280848865778974e-05+0j)) <f>: (np.float32(-3.0672002e-06), np.complex128(0.00017365544465310905+0j))\n",
      "Epoch 4000: <Test loss>: 1.1700390132318716e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.250685e-05), np.complex128(8.567382158749014e-05+0j)) <f>: (np.float32(-3.7512185e-05), np.complex128(0.00017810614461436847+0j))\n",
      "Epoch 4800: <Test loss>: 3.8302980101434514e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-0.00014221887), np.complex128(0.00016552784715841243+0j)) <f>: (np.float32(0.00018721329), np.complex128(0.00022663096391536326+0j))\n",
      "Epoch 5600: <Test loss>: 1.0002875569625758e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(7.919558e-05), np.complex128(7.991235768623151e-05+0j)) <f>: (np.float32(-3.420109e-05), np.complex128(0.00018604050161507077+0j))\n",
      "Epoch 6400: <Test loss>: 9.474456419411581e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.818868e-05), np.complex128(8.391919599911176e-05+0j)) <f>: (np.float32(-2.3194123e-05), np.complex128(0.00017848750360337795+0j))\n",
      "Epoch 7200: <Test loss>: 7.932041626190767e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.795904e-05), np.complex128(7.191459414398675e-05+0j)) <f>: (np.float32(-1.2964309e-05), np.complex128(0.00018458190536707367+0j))\n",
      "Epoch 8000: <Test loss>: 8.363323104276787e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.9964106e-05), np.complex128(7.283389781525595e-05+0j)) <f>: (np.float32(-4.969519e-06), np.complex128(0.00017978604927646247+0j))\n",
      "Epoch 8800: <Test loss>: 7.4835988925769925e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.3331933e-05), np.complex128(6.795583601966639e-05+0j)) <f>: (np.float32(-8.33734e-06), np.complex128(0.000184891397794555+0j))\n",
      "Epoch 9600: <Test loss>: 7.756377272016834e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.1389794e-05), np.complex128(6.921019454056681e-05+0j)) <f>: (np.float32(3.6048045e-06), np.complex128(0.00018407381258288914+0j))\n",
      "Epoch 10400: <Test loss>: 8.889562195690814e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.374627e-05), np.complex128(7.645411242190974e-05+0j)) <f>: (np.float32(-8.751599e-06), np.complex128(0.0001777840771243779+0j))\n",
      "Epoch 11200: <Test loss>: 7.770779120619409e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.423589e-05), np.complex128(6.931491008316419e-05+0j)) <f>: (np.float32(7.586507e-07), np.complex128(0.00018731369101457107+0j))\n",
      "Epoch 12000: <Test loss>: 8.030883691390045e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.0848448e-05), np.complex128(6.698252142417721e-05+0j)) <f>: (np.float32(-1.5853879e-05), np.complex128(0.00018024587158110334+0j))\n",
      "Epoch 12800: <Test loss>: 9.779127140063792e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.3367625e-05), np.complex128(6.239290111599203e-05+0j)) <f>: (np.float32(1.1626913e-05), np.complex128(0.00018031361517131702+0j))\n",
      "Epoch 13600: <Test loss>: 8.046031325648073e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.3705426e-05), np.complex128(6.46827676689027e-05+0j)) <f>: (np.float32(2.128908e-05), np.complex128(0.00018569085779001897+0j))\n",
      "Epoch 14400: <Test loss>: 8.216426977014635e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.2863685e-05), np.complex128(6.914247473672611e-05+0j)) <f>: (np.float32(1.2130897e-05), np.complex128(0.00018337731586721387+0j))\n",
      "Epoch 15200: <Test loss>: 8.169802640622947e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.6470992e-05), np.complex128(6.76727861101348e-05+0j)) <f>: (np.float32(1.8523626e-05), np.complex128(0.00018409325397839523+0j))\n",
      "Epoch 16000: <Test loss>: 8.614842045062687e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(1.9498064e-05), np.complex128(7.189269482327396e-05+0j)) <f>: (np.float32(2.5496453e-05), np.complex128(0.00018390857657866948+0j))\n",
      "Epoch 16800: <Test loss>: 8.649353731016163e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.2220225e-05), np.complex128(7.085870119034269e-05+0j)) <f>: (np.float32(2.2774362e-05), np.complex128(0.00018410638405627372+0j))\n",
      "Epoch 17600: <Test loss>: 8.447933396382723e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.814505e-06), np.complex128(6.214316798619802e-05+0j)) <f>: (np.float32(3.6180092e-05), np.complex128(0.00018086488815122995+0j))\n",
      "Epoch 18400: <Test loss>: 8.864128176355734e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(9.270343e-06), np.complex128(7.059058912303519e-05+0j)) <f>: (np.float32(3.57242e-05), np.complex128(0.00018222259846267756+0j))\n",
      "Epoch 19200: <Test loss>: 8.93028118298389e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(1.3641461e-05), np.complex128(7.593908194560948e-05+0j)) <f>: (np.float32(3.1353105e-05), np.complex128(0.00018442515316921266+0j))\n",
      "Epoch 20000: <Test loss>: 8.84114888322074e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(1.8361878e-05), np.complex128(7.597097154296897e-05+0j)) <f>: (np.float32(2.6632519e-05), np.complex128(0.00018306273315591733+0j))\n",
      "Epoch 20800: <Test loss>: 9.122059054789133e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.5590036e-05), np.complex128(7.636952807963453e-05+0j)) <f>: (np.float32(1.9404504e-05), np.complex128(0.00018267490427346047+0j))\n",
      "Epoch 21600: <Test loss>: 9.149357538262848e-06 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(1.5354306e-05), np.complex128(7.62572088264783e-05+0j)) <f>: (np.float32(2.9640223e-05), np.complex128(0.0001800311757786916+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c54d3b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d82a1c32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 5.536474418477155e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 8.453460031887516e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00021319932), np.complex128(0.000263404696524329+0j)) <f>: (np.float32(-0.00016820477), np.complex128(0.00019638771657451174+0j))\n",
      "Epoch 3200: <Test loss>: 7.627469312865287e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00014384404), np.complex128(0.0002518320076295621+0j)) <f>: (np.float32(-9.884936e-05), np.complex128(0.0001769657148911951+0j))\n",
      "Epoch 4800: <Test loss>: 6.086793655413203e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(-0.00015521325), np.complex128(0.00020829799753202652+0j)) <f>: (np.float32(0.00020020793), np.complex128(0.00016699844759596752+0j))\n",
      "Epoch 6400: <Test loss>: 5.006723222322762e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00011138899), np.complex128(0.0001793662039359903+0j)) <f>: (np.float32(-6.639434e-05), np.complex128(0.00015844420185812978+0j))\n",
      "Epoch 8000: <Test loss>: 4.053345037391409e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.2034342e-05), np.complex128(0.00015122516452291168+0j)) <f>: (np.float32(1.2960175e-05), np.complex128(0.00011380681327771449+0j))\n",
      "Epoch 9600: <Test loss>: 3.5829460102831945e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00020783214), np.complex128(0.0001394257439210111+0j)) <f>: (np.float32(-0.0001628375), np.complex128(0.00014443510649538558+0j))\n",
      "Epoch 11200: <Test loss>: 3.228762943763286e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(7.324747e-05), np.complex128(0.00010970697024652823+0j)) <f>: (np.float32(-2.8252727e-05), np.complex128(0.00013126580488786858+0j))\n",
      "Epoch 12800: <Test loss>: 3.2887979614315554e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.000119369026), np.complex128(0.00012065599629964875+0j)) <f>: (np.float32(-7.4374526e-05), np.complex128(0.00014689109707710676+0j))\n",
      "Epoch 14400: <Test loss>: 2.5505465600872412e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00012483551), np.complex128(0.00010875936079142678+0j)) <f>: (np.float32(-7.984108e-05), np.complex128(0.000148234091552478+0j))\n",
      "Epoch 16000: <Test loss>: 2.7062740628025495e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(9.01981e-05), np.complex128(0.00010953759541341207+0j)) <f>: (np.float32(-4.5203513e-05), np.complex128(0.00015544730915510987+0j))\n",
      "Epoch 17600: <Test loss>: 2.3343336579273455e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(0.00010755334), np.complex128(9.4733440534203e-05+0j)) <f>: (np.float32(-6.2558815e-05), np.complex128(0.0001702134613391066+0j))\n",
      "Epoch 19200: <Test loss>: 2.4586008294136263e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.5941808e-05), np.complex128(9.358028510149163e-05+0j)) <f>: (np.float32(-9.4732644e-07), np.complex128(0.0001567092396714052+0j))\n",
      "Epoch 20800: <Test loss>: 2.3777314709150232e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(6.574658e-05), np.complex128(0.00010236693956081147+0j)) <f>: (np.float32(-2.0752082e-05), np.complex128(0.00016780591567033083+0j))\n",
      "Epoch 22400: <Test loss>: 2.2300731870927848e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.5473174e-05), np.complex128(9.917795603849067e-05+0j)) <f>: (np.float32(-1.04787705e-05), np.complex128(0.00016639482873794239+0j))\n",
      "Epoch 24000: <Test loss>: 2.1945783373666927e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.3432166e-05), np.complex128(9.861746187465703e-05+0j)) <f>: (np.float32(1.5623569e-06), np.complex128(0.00016084903588038717+0j))\n",
      "Epoch 25600: <Test loss>: 2.2489701223094016e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(5.3492062e-05), np.complex128(0.00010079497745924752+0j)) <f>: (np.float32(-8.497496e-06), np.complex128(0.00016603137295898701+0j))\n",
      "Epoch 27200: <Test loss>: 2.451816362736281e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(9.6153024e-05), np.complex128(0.00011875246801648893+0j)) <f>: (np.float32(-5.115843e-05), np.complex128(0.0001745361588986327+0j))\n",
      "Epoch 28800: <Test loss>: 2.500281152606476e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(8.918586e-05), np.complex128(0.00012476950856650079+0j)) <f>: (np.float32(-4.419125e-05), np.complex128(0.00017171717240783356+0j))\n",
      "Epoch 30400: <Test loss>: 2.1758334696642123e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.156629e-05), np.complex128(0.00010208610971275692+0j)) <f>: (np.float32(3.4282962e-06), np.complex128(0.0001638840343529023+0j))\n",
      "Epoch 32000: <Test loss>: 2.3221455194288865e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.3826785e-05), np.complex128(0.00010688734945244992+0j)) <f>: (np.float32(1.1678092e-06), np.complex128(0.00016738629233600637+0j))\n",
      "Epoch 33600: <Test loss>: 2.4318644136656076e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.1504347e-05), np.complex128(0.00011834456343524683+0j)) <f>: (np.float32(1.3490208e-05), np.complex128(0.00017389652747193358+0j))\n",
      "Epoch 35200: <Test loss>: 2.2640966562903486e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.4223132e-05), np.complex128(0.00010900039603750538+0j)) <f>: (np.float32(1.0771551e-05), np.complex128(0.00016569853817083283+0j))\n",
      "Epoch 36800: <Test loss>: 2.208636396971997e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(4.0924027e-05), np.complex128(0.00010630194302732752+0j)) <f>: (np.float32(4.0704504e-06), np.complex128(0.00016485677014664923+0j))\n",
      "Epoch 38400: <Test loss>: 2.3512115149060264e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.6199723e-05), np.complex128(0.00010931972809460459+0j)) <f>: (np.float32(8.794881e-06), np.complex128(0.00016674756479148004+0j))\n",
      "Epoch 40000: <Test loss>: 2.9831882784492336e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(1.3628776e-05), np.complex128(0.00014624123751003128+0j)) <f>: (np.float32(3.1365813e-05), np.complex128(0.00018658357622510063+0j))\n",
      "Epoch 41600: <Test loss>: 2.409879925835412e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.2132564e-05), np.complex128(0.00011430921318996859+0j)) <f>: (np.float32(1.2862031e-05), np.complex128(0.00016831253369939132+0j))\n",
      "Epoch 43200: <Test loss>: 2.392015994701069e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(3.6471305e-05), np.complex128(0.00011313659257537815+0j)) <f>: (np.float32(8.523326e-06), np.complex128(0.00016855360066063386+0j))\n",
      "Epoch 44800: <Test loss>: 2.440181742713321e-05 <O>: (np.float32(4.4994566e-05), np.complex128(0.00018399458810332278+0j)) <O-f>: (np.float32(2.6664964e-05), np.complex128(0.00011357605374475818+0j)) <f>: (np.float32(1.8329521e-05), np.complex128(0.00016729836204393362+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e048fdf1",
   "metadata": {},
   "source": [
    "## m^2=0.01, lamda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9c2f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.006508924), np.complex128(9.908339041861463e-05+0j))\n",
      "bin size 1: (np.float32(0.006508924), np.complex128(9.908319722829053e-05+0j))\n",
      "jack bin size 2: (np.float32(0.006508924), np.complex128(0.00013985373033766635+0j))\n",
      "bin size 2: (np.float32(0.006508924), np.complex128(0.00013985509071961098+0j))\n",
      "jack bin size 4: (np.float32(0.006508924), np.complex128(0.00019723792845344844+0j))\n",
      "bin size 4: (np.float32(0.006508924), np.complex128(0.0001972376246166751+0j))\n",
      "jack bin size 5: (np.float32(0.006508924), np.complex128(0.000220239479233654+0j))\n",
      "bin size 5: (np.float32(0.006508924), np.complex128(0.00022023852871520623+0j))\n",
      "jack bin size 10: (np.float32(0.006508924), np.complex128(0.0003096374950672679+0j))\n",
      "bin size 10: (np.float32(0.006508924), np.complex128(0.0003096358886601406+0j))\n",
      "jack bin size 20: (np.float32(0.006508924), np.complex128(0.00043310780528221386+0j))\n",
      "bin size 20: (np.float32(0.006508924), np.complex128(0.0004331068174502681+0j))\n",
      "jack bin size 50: (np.float32(0.006508924), np.complex128(0.000663788582109225+0j))\n",
      "bin size 50: (np.float32(0.006508924), np.complex128(0.000663788641040016+0j))\n",
      "jack bin size 100: (np.float32(0.006508924), np.complex128(0.0008999902955784845+0j))\n",
      "bin size 100: (np.float32(0.006508924), np.complex128(0.0008999911423157231+0j))\n",
      "jack bin size 200: (np.float32(0.006508924), np.complex128(0.0011737471595393365+0j))\n",
      "bin size 200: (np.float32(0.006508924), np.complex128(0.0011737475313448311+0j))\n",
      "jack bin size 500: (np.float32(0.006508924), np.complex128(0.0015569514901460993+0j))\n",
      "bin size 500: (np.float32(0.006508924), np.complex128(0.0015569513302546214+0j))\n",
      "jack bin size 1000: (np.float32(0.006508924), np.complex128(0.0018180174748768866+0j))\n",
      "bin size 1000: (np.float32(0.006508924), np.complex128(0.0018180170361201341+0j))\n",
      "jack bin size 2000: (np.float32(0.006508924), np.complex128(0.0021222651994321495+0j))\n",
      "bin size 2000: (np.float32(0.006508924), np.complex128(0.00212226462151323+0j))\n",
      "jack bin size 5000: (np.float32(0.006508924), np.complex128(0.001876885896246824+0j))\n",
      "bin size 5000: (np.float32(0.006508924), np.complex128(0.0018768862567981612+0j))\n",
      "jack bin size 10000: (np.float32(0.006508924), np.complex128(0.0020088654127903283+0j))\n",
      "bin size 10000: (np.float32(0.006508924), np.complex128(0.002008865897854169+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar3d/config/c_32x8x8_0.01_0.1_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(32,8,8), m2=0.01, lamda=0.1)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "691ce4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aec3e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 7.971414015628397e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008530789), np.complex128(0.00037087998168622695+0j)) <f>: (np.float32(-0.002569545), np.complex128(0.0012972759247803025+0j))\n",
      "Epoch 200: <Test loss>: 7.236925739562139e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008154386), np.complex128(0.0003491752969346361+0j)) <f>: (np.float32(-0.0021931345), np.complex128(0.0012948678558113214+0j))\n",
      "Epoch 300: <Test loss>: 7.03048353898339e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0082544815), np.complex128(0.0003393744866774405+0j)) <f>: (np.float32(-0.0022932312), np.complex128(0.0013303726012800113+0j))\n",
      "Epoch 400: <Test loss>: 7.32821790734306e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00818151), np.complex128(0.00037067110561640146+0j)) <f>: (np.float32(-0.0022202604), np.complex128(0.0012902014134472397+0j))\n",
      "Epoch 500: <Test loss>: 7.510414434364066e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008277501), np.complex128(0.0003447059325993876+0j)) <f>: (np.float32(-0.0023162544), np.complex128(0.0013288838914838363+0j))\n",
      "Epoch 600: <Test loss>: 7.633113273186609e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008292582), np.complex128(0.00036394263843728125+0j)) <f>: (np.float32(-0.0023313363), np.complex128(0.001306020049204755+0j))\n",
      "Epoch 700: <Test loss>: 7.90935882832855e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008279663), np.complex128(0.00038164499635806484+0j)) <f>: (np.float32(-0.0023184137), np.complex128(0.0013051266964663943+0j))\n",
      "Epoch 800: <Test loss>: 7.78753383201547e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008242694), np.complex128(0.0003413097142669761+0j)) <f>: (np.float32(-0.0022814446), np.complex128(0.0013307525489442246+0j))\n",
      "Epoch 900: <Test loss>: 7.421262125717476e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008413257), np.complex128(0.0003368088250587536+0j)) <f>: (np.float32(-0.0024520094), np.complex128(0.0013489968873018816+0j))\n",
      "Epoch 1000: <Test loss>: 7.769261719658971e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008347603), np.complex128(0.0003502511704415735+0j)) <f>: (np.float32(-0.0023863507), np.complex128(0.0013296884417631155+0j))\n",
      "Epoch 1100: <Test loss>: 8.404438267461956e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008352495), np.complex128(0.0003831567313632731+0j)) <f>: (np.float32(-0.0023912417), np.complex128(0.0012959568276809733+0j))\n",
      "Epoch 1200: <Test loss>: 7.874057337176055e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00833298), np.complex128(0.0003443894152630896+0j)) <f>: (np.float32(-0.002371734), np.complex128(0.0013267898026380364+0j))\n",
      "Epoch 1300: <Test loss>: 7.920690404716879e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008339932), np.complex128(0.0003381993447073155+0j)) <f>: (np.float32(-0.0023786833), np.complex128(0.0013272000699989935+0j))\n",
      "Epoch 1400: <Test loss>: 8.46001275931485e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008419359), np.complex128(0.0003635278675230423+0j)) <f>: (np.float32(-0.0024581063), np.complex128(0.0013169627956554119+0j))\n",
      "Epoch 1500: <Test loss>: 8.627610077382997e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0083983075), np.complex128(0.00036370122260923516+0j)) <f>: (np.float32(-0.0024370558), np.complex128(0.0013093059939120854+0j))\n",
      "Epoch 1600: <Test loss>: 8.488438470521942e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008339418), np.complex128(0.00036969595947019374+0j)) <f>: (np.float32(-0.0023781678), np.complex128(0.0013044235078511239+0j))\n",
      "Epoch 1700: <Test loss>: 8.57809791341424e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008412767), np.complex128(0.00036450624861597374+0j)) <f>: (np.float32(-0.0024515267), np.complex128(0.0013170876265407494+0j))\n",
      "Epoch 1800: <Test loss>: 9.003612649394199e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008430688), np.complex128(0.0003721890567937454+0j)) <f>: (np.float32(-0.0024694402), np.complex128(0.0013367109402272876+0j))\n",
      "Epoch 1900: <Test loss>: 9.110160317504779e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008399796), np.complex128(0.00037503295554583335+0j)) <f>: (np.float32(-0.002438538), np.complex128(0.0013048675201465319+0j))\n",
      "Epoch 2000: <Test loss>: 9.073637920664623e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008405206), np.complex128(0.00036157199336177027+0j)) <f>: (np.float32(-0.0024439641), np.complex128(0.0013192967780013877+0j))\n",
      "Epoch 2100: <Test loss>: 9.424261224921793e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008353584), np.complex128(0.0003952804236590593+0j)) <f>: (np.float32(-0.002392337), np.complex128(0.0012774687900034653+0j))\n",
      "Epoch 2200: <Test loss>: 9.512876567896456e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008409812), np.complex128(0.0003904939711145616+0j)) <f>: (np.float32(-0.002448563), np.complex128(0.0013021537169969984+0j))\n",
      "Epoch 2300: <Test loss>: 9.706695709610358e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00843322), np.complex128(0.0003861750000868005+0j)) <f>: (np.float32(-0.00247198), np.complex128(0.0013077334292224065+0j))\n",
      "Epoch 2400: <Test loss>: 9.823532309383154e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008468903), np.complex128(0.00036087603580388226+0j)) <f>: (np.float32(-0.0025076487), np.complex128(0.0013191227251815878+0j))\n",
      "Epoch 2500: <Test loss>: 9.768015297595412e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008381305), np.complex128(0.0003907144549343956+0j)) <f>: (np.float32(-0.0024200564), np.complex128(0.0012991875880030174+0j))\n",
      "Epoch 2600: <Test loss>: 9.717185457702726e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008456849), np.complex128(0.0003709285693174102+0j)) <f>: (np.float32(-0.002495601), np.complex128(0.0013436589714864871+0j))\n",
      "Epoch 2700: <Test loss>: 9.844842861639336e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008398013), np.complex128(0.00037372667137274316+0j)) <f>: (np.float32(-0.0024367643), np.complex128(0.0013209429218714488+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d98b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ab644cf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.0001447384274797514 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008786411), np.complex128(0.00044434980818279+0j)) <f>: (np.float32(-0.0028251628), np.complex128(0.0015478659348741165+0j))\n",
      "Epoch 400: <Test loss>: 0.00012253581371624023 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008454795), np.complex128(0.0004265236659770794+0j)) <f>: (np.float32(-0.0024935436), np.complex128(0.0012823394780234348+0j))\n",
      "Epoch 600: <Test loss>: 0.00012805138248950243 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008326348), np.complex128(0.00048825548828674594+0j)) <f>: (np.float32(-0.0023650965), np.complex128(0.0012284597277215823+0j))\n",
      "Epoch 800: <Test loss>: 0.0001068311685230583 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008314172), np.complex128(0.0004266242664771533+0j)) <f>: (np.float32(-0.0023529215), np.complex128(0.0012723001063029494+0j))\n",
      "Epoch 1000: <Test loss>: 0.00011497491504997015 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008256621), np.complex128(0.0004691879828485666+0j)) <f>: (np.float32(-0.0022953728), np.complex128(0.00124403212694613+0j))\n",
      "Epoch 1200: <Test loss>: 0.00011621130397543311 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0081963595), np.complex128(0.00043300516717446094+0j)) <f>: (np.float32(-0.0022351143), np.complex128(0.001270457582137662+0j))\n",
      "Epoch 1400: <Test loss>: 0.00011651148815872148 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008197123), np.complex128(0.00045964225765508294+0j)) <f>: (np.float32(-0.0022358727), np.complex128(0.0012627275817959213+0j))\n",
      "Epoch 1600: <Test loss>: 0.0001274899987038225 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008245327), np.complex128(0.00046830487410816417+0j)) <f>: (np.float32(-0.0022840775), np.complex128(0.0012859447310014916+0j))\n",
      "Epoch 1800: <Test loss>: 0.000138016403070651 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008205992), np.complex128(0.0004714409646657945+0j)) <f>: (np.float32(-0.002244735), np.complex128(0.0012555796181431325+0j))\n",
      "Epoch 2000: <Test loss>: 0.00011404583347029984 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008139035), np.complex128(0.00048470110643167736+0j)) <f>: (np.float32(-0.0021777856), np.complex128(0.0012781412783400246+0j))\n",
      "Epoch 2200: <Test loss>: 0.00013291985669638962 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008301379), np.complex128(0.0005055695257400314+0j)) <f>: (np.float32(-0.0023401289), np.complex128(0.0013964663686646015+0j))\n",
      "Epoch 2400: <Test loss>: 0.00012188521941425279 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0082443645), np.complex128(0.0004897443883739046+0j)) <f>: (np.float32(-0.002283116), np.complex128(0.0012993915799375934+0j))\n",
      "Epoch 2600: <Test loss>: 0.00012690061703324318 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008174434), np.complex128(0.0005106794095265698+0j)) <f>: (np.float32(-0.0022131754), np.complex128(0.0013016398044802277+0j))\n",
      "Epoch 2800: <Test loss>: 0.00013019845937378705 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008262205), np.complex128(0.0005218930034774255+0j)) <f>: (np.float32(-0.0023009493), np.complex128(0.0012960623757466245+0j))\n",
      "Epoch 3000: <Test loss>: 0.0001288742496399209 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008255862), np.complex128(0.0005159515480919159+0j)) <f>: (np.float32(-0.0022946126), np.complex128(0.0013020248265706743+0j))\n",
      "Epoch 3200: <Test loss>: 0.0001307543134316802 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008203294), np.complex128(0.0005261017643104336+0j)) <f>: (np.float32(-0.002242042), np.complex128(0.001298110287313702+0j))\n",
      "Epoch 3400: <Test loss>: 0.00012937489373143762 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008209468), np.complex128(0.000524972767903867+0j)) <f>: (np.float32(-0.002248214), np.complex128(0.001286872082395615+0j))\n",
      "Epoch 3600: <Test loss>: 0.0001321701129199937 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008282805), np.complex128(0.000529541495847795+0j)) <f>: (np.float32(-0.002321554), np.complex128(0.0012982590948629916+0j))\n",
      "Epoch 3800: <Test loss>: 0.00013854748976882547 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008291936), np.complex128(0.0005321411878374086+0j)) <f>: (np.float32(-0.0023306862), np.complex128(0.0012903559297260416+0j))\n",
      "Epoch 4000: <Test loss>: 0.0001388612436130643 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008126972), np.complex128(0.0005563306605395942+0j)) <f>: (np.float32(-0.0021657206), np.complex128(0.0012439032365198059+0j))\n",
      "Epoch 4200: <Test loss>: 0.0001423757494194433 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008248144), np.complex128(0.0005400524720563316+0j)) <f>: (np.float32(-0.0022868875), np.complex128(0.0012863685724526224+0j))\n",
      "Epoch 4600: <Test loss>: 0.0001407317176926881 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008288164), np.complex128(0.0005492761931901527+0j)) <f>: (np.float32(-0.0023269192), np.complex128(0.001310372892027623+0j))\n",
      "Epoch 4800: <Test loss>: 0.0001450472482247278 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008259338), np.complex128(0.0005588674296439156+0j)) <f>: (np.float32(-0.002298088), np.complex128(0.001274792410747402+0j))\n",
      "Epoch 5000: <Test loss>: 0.00014574344095308334 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008265809), np.complex128(0.0005529323807215255+0j)) <f>: (np.float32(-0.0023045603), np.complex128(0.0012810033815962244+0j))\n",
      "Epoch 5200: <Test loss>: 0.0001505279797129333 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008362604), np.complex128(0.0005601764413211062+0j)) <f>: (np.float32(-0.0024013473), np.complex128(0.001295388618803507+0j))\n",
      "Epoch 5400: <Test loss>: 0.00015389506006613374 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008268744), np.complex128(0.0005838987496581948+0j)) <f>: (np.float32(-0.002307493), np.complex128(0.0012527206864033286+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a22f4eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b3c65dd1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.00019380707817617804 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00794486), np.complex128(0.0006152863594025504+0j)) <f>: (np.float32(-0.0019836135), np.complex128(0.0014452196798643295+0j))\n",
      "Epoch 800: <Test loss>: 0.0001648902107262984 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007993109), np.complex128(0.0005915510478482392+0j)) <f>: (np.float32(-0.0020318576), np.complex128(0.0012331753920201264+0j))\n",
      "Epoch 1200: <Test loss>: 0.000167268022778444 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008109554), np.complex128(0.0005611775621865953+0j)) <f>: (np.float32(-0.0021483044), np.complex128(0.0011909801424235737+0j))\n",
      "Epoch 1600: <Test loss>: 0.00020231760572642088 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008331501), np.complex128(0.000620960075373929+0j)) <f>: (np.float32(-0.002370256), np.complex128(0.001305102846663098+0j))\n",
      "Epoch 2000: <Test loss>: 0.00016020171460695565 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008410858), np.complex128(0.0005733189515438699+0j)) <f>: (np.float32(-0.0024496056), np.complex128(0.0012806704992353241+0j))\n",
      "Epoch 2400: <Test loss>: 0.00015160883776843548 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0081766965), np.complex128(0.0005618858252280989+0j)) <f>: (np.float32(-0.0022154478), np.complex128(0.0012414451844524274+0j))\n",
      "Epoch 2800: <Test loss>: 0.00016865570796653628 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008190869), np.complex128(0.0005505911673181673+0j)) <f>: (np.float32(-0.0022296219), np.complex128(0.0012740922667878716+0j))\n",
      "Epoch 3200: <Test loss>: 0.000171347739524208 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008150419), np.complex128(0.0006190255138028366+0j)) <f>: (np.float32(-0.0021891722), np.complex128(0.001327147295966168+0j))\n",
      "Epoch 3600: <Test loss>: 0.00018206956156063825 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007955834), np.complex128(0.0005959562206916373+0j)) <f>: (np.float32(-0.0019945875), np.complex128(0.0012388394665816618+0j))\n",
      "Epoch 4000: <Test loss>: 0.00014973215002100915 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007922384), np.complex128(0.0005480584577548324+0j)) <f>: (np.float32(-0.001961135), np.complex128(0.0012342081646192452+0j))\n",
      "Epoch 4400: <Test loss>: 0.0001546221465105191 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007958875), np.complex128(0.000558618211885536+0j)) <f>: (np.float32(-0.0019976208), np.complex128(0.0012137395783829062+0j))\n",
      "Epoch 4800: <Test loss>: 0.00015123063349165022 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007949367), np.complex128(0.0005395793452404105+0j)) <f>: (np.float32(-0.0019881148), np.complex128(0.0012339696665862833+0j))\n",
      "Epoch 5200: <Test loss>: 0.00016461823543068022 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007870303), np.complex128(0.0005685468342534811+0j)) <f>: (np.float32(-0.0019090582), np.complex128(0.001175695843748418+0j))\n",
      "Epoch 5600: <Test loss>: 0.0001838535681599751 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0076865666), np.complex128(0.0005989631988167958+0j)) <f>: (np.float32(-0.0017253209), np.complex128(0.0011944962123605813+0j))\n",
      "Epoch 6000: <Test loss>: 0.000168643455253914 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0077904435), np.complex128(0.0005701842247382898+0j)) <f>: (np.float32(-0.001829196), np.complex128(0.0011719790802538859+0j))\n",
      "Epoch 6400: <Test loss>: 0.00017379139899276197 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007701134), np.complex128(0.0005615753337729529+0j)) <f>: (np.float32(-0.0017398861), np.complex128(0.0011569288394887378+0j))\n",
      "Epoch 6800: <Test loss>: 0.00018818193348124623 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0075909165), np.complex128(0.0005967051425733346+0j)) <f>: (np.float32(-0.0016296674), np.complex128(0.0011182296158455717+0j))\n",
      "Epoch 7200: <Test loss>: 0.00019136632909066975 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007638666), np.complex128(0.0006033070979634276+0j)) <f>: (np.float32(-0.0016774158), np.complex128(0.0010979527160601928+0j))\n",
      "Epoch 7600: <Test loss>: 0.0001789851812645793 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007870096), np.complex128(0.000551646584544352+0j)) <f>: (np.float32(-0.0019088429), np.complex128(0.0011448567794798748+0j))\n",
      "Epoch 8000: <Test loss>: 0.0002008912997553125 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0076408316), np.complex128(0.0006178247142650697+0j)) <f>: (np.float32(-0.0016795802), np.complex128(0.001093605835688149+0j))\n",
      "Epoch 8400: <Test loss>: 0.00020440740627236664 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0075070495), np.complex128(0.0006176726083387286+0j)) <f>: (np.float32(-0.0015458044), np.complex128(0.0010748966798875279+0j))\n",
      "Epoch 8800: <Test loss>: 0.00021276948973536491 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0074187084), np.complex128(0.0006231642792689899+0j)) <f>: (np.float32(-0.0014574679), np.complex128(0.0010532515244986856+0j))\n",
      "Epoch 9200: <Test loss>: 0.000224521936615929 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007413881), np.complex128(0.0006295008421670847+0j)) <f>: (np.float32(-0.0014526279), np.complex128(0.0010433246147495943+0j))\n",
      "Epoch 9600: <Test loss>: 0.00022521858045365661 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007410769), np.complex128(0.0006367699577461921+0j)) <f>: (np.float32(-0.0014495158), np.complex128(0.0010396761657182271+0j))\n",
      "Epoch 10000: <Test loss>: 0.00022023773635737598 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007452713), np.complex128(0.0006210764065953259+0j)) <f>: (np.float32(-0.0014914604), np.complex128(0.0010462627709689645+0j))\n",
      "Epoch 10400: <Test loss>: 0.00024151895195245743 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0072986125), np.complex128(0.0006645326511108371+0j)) <f>: (np.float32(-0.0013373601), np.complex128(0.0010081350545803162+0j))\n",
      "Epoch 10800: <Test loss>: 0.00024948149803094566 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0073093353), np.complex128(0.0006709580798983415+0j)) <f>: (np.float32(-0.0013480858), np.complex128(0.0009984755036633883+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44e69d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe1d373e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.00037289317697286606 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007971172), np.complex128(0.0007047917533780995+0j)) <f>: (np.float32(-0.0020099245), np.complex128(0.001090831900587752+0j))\n",
      "Epoch 1600: <Test loss>: 0.0010305542964488268 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00858693), np.complex128(0.0009094510384340939+0j)) <f>: (np.float32(-0.0026256861), np.complex128(0.0018075352352450885+0j))\n",
      "Epoch 2400: <Test loss>: 0.000349764886777848 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0080245845), np.complex128(0.0007049349156282046+0j)) <f>: (np.float32(-0.002063333), np.complex128(0.0010365675722077482+0j))\n",
      "Epoch 3200: <Test loss>: 0.0003574638394638896 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007941385), np.complex128(0.0006643276442910146+0j)) <f>: (np.float32(-0.0019801357), np.complex128(0.001072225882500321+0j))\n",
      "Epoch 4000: <Test loss>: 0.0002947541943285614 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00804444), np.complex128(0.0005963819016222777+0j)) <f>: (np.float32(-0.0020831882), np.complex128(0.0011207980366835229+0j))\n",
      "Epoch 4800: <Test loss>: 0.0003283368714619428 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007861217), np.complex128(0.0007129162903594742+0j)) <f>: (np.float32(-0.0018999645), np.complex128(0.0010449636544229287+0j))\n",
      "Epoch 5600: <Test loss>: 0.0005217808065935969 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007510832), np.complex128(0.0009924496859417512+0j)) <f>: (np.float32(-0.0015495841), np.complex128(0.0008474215058803564+0j))\n",
      "Epoch 6400: <Test loss>: 0.0002796045155264437 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007856295), np.complex128(0.0006804454809053069+0j)) <f>: (np.float32(-0.00189505), np.complex128(0.0010524316875103789+0j))\n",
      "Epoch 7200: <Test loss>: 0.0003352921921759844 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00782081), np.complex128(0.0007107724721365888+0j)) <f>: (np.float32(-0.0018595605), np.complex128(0.001007450514481453+0j))\n",
      "Epoch 8000: <Test loss>: 0.00028115787426941097 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007883832), np.complex128(0.0006862666089587611+0j)) <f>: (np.float32(-0.0019225805), np.complex128(0.0010397311598125299+0j))\n",
      "Epoch 8800: <Test loss>: 0.0003290472668595612 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007698158), np.complex128(0.0007796894599863666+0j)) <f>: (np.float32(-0.0017369101), np.complex128(0.0009379826976640577+0j))\n",
      "Epoch 9600: <Test loss>: 0.0003959405585192144 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0075398), np.complex128(0.0008933326943771782+0j)) <f>: (np.float32(-0.0015785438), np.complex128(0.0008630580965967545+0j))\n",
      "Epoch 10400: <Test loss>: 0.00026569905458018184 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0080992635), np.complex128(0.0006526843098685318+0j)) <f>: (np.float32(-0.0021380128), np.complex128(0.0011191408559364045+0j))\n",
      "Epoch 11200: <Test loss>: 0.00043100325274281204 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007411814), np.complex128(0.0009396261140300183+0j)) <f>: (np.float32(-0.0014505609), np.complex128(0.0008190966295193968+0j))\n",
      "Epoch 12000: <Test loss>: 0.0004175778303761035 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007391152), np.complex128(0.0009439898668692876+0j)) <f>: (np.float32(-0.0014298992), np.complex128(0.0008162855242468412+0j))\n",
      "Epoch 12800: <Test loss>: 0.0003121016197837889 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007663832), np.complex128(0.0008108570299231836+0j)) <f>: (np.float32(-0.00170258), np.complex128(0.0009424396296550348+0j))\n",
      "Epoch 13600: <Test loss>: 0.0005228389636613429 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0072284695), np.complex128(0.0010788762986609416+0j)) <f>: (np.float32(-0.001267223), np.complex128(0.0007339854983708444+0j))\n",
      "Epoch 14400: <Test loss>: 0.0004815652791876346 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0072405883), np.complex128(0.001034245768484732+0j)) <f>: (np.float32(-0.0012793419), np.complex128(0.0007399911452482045+0j))\n",
      "Epoch 15200: <Test loss>: 0.0005678581655956805 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0071317446), np.complex128(0.0011290943429928925+0j)) <f>: (np.float32(-0.0011704968), np.complex128(0.000703212718794973+0j))\n",
      "Epoch 16000: <Test loss>: 0.0006898688152432442 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0070119393), np.complex128(0.0012495452373272268+0j)) <f>: (np.float32(-0.0010506923), np.complex128(0.0006734954198756142+0j))\n",
      "Epoch 16800: <Test loss>: 0.0005858769873157144 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0070904572), np.complex128(0.0011622628226238016+0j)) <f>: (np.float32(-0.0011292029), np.complex128(0.0006709500876770242+0j))\n",
      "Epoch 17600: <Test loss>: 0.0005263282801024616 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007154579), np.complex128(0.0010963329592065447+0j)) <f>: (np.float32(-0.001193326), np.complex128(0.0006973824567742991+0j))\n",
      "Epoch 18400: <Test loss>: 0.000619603437371552 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007071632), np.complex128(0.0011908386927923223+0j)) <f>: (np.float32(-0.0011103831), np.complex128(0.0006663111740752577+0j))\n",
      "Epoch 19200: <Test loss>: 0.0006179242627695203 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007074487), np.complex128(0.0011952838901726351+0j)) <f>: (np.float32(-0.0011132371), np.complex128(0.0006650746632628745+0j))\n",
      "Epoch 20000: <Test loss>: 0.0006752894842065871 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0070080822), np.complex128(0.0012508653493118024+0j)) <f>: (np.float32(-0.0010468374), np.complex128(0.0006484868713490567+0j))\n",
      "Epoch 20800: <Test loss>: 0.0007321644225157797 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0069121155), np.complex128(0.0013055511722208043+0j)) <f>: (np.float32(-0.0009508613), np.complex128(0.0006245509930978769+0j))\n",
      "Epoch 21600: <Test loss>: 0.0005749926785938442 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00712443), np.complex128(0.0011604896912372539+0j)) <f>: (np.float32(-0.0011631842), np.complex128(0.0006857869488190646+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3e95ce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8aa19d8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0007773259421810508 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006889643), np.complex128(0.0012443601886021085+0j)) <f>: (np.float32(-0.0009283946), np.complex128(0.000653781337389829+0j))\n",
      "Epoch 3200: <Test loss>: 0.0006261547096073627 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007762565), np.complex128(0.0009961672740305461+0j)) <f>: (np.float32(-0.0018013142), np.complex128(0.0009006421516448397+0j))\n",
      "Epoch 4800: <Test loss>: 0.0005258755409158766 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007478236), np.complex128(0.0009901959429605882+0j)) <f>: (np.float32(-0.0015169834), np.complex128(0.0007918660531756343+0j))\n",
      "Epoch 6400: <Test loss>: 0.0006404364248737693 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006982302), np.complex128(0.0011579939615550938+0j)) <f>: (np.float32(-0.001021058), np.complex128(0.0006830377515465544+0j))\n",
      "Epoch 8000: <Test loss>: 0.0028842499013990164 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0053241523), np.complex128(0.002159611613372428+0j)) <f>: (np.float32(0.0006370981), np.complex128(0.001268833258300404+0j))\n",
      "Epoch 9600: <Test loss>: 0.0005736903403885663 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0070865853), np.complex128(0.0011432264931917928+0j)) <f>: (np.float32(-0.0011253314), np.complex128(0.0007639908344092671+0j))\n",
      "Epoch 11200: <Test loss>: 0.0004979310324415565 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007103482), np.complex128(0.0010227726810623744+0j)) <f>: (np.float32(-0.0011422302), np.complex128(0.0008402796315390474+0j))\n",
      "Epoch 12800: <Test loss>: 0.000533769023604691 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0069948346), np.complex128(0.0010639537431458874+0j)) <f>: (np.float32(-0.0010335813), np.complex128(0.0008185129436419191+0j))\n",
      "Epoch 14400: <Test loss>: 0.0007988620200194418 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0066678873), np.complex128(0.0012365748770144255+0j)) <f>: (np.float32(-0.0007066404), np.complex128(0.0008265795053035792+0j))\n",
      "Epoch 16000: <Test loss>: 0.0005282644415274262 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006869767), np.complex128(0.0010704150734989917+0j)) <f>: (np.float32(-0.0009085197), np.complex128(0.000784273633215142+0j))\n",
      "Epoch 17600: <Test loss>: 0.0005092557985335588 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006848638), np.complex128(0.0010850843517146796+0j)) <f>: (np.float32(-0.00088739296), np.complex128(0.0007802137750769155+0j))\n",
      "Epoch 19200: <Test loss>: 0.0004223502182867378 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0070993877), np.complex128(0.0009682864733953225+0j)) <f>: (np.float32(-0.0011381385), np.complex128(0.000864259149855833+0j))\n",
      "Epoch 20800: <Test loss>: 0.0004858180182054639 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007027412), np.complex128(0.0010577963074938257+0j)) <f>: (np.float32(-0.0010661605), np.complex128(0.0007867354911021954+0j))\n",
      "Epoch 22400: <Test loss>: 0.00047427735989913344 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007029051), np.complex128(0.001011262613758842+0j)) <f>: (np.float32(-0.0010677995), np.complex128(0.0008407020140926362+0j))\n",
      "Epoch 24000: <Test loss>: 0.0003665108233690262 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0072974362), np.complex128(0.0009162285055415288+0j)) <f>: (np.float32(-0.0013361851), np.complex128(0.0008440602059434624+0j))\n",
      "Epoch 25600: <Test loss>: 0.0004842315975110978 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0068775197), np.complex128(0.0010126892886943156+0j)) <f>: (np.float32(-0.0009162692), np.complex128(0.0008039265054344888+0j))\n",
      "Epoch 27200: <Test loss>: 0.0005127509357407689 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0070519485), np.complex128(0.0011138932649079618+0j)) <f>: (np.float32(-0.0010907002), np.complex128(0.0008035532179547065+0j))\n",
      "Epoch 28800: <Test loss>: 0.000610628048889339 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006791902), np.complex128(0.0011946848541558018+0j)) <f>: (np.float32(-0.0008306474), np.complex128(0.0007359123214419313+0j))\n",
      "Epoch 30400: <Test loss>: 0.000508184835780412 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007117667), np.complex128(0.0011033795611953248+0j)) <f>: (np.float32(-0.0011564217), np.complex128(0.0007726074638746098+0j))\n",
      "Epoch 32000: <Test loss>: 0.0005742477951571345 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007039017), np.complex128(0.0011080739129036893+0j)) <f>: (np.float32(-0.0010777675), np.complex128(0.0007954032454118377+0j))\n",
      "Epoch 33600: <Test loss>: 0.0006803003489039838 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006667638), np.complex128(0.001201905382103726+0j)) <f>: (np.float32(-0.0007063892), np.complex128(0.0007232861975187269+0j))\n",
      "Epoch 35200: <Test loss>: 0.0006955084973014891 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0068510594), np.complex128(0.0012461424539558761+0j)) <f>: (np.float32(-0.0008898088), np.complex128(0.0007390684242680189+0j))\n",
      "Epoch 36800: <Test loss>: 0.0006306397262960672 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006936546), np.complex128(0.001164898606469999+0j)) <f>: (np.float32(-0.0009752932), np.complex128(0.0007558838041983972+0j))\n",
      "Epoch 38400: <Test loss>: 0.0005190252559259534 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0069225496), np.complex128(0.0010619983764272385+0j)) <f>: (np.float32(-0.0009613007), np.complex128(0.0007294263801215956+0j))\n",
      "Epoch 40000: <Test loss>: 0.0006259876536205411 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0068311417), np.complex128(0.0010965894714526348+0j)) <f>: (np.float32(-0.000869895), np.complex128(0.0007681460283303506+0j))\n",
      "Epoch 41600: <Test loss>: 0.0007001679041422904 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0069504585), np.complex128(0.001288536240478804+0j)) <f>: (np.float32(-0.0009892118), np.complex128(0.0007415676426182143+0j))\n",
      "Epoch 43200: <Test loss>: 0.0007257236866280437 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006798499), np.complex128(0.0012490362723760334+0j)) <f>: (np.float32(-0.0008372502), np.complex128(0.0007607895691897037+0j))\n",
      "Epoch 44800: <Test loss>: 0.0007505029789172113 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.006781904), np.complex128(0.0012861837364770768+0j)) <f>: (np.float32(-0.00082065735), np.complex128(0.000750514997813308+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c4211",
   "metadata": {},
   "source": [
    "### l4, w64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3e24721",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2b5a8e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.00010233582725049928 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008552828), np.complex128(0.0004144443114805061+0j)) <f>: (np.float32(-0.0025915867), np.complex128(0.0014118748639218156+0j))\n",
      "Epoch 200: <Test loss>: 0.00010019226465374231 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008451864), np.complex128(0.000377807334658526+0j)) <f>: (np.float32(-0.00249061), np.complex128(0.001339158589720888+0j))\n",
      "Epoch 300: <Test loss>: 9.757267980603501e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008166841), np.complex128(0.0003644392344745311+0j)) <f>: (np.float32(-0.002205589), np.complex128(0.001297996112723454+0j))\n",
      "Epoch 400: <Test loss>: 8.772227738518268e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008231287), np.complex128(0.00036024243025833513+0j)) <f>: (np.float32(-0.002270037), np.complex128(0.0013169990778029795+0j))\n",
      "Epoch 500: <Test loss>: 8.976919343695045e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00815421), np.complex128(0.00036963731813203593+0j)) <f>: (np.float32(-0.0021929643), np.complex128(0.0013218984363311667+0j))\n",
      "Epoch 600: <Test loss>: 9.736530773807317e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008177154), np.complex128(0.0004136741087237931+0j)) <f>: (np.float32(-0.0022159042), np.complex128(0.0012495199920567164+0j))\n",
      "Epoch 700: <Test loss>: 9.68289896263741e-05 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008159534), np.complex128(0.00040667993190103514+0j)) <f>: (np.float32(-0.0021982805), np.complex128(0.0012612185742948135+0j))\n",
      "Epoch 800: <Test loss>: 0.00010796507558552548 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008123607), np.complex128(0.0004386416178281893+0j)) <f>: (np.float32(-0.0021623643), np.complex128(0.001235989415087766+0j))\n",
      "Epoch 900: <Test loss>: 0.00010962310625473037 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008157935), np.complex128(0.0004418944836195119+0j)) <f>: (np.float32(-0.0021966884), np.complex128(0.0012275208320077782+0j))\n",
      "Epoch 1000: <Test loss>: 0.00011216735583730042 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008155555), np.complex128(0.0004344209638086971+0j)) <f>: (np.float32(-0.002194305), np.complex128(0.00121855444771431+0j))\n",
      "Epoch 1100: <Test loss>: 0.00011853580508613959 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008089425), np.complex128(0.00046260106044618965+0j)) <f>: (np.float32(-0.0021281717), np.complex128(0.0011976469236087961+0j))\n",
      "Epoch 1200: <Test loss>: 0.00011971381172770634 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008129684), np.complex128(0.00044622369864523133+0j)) <f>: (np.float32(-0.0021684333), np.complex128(0.001214752180137748+0j))\n",
      "Epoch 1300: <Test loss>: 0.00012632174184545875 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008025239), np.complex128(0.0004671840919290626+0j)) <f>: (np.float32(-0.0020639875), np.complex128(0.0011909909255793194+0j))\n",
      "Epoch 1400: <Test loss>: 0.0001254111557500437 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008057556), np.complex128(0.0004590095718492906+0j)) <f>: (np.float32(-0.002096309), np.complex128(0.001200964202898117+0j))\n",
      "Epoch 1500: <Test loss>: 0.00012684777902904898 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008089501), np.complex128(0.00046147615529577363+0j)) <f>: (np.float32(-0.002128258), np.complex128(0.0012192715910017218+0j))\n",
      "Epoch 1600: <Test loss>: 0.00013174030755180866 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00798966), np.complex128(0.000476859785864445+0j)) <f>: (np.float32(-0.0020284082), np.complex128(0.0011869688084862025+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9750af12",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d82a39c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.00027715787291526794 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008655073), np.complex128(0.0005660483136368926+0j)) <f>: (np.float32(-0.002693829), np.complex128(0.0013255954095633892+0j))\n",
      "Epoch 400: <Test loss>: 0.00013761958689428866 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008002424), np.complex128(0.00038976106539066285+0j)) <f>: (np.float32(-0.00204117), np.complex128(0.0013247039597348658+0j))\n",
      "Epoch 600: <Test loss>: 0.00013574377226177603 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008084832), np.complex128(0.00040592479384720206+0j)) <f>: (np.float32(-0.0021235796), np.complex128(0.0013600100414165245+0j))\n",
      "Epoch 800: <Test loss>: 0.00012185702507849783 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007977488), np.complex128(0.00045072036973664743+0j)) <f>: (np.float32(-0.0020162365), np.complex128(0.0012722227213028926+0j))\n",
      "Epoch 1000: <Test loss>: 0.00013782830501440912 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007923045), np.complex128(0.0005028883892093743+0j)) <f>: (np.float32(-0.0019617996), np.complex128(0.0011884419144217102+0j))\n",
      "Epoch 1200: <Test loss>: 0.00013227234012447298 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007943792), np.complex128(0.0004648103387674838+0j)) <f>: (np.float32(-0.0019825504), np.complex128(0.0012293811166648816+0j))\n",
      "Epoch 1400: <Test loss>: 0.00014228714280761778 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007844254), np.complex128(0.0004972637048814743+0j)) <f>: (np.float32(-0.0018830086), np.complex128(0.001194896204008416+0j))\n",
      "Epoch 1600: <Test loss>: 0.0001518924837000668 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007839711), np.complex128(0.0005052423521086436+0j)) <f>: (np.float32(-0.0018784679), np.complex128(0.0011565524439228877+0j))\n",
      "Epoch 1800: <Test loss>: 0.00015988080122042447 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0077141398), np.complex128(0.0005376480820463696+0j)) <f>: (np.float32(-0.0017528946), np.complex128(0.0011125659218660035+0j))\n",
      "Epoch 2000: <Test loss>: 0.00015586311928927898 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0077877794), np.complex128(0.0005234684859621888+0j)) <f>: (np.float32(-0.0018265327), np.complex128(0.0011433775842328872+0j))\n",
      "Epoch 2200: <Test loss>: 0.00016017664165701717 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00774422), np.complex128(0.0005272866745510576+0j)) <f>: (np.float32(-0.0017829722), np.complex128(0.0011444222817336542+0j))\n",
      "Epoch 2400: <Test loss>: 0.00016529513231944293 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0077103646), np.complex128(0.0005400847580932406+0j)) <f>: (np.float32(-0.001749117), np.complex128(0.0011242254310221047+0j))\n",
      "Epoch 2600: <Test loss>: 0.00016975612379610538 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0076858187), np.complex128(0.00055172606274523+0j)) <f>: (np.float32(-0.0017245689), np.complex128(0.0010956056670666664+0j))\n",
      "Epoch 2800: <Test loss>: 0.00017071602633222938 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0077084117), np.complex128(0.000557433269929748+0j)) <f>: (np.float32(-0.0017471663), np.complex128(0.001097906285060159+0j))\n",
      "Epoch 3000: <Test loss>: 0.0001852974237408489 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0075742425), np.complex128(0.0005996119007803868+0j)) <f>: (np.float32(-0.001612999), np.complex128(0.0010402825596530986+0j))\n",
      "Epoch 3200: <Test loss>: 0.00017921358812600374 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007634393), np.complex128(0.0005797227505895551+0j)) <f>: (np.float32(-0.0016731449), np.complex128(0.0010678231834397119+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2da6fcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "30cc8581",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.00027226805104874074 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008015344), np.complex128(0.0006757140224551119+0j)) <f>: (np.float32(-0.0020540995), np.complex128(0.0011446628095371095+0j))\n",
      "Epoch 800: <Test loss>: 0.00022942625219002366 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008054378), np.complex128(0.0006141604076517238+0j)) <f>: (np.float32(-0.0020931293), np.complex128(0.0011584814870554516+0j))\n",
      "Epoch 1200: <Test loss>: 0.0002140891010640189 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008081398), np.complex128(0.000581614242967993+0j)) <f>: (np.float32(-0.0021201533), np.complex128(0.0011281379405085838+0j))\n",
      "Epoch 1600: <Test loss>: 0.0002245139330625534 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00798746), np.complex128(0.0006159732464235465+0j)) <f>: (np.float32(-0.00202621), np.complex128(0.0010886203387746529+0j))\n",
      "Epoch 2000: <Test loss>: 0.00023034089826978743 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007918224), np.complex128(0.0006514209045971125+0j)) <f>: (np.float32(-0.0019569732), np.complex128(0.0010408597756371288+0j))\n",
      "Epoch 2400: <Test loss>: 0.00023467434220947325 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007881536), np.complex128(0.0006601071805625061+0j)) <f>: (np.float32(-0.0019202875), np.complex128(0.0009885766935495634+0j))\n",
      "Epoch 2800: <Test loss>: 0.0002426719875074923 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0078096148), np.complex128(0.00068650884938107+0j)) <f>: (np.float32(-0.0018483656), np.complex128(0.0009650569182495088+0j))\n",
      "Epoch 3200: <Test loss>: 0.00026883860118687153 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007579468), np.complex128(0.0007259817961641486+0j)) <f>: (np.float32(-0.0016182167), np.complex128(0.0008984653496514383+0j))\n",
      "Epoch 3600: <Test loss>: 0.0002766258257906884 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007610095), np.complex128(0.0007374512046274872+0j)) <f>: (np.float32(-0.001648843), np.complex128(0.0008914917559700888+0j))\n",
      "Epoch 4000: <Test loss>: 0.0002905190922319889 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0075313933), np.complex128(0.0007652573477667544+0j)) <f>: (np.float32(-0.0015701392), np.complex128(0.0008601888257135004+0j))\n",
      "Epoch 4400: <Test loss>: 0.00029033643659204245 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0075038015), np.complex128(0.0007624932443671841+0j)) <f>: (np.float32(-0.0015425541), np.complex128(0.0008625968312521535+0j))\n",
      "Epoch 4800: <Test loss>: 0.00029761443147435784 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0074167284), np.complex128(0.0007664075933331727+0j)) <f>: (np.float32(-0.0014554753), np.complex128(0.0008528910396261752+0j))\n",
      "Epoch 5200: <Test loss>: 0.0003106252697762102 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007398397), np.complex128(0.0007968463488022415+0j)) <f>: (np.float32(-0.0014371502), np.complex128(0.0008296539732976397+0j))\n",
      "Epoch 6000: <Test loss>: 0.00031983622466214 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007366066), np.complex128(0.0008052754147879381+0j)) <f>: (np.float32(-0.0014048115), np.complex128(0.000815339968348606+0j))\n",
      "Epoch 6400: <Test loss>: 0.0003269785374868661 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007317386), np.complex128(0.0008198222724707493+0j)) <f>: (np.float32(-0.0013561356), np.complex128(0.0007959283216663216+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "64ce3d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad63872e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0005157232517376542 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007827702), np.complex128(0.0010013921570015951+0j)) <f>: (np.float32(-0.0018664562), np.complex128(0.0010583747920844143+0j))\n",
      "Epoch 1600: <Test loss>: 0.0003734381461981684 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008067648), np.complex128(0.0008573160661622109+0j)) <f>: (np.float32(-0.0021064002), np.complex128(0.000809628321041134+0j))\n",
      "Epoch 2400: <Test loss>: 0.0006011234945617616 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0074674455), np.complex128(0.0009698703921136985+0j)) <f>: (np.float32(-0.0015061912), np.complex128(0.00075168186212564+0j))\n",
      "Epoch 3200: <Test loss>: 0.00035206504981033504 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007835239), np.complex128(0.0008207137222992726+0j)) <f>: (np.float32(-0.0018739873), np.complex128(0.000874386562871465+0j))\n",
      "Epoch 4000: <Test loss>: 0.0003531656402628869 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007894676), np.complex128(0.0008187216294207609+0j)) <f>: (np.float32(-0.001933428), np.complex128(0.0008484212947089592+0j))\n",
      "Epoch 4800: <Test loss>: 0.0003710764867719263 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007834004), np.complex128(0.0008681835208136321+0j)) <f>: (np.float32(-0.0018727462), np.complex128(0.000796836199949775+0j))\n",
      "Epoch 5600: <Test loss>: 0.00038723874604329467 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0077608167), np.complex128(0.0008808030479827332+0j)) <f>: (np.float32(-0.001799567), np.complex128(0.0007897174776781552+0j))\n",
      "Epoch 6400: <Test loss>: 0.0004058975900989026 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0076529994), np.complex128(0.0009071864488668576+0j)) <f>: (np.float32(-0.0016917555), np.complex128(0.0007381545201034134+0j))\n",
      "Epoch 7200: <Test loss>: 0.0003939720627386123 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007609191), np.complex128(0.0009067391381943979+0j)) <f>: (np.float32(-0.0016479411), np.complex128(0.0007523972927941981+0j))\n",
      "Epoch 8000: <Test loss>: 0.0004223373834975064 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0074957227), np.complex128(0.0009488498351638393+0j)) <f>: (np.float32(-0.0015344782), np.complex128(0.0007014179576666061+0j))\n",
      "Epoch 8800: <Test loss>: 0.0004194691719021648 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007425985), np.complex128(0.0009482152147330456+0j)) <f>: (np.float32(-0.001464738), np.complex128(0.0007035213707706094+0j))\n",
      "Epoch 9600: <Test loss>: 0.0004299091815482825 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007372891), np.complex128(0.0009686353401988573+0j)) <f>: (np.float32(-0.0014116436), np.complex128(0.0006808712252662752+0j))\n",
      "Epoch 10400: <Test loss>: 0.00045631476677954197 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0072892425), np.complex128(0.0009975348319004025+0j)) <f>: (np.float32(-0.0013279956), np.complex128(0.0006482576341439704+0j))\n",
      "Epoch 11200: <Test loss>: 0.0004457416944205761 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.00731839), np.complex128(0.0009807208474372385+0j)) <f>: (np.float32(-0.0013571412), np.complex128(0.000672231190009932+0j))\n",
      "Epoch 12000: <Test loss>: 0.00045551115181297064 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0073282854), np.complex128(0.0010009132580258337+0j)) <f>: (np.float32(-0.001367034), np.complex128(0.0006481340718651911+0j))\n",
      "Epoch 12800: <Test loss>: 0.00043711543548852205 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007322703), np.complex128(0.0009648247632493383+0j)) <f>: (np.float32(-0.0013614515), np.complex128(0.0006839112505922777+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f3c1acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "080395b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.0009862387087196112 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0006281701498664916 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008192196), np.complex128(0.001097628587084545+0j)) <f>: (np.float32(-0.002230948), np.complex128(0.0007525598647246453+0j))\n",
      "Epoch 3200: <Test loss>: 0.000512263854034245 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007579581), np.complex128(0.0010243187318749849+0j)) <f>: (np.float32(-0.0016183323), np.complex128(0.0007224198026697302+0j))\n",
      "Epoch 4800: <Test loss>: 0.0004546348354779184 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007992708), np.complex128(0.000858205422789913+0j)) <f>: (np.float32(-0.002031462), np.complex128(0.0008453397859485001+0j))\n",
      "Epoch 6400: <Test loss>: 0.0005220230086706579 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.008032634), np.complex128(0.0009450480115995727+0j)) <f>: (np.float32(-0.0020713871), np.complex128(0.000909999203327939+0j))\n",
      "Epoch 8000: <Test loss>: 0.000492457184009254 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007858128), np.complex128(0.0010481396109416538+0j)) <f>: (np.float32(-0.0018968724), np.complex128(0.0007167977824156026+0j))\n",
      "Epoch 9600: <Test loss>: 0.000484676769701764 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007772621), np.complex128(0.0009937235572172766+0j)) <f>: (np.float32(-0.0018113737), np.complex128(0.0006932585974352059+0j))\n",
      "Epoch 11200: <Test loss>: 0.0005073677748441696 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007644889), np.complex128(0.0010033055962734919+0j)) <f>: (np.float32(-0.0016836397), np.complex128(0.0006872455292094797+0j))\n",
      "Epoch 12800: <Test loss>: 0.0004949432332068682 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007577129), np.complex128(0.0009915253792033675+0j)) <f>: (np.float32(-0.001615878), np.complex128(0.0006725309617396602+0j))\n",
      "Epoch 14400: <Test loss>: 0.0005317347822710872 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0073854206), np.complex128(0.0009958781585959076+0j)) <f>: (np.float32(-0.0014241664), np.complex128(0.0006439490022596595+0j))\n",
      "Epoch 16000: <Test loss>: 0.0006045927293598652 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0073624197), np.complex128(0.0011040533181384423+0j)) <f>: (np.float32(-0.0014011707), np.complex128(0.0005692735555204081+0j))\n",
      "Epoch 17600: <Test loss>: 0.00064485875191167 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0073553394), np.complex128(0.0011217402307743785+0j)) <f>: (np.float32(-0.0013940928), np.complex128(0.0005487345616200828+0j))\n",
      "Epoch 19200: <Test loss>: 0.0005682902410626411 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0073733097), np.complex128(0.0010907179797188158+0j)) <f>: (np.float32(-0.0014120588), np.complex128(0.0005597784794437649+0j))\n",
      "Epoch 20800: <Test loss>: 0.0005311596905812621 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0072532245), np.complex128(0.0010442140348076244+0j)) <f>: (np.float32(-0.0012919783), np.complex128(0.0005752299170329779+0j))\n",
      "Epoch 22400: <Test loss>: 0.0004886716487817466 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007463697), np.complex128(0.0009741547936127456+0j)) <f>: (np.float32(-0.0015024521), np.complex128(0.0006142148308730752+0j))\n",
      "Epoch 24000: <Test loss>: 0.0005762739456258714 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.007003569), np.complex128(0.0011163340639261472+0j)) <f>: (np.float32(-0.0010423249), np.complex128(0.0005013214698188796+0j))\n",
      "Epoch 25600: <Test loss>: 0.000567168986890465 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0070265112), np.complex128(0.0011055852874182555+0j)) <f>: (np.float32(-0.0010652653), np.complex128(0.0005062031947159064+0j))\n",
      "Epoch 27200: <Test loss>: 0.0005672831903211772 <O>: (np.float32(0.005961247), np.complex128(0.0015081992718695844+0j)) <O-f>: (np.float32(0.0070620966), np.complex128(0.0011056867759429202+0j)) <f>: (np.float32(-0.0011008481), np.complex128(0.0004967611463934004+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar3d/cv_32x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_1h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxgpu_3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
