{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d82496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CudaDevice(id=0)]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "os.environ['XLA_PYTHON_CLIENT_MEM_FRACTION'] = '0.9'\n",
    "os.environ['OMP_NUM_THREADS']='1'\n",
    "os.environ['XLA_FLAGS'] = (\n",
    "    '--xla_gpu_triton_gemm_any=True '\n",
    "    '--xla_gpu_enable_latency_hiding_scheduler=true '\n",
    ")\n",
    "#os.environ[\"TF_CUDNN_DETERMINISTIC\"] = \"1\"\n",
    "#os.environ['XLA_FLAGS']='--xla_gpu_deterministic_reductions --xla_gpu_autotune_level=1'\n",
    "\n",
    "os.environ.update({\n",
    "  \"NCCL_LL128_BUFFSIZE\": \"-2\",\n",
    "  \"NCCL_LL_BUFFSIZE\": \"-2\",\n",
    "   \"NCCL_PROTO\": \"SIMPLE,LL,LL128\",\n",
    " })\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from models import scalar\n",
    "import pickle\n",
    "import time\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import optax\n",
    "#import sympy\n",
    "#import optuna\n",
    "from util import *\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.special as special\n",
    "\n",
    "matplotlib.style.use('default') # 'classic'\n",
    "matplotlib.rcParams['font.family'] = 'STIXGeneral'\n",
    "matplotlib.rcParams['axes.prop_cycle'] = plt.cycler(color='krbg')\n",
    "matplotlib.rcParams['legend.numpoints'] = 1\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "@jax.jit\n",
    "def arcsinh(x: any) -> any:\n",
    "    return jnp.arcsinh(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def sinh(x: any) -> any:\n",
    "    return jnp.sinh(x)\n",
    "\n",
    "jax.config.update(\"jax_default_matmul_precision\", \"highest\")\n",
    "# jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# jax.config.update(\"jax_platform_name\",\"cpu\")\n",
    "num_devices = jax.local_device_count()\n",
    "jax.devices()\n",
    "\n",
    "# jax.config.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3904335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    volume: int\n",
    "    features: Sequence[int]\n",
    "    kernel_init: Callable = nn.initializers.variance_scaling(\n",
    "        2, \"fan_in\", \"truncated_normal\")  # for ReLU / CELU\n",
    "    bias_init: Callable = nn.initializers.zeros\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features:\n",
    "            x = nn.Dense(feat, use_bias=False,\n",
    "                         kernel_init=self.kernel_init,\n",
    "                         bias_init=self.bias_init)(x)\n",
    "            x = arcsinh(x)\n",
    "        x = nn.Dense(1, use_bias=False,\n",
    "                     kernel_init=self.bias_init)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CV_MLP(nn.Module):\n",
    "    volume: int\n",
    "    features: Sequence[int]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = MLP(self.volume, self.features)(x)\n",
    "        y = self.param('bias', nn.initializers.zeros, (1,))\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b256c1",
   "metadata": {},
   "source": [
    "# 8x8x8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24f76c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0012261154), np.complex128(8.457335901062821e-06+0j))\n",
      "bin size 1: (np.float32(0.0012261154), np.complex128(8.457215177327146e-06+0j))\n",
      "jack bin size 2: (np.float32(0.0012261154), np.complex128(1.188898502447743e-05+0j))\n",
      "bin size 2: (np.float32(0.0012261154), np.complex128(1.188875654250737e-05+0j))\n",
      "jack bin size 4: (np.float32(0.0012261154), np.complex128(1.6674702039371887e-05+0j))\n",
      "bin size 4: (np.float32(0.0012261154), np.complex128(1.6674478345081373e-05+0j))\n",
      "jack bin size 5: (np.float32(0.0012261154), np.complex128(1.8570313994637877e-05+0j))\n",
      "bin size 5: (np.float32(0.0012261154), np.complex128(1.8570103940859812e-05+0j))\n",
      "jack bin size 10: (np.float32(0.0012261154), np.complex128(2.5810362289028454e-05+0j))\n",
      "bin size 10: (np.float32(0.0012261154), np.complex128(2.5810409076912232e-05+0j))\n",
      "jack bin size 20: (np.float32(0.0012261154), np.complex128(3.539596039395593e-05+0j))\n",
      "bin size 20: (np.float32(0.0012261154), np.complex128(3.5395973855591765e-05+0j))\n",
      "jack bin size 50: (np.float32(0.0012261154), np.complex128(5.169873765644382e-05+0j))\n",
      "bin size 50: (np.float32(0.0012261154), np.complex128(5.1698784282864135e-05+0j))\n",
      "jack bin size 100: (np.float32(0.0012261154), np.complex128(6.635033031328507e-05+0j))\n",
      "bin size 100: (np.float32(0.0012261154), np.complex128(6.635045017612634e-05+0j))\n",
      "jack bin size 200: (np.float32(0.0012261154), np.complex128(7.802976420747159e-05+0j))\n",
      "bin size 200: (np.float32(0.0012261154), np.complex128(7.802970485580059e-05+0j))\n",
      "jack bin size 500: (np.float32(0.0012261154), np.complex128(9.073681867266015e-05+0j))\n",
      "bin size 500: (np.float32(0.0012261154), np.complex128(9.073683321117957e-05+0j))\n",
      "jack bin size 1000: (np.float32(0.0012261154), np.complex128(9.114285357740051e-05+0j))\n",
      "bin size 1000: (np.float32(0.0012261154), np.complex128(9.114295193203923e-05+0j))\n",
      "jack bin size 2000: (np.float32(0.0012261154), np.complex128(8.331699609698262e-05+0j))\n",
      "bin size 2000: (np.float32(0.0012261154), np.complex128(8.331699895539453e-05+0j))\n",
      "jack bin size 5000: (np.float32(0.0012261154), np.complex128(8.367422014391141e-05+0j))\n",
      "bin size 5000: (np.float32(0.0012261154), np.complex128(8.367411039275192e-05+0j))\n",
      "jack bin size 10000: (np.float32(0.0012261154), np.complex128(8.254278873209842e-05+0j))\n",
      "bin size 10000: (np.float32(0.0012261154), np.complex128(8.254277054220438e-05+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar4d/config/c_8x8x8x8_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(8,8,8,8), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, 4))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "322ba367",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b055744f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.0001977291249204427 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0013349775), np.complex128(0.0004457945290465556+0j)) <f>: (np.float32(-0.00014708974), np.complex128(0.0004315429078250267+0j))\n",
      "Epoch 200: <Test loss>: 1.5367608057204052e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012696672), np.complex128(3.786906336899122e-05+0j)) <f>: (np.float32(-8.177896e-05), np.complex128(7.870202774169537e-05+0j))\n",
      "Epoch 300: <Test loss>: 2.450232443607092e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012460483), np.complex128(1.726140217430207e-05+0j)) <f>: (np.float32(-5.815997e-05), np.complex128(7.109762737801811e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "285bb208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 1.1525461331984843e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.00128225), np.complex128(3.109199071890805e-05+0j)) <f>: (np.float32(-9.436116e-05), np.complex128(7.198642106155998e-05+0j))\n",
      "Epoch 400: <Test loss>: 8.321566724589502e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012537589), np.complex128(2.8575503461937545e-05+0j)) <f>: (np.float32(-6.58708e-05), np.complex128(7.597172477811295e-05+0j))\n",
      "Epoch 600: <Test loss>: 5.552923880713934e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012380121), np.complex128(2.203113884519633e-05+0j)) <f>: (np.float32(-5.012356e-05), np.complex128(7.553856699756954e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2aff3614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 5.477861577674048e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012652889), np.complex128(7.829892007900187e-05+0j)) <f>: (np.float32(-7.739968e-05), np.complex128(0.00011691616795212846+0j))\n",
      "Epoch 800: <Test loss>: 1.7835769767771126e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001247098), np.complex128(4.8570187843036296e-05+0j)) <f>: (np.float32(-5.9209928e-05), np.complex128(8.126070409552061e-05+0j))\n",
      "Epoch 1200: <Test loss>: 1.172783640868147e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001234179), np.complex128(3.6023038642482217e-05+0j)) <f>: (np.float32(-4.6291592e-05), np.complex128(6.876284422415231e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b658f96a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 9.261979357688688e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012220592), np.complex128(8.877955280442195e-05+0j)) <f>: (np.float32(-3.4172383e-05), np.complex128(9.280778299539169e-05+0j))\n",
      "Epoch 1600: <Test loss>: 1.4695173376821913e-05 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0009915578), np.complex128(0.00010695480760618799+0j)) <f>: (np.float32(0.0001963303), np.complex128(8.663077115837713e-05+0j))\n",
      "Epoch 2400: <Test loss>: 6.63722539684386e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012030159), np.complex128(7.502183183120655e-05+0j)) <f>: (np.float32(-1.5128101e-05), np.complex128(7.410304353135166e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba2cdcb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 2.1847345124115236e-05 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011276483), np.complex128(0.00013344691221477728+0j)) <f>: (np.float32(6.0240276e-05), np.complex128(0.00012199213237081415+0j))\n",
      "Epoch 3200: <Test loss>: 9.977076842915267e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0010025416), np.complex128(7.56549933644583e-05+0j)) <f>: (np.float32(0.00018534606), np.complex128(8.04212829934699e-05+0j))\n",
      "Epoch 4800: <Test loss>: 1.2470858564483933e-05 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011014277), np.complex128(0.00011633308466276595+0j)) <f>: (np.float32(8.6460335e-05), np.complex128(7.773580941414172e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 1800:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334a6db8",
   "metadata": {},
   "source": [
    "## 2 hours run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d95186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ac4e00c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.0001977291249204427 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0013349775), np.complex128(0.0004457945290465556+0j)) <f>: (np.float32(-0.00014708974), np.complex128(0.0004315429078250267+0j))\n",
      "Epoch 200: <Test loss>: 1.5367608057204052e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012696672), np.complex128(3.786906336899122e-05+0j)) <f>: (np.float32(-8.177896e-05), np.complex128(7.870202774169537e-05+0j))\n",
      "Epoch 300: <Test loss>: 2.450232443607092e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012460483), np.complex128(1.726140217430207e-05+0j)) <f>: (np.float32(-5.815997e-05), np.complex128(7.109762737801811e-05+0j))\n",
      "Epoch 400: <Test loss>: 6.418839575417223e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012512755), np.complex128(2.388810728546849e-05+0j)) <f>: (np.float32(-6.338664e-05), np.complex128(6.921956637151633e-05+0j))\n",
      "Epoch 500: <Test loss>: 2.616104382013873e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012430229), np.complex128(1.6651543357768257e-05+0j)) <f>: (np.float32(-5.5134187e-05), np.complex128(7.168082167045447e-05+0j))\n",
      "Epoch 600: <Test loss>: 2.99235750844673e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001237962), np.complex128(1.7594294446190938e-05+0j)) <f>: (np.float32(-5.0074003e-05), np.complex128(7.301814706026827e-05+0j))\n",
      "Epoch 700: <Test loss>: 3.460191919657518e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012384112), np.complex128(1.9258119520158386e-05+0j)) <f>: (np.float32(-5.0523427e-05), np.complex128(7.320153206706275e-05+0j))\n",
      "Epoch 800: <Test loss>: 4.938363531437062e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012312717), np.complex128(1.955036286284221e-05+0j)) <f>: (np.float32(-4.3383923e-05), np.complex128(6.842801138066878e-05+0j))\n",
      "Epoch 900: <Test loss>: 4.7570816263942106e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012406445), np.complex128(2.3720647255376283e-05+0j)) <f>: (np.float32(-5.2756623e-05), np.complex128(7.272565396066828e-05+0j))\n",
      "Epoch 1000: <Test loss>: 4.386341458939569e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012397262), np.complex128(2.087857212521592e-05+0j)) <f>: (np.float32(-5.183822e-05), np.complex128(7.376795696655648e-05+0j))\n",
      "Epoch 1100: <Test loss>: 4.584646831062855e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012409213), np.complex128(2.0786322624251672e-05+0j)) <f>: (np.float32(-5.303337e-05), np.complex128(7.339516900060656e-05+0j))\n",
      "Epoch 1200: <Test loss>: 5.645454166369746e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012435762), np.complex128(2.537844843038951e-05+0j)) <f>: (np.float32(-5.568908e-05), np.complex128(7.798069805264118e-05+0j))\n",
      "Epoch 1300: <Test loss>: 5.564048706219182e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012500242), np.complex128(2.4154399695243916e-05+0j)) <f>: (np.float32(-6.2136576e-05), np.complex128(7.561825134701328e-05+0j))\n",
      "Epoch 1400: <Test loss>: 5.784096970273822e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012402358), np.complex128(2.5512437069319853e-05+0j)) <f>: (np.float32(-5.2347827e-05), np.complex128(7.454934724735557e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b99085",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 1.1525461331984843e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.00128225), np.complex128(3.109199071890805e-05+0j)) <f>: (np.float32(-9.436116e-05), np.complex128(7.198642106155998e-05+0j))\n",
      "Epoch 400: <Test loss>: 8.321566724589502e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012537589), np.complex128(2.8575503461937545e-05+0j)) <f>: (np.float32(-6.58708e-05), np.complex128(7.597172477811295e-05+0j))\n",
      "Epoch 600: <Test loss>: 5.552923880713934e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012380121), np.complex128(2.203113884519633e-05+0j)) <f>: (np.float32(-5.012356e-05), np.complex128(7.553856699756954e-05+0j))\n",
      "Epoch 800: <Test loss>: 3.63263637837008e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012457514), np.complex128(2.1570309584099803e-05+0j)) <f>: (np.float32(-5.786335e-05), np.complex128(7.294638357302297e-05+0j))\n",
      "Epoch 1000: <Test loss>: 5.656069674842001e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012228494), np.complex128(2.8001978330112786e-05+0j)) <f>: (np.float32(-3.496163e-05), np.complex128(7.377908106031464e-05+0j))\n",
      "Epoch 1200: <Test loss>: 5.446858040158986e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012484806), np.complex128(2.7968411793459043e-05+0j)) <f>: (np.float32(-6.059193e-05), np.complex128(7.556906112771488e-05+0j))\n",
      "Epoch 1400: <Test loss>: 5.724741072299366e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012391993), np.complex128(3.0165709711568157e-05+0j)) <f>: (np.float32(-5.13115e-05), np.complex128(7.23573299040454e-05+0j))\n",
      "Epoch 1600: <Test loss>: 7.366393788288406e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012412391), np.complex128(3.332153701216891e-05+0j)) <f>: (np.float32(-5.3350188e-05), np.complex128(7.433021132198976e-05+0j))\n",
      "Epoch 1800: <Test loss>: 8.3780713566739e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012487465), np.complex128(3.6236049576808685e-05+0j)) <f>: (np.float32(-6.0859733e-05), np.complex128(7.646228700541984e-05+0j))\n",
      "Epoch 2000: <Test loss>: 9.242124292541121e-07 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012409615), np.complex128(3.80306957377062e-05+0j)) <f>: (np.float32(-5.307464e-05), np.complex128(7.580234201620582e-05+0j))\n",
      "Epoch 2200: <Test loss>: 1.0381618267274462e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012419306), np.complex128(3.2786482374224854e-05+0j)) <f>: (np.float32(-5.4043725e-05), np.complex128(6.825319739693388e-05+0j))\n",
      "Epoch 2400: <Test loss>: 1.0561276440057554e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012429196), np.complex128(3.781056078467573e-05+0j)) <f>: (np.float32(-5.503234e-05), np.complex128(7.482182807849824e-05+0j))\n",
      "Epoch 2600: <Test loss>: 1.2309419616940431e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001245789), np.complex128(4.1045448042429815e-05+0j)) <f>: (np.float32(-5.7901136e-05), np.complex128(7.763976796888679e-05+0j))\n",
      "Epoch 2800: <Test loss>: 1.2666548627748853e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012380965), np.complex128(4.059641285793006e-05+0j)) <f>: (np.float32(-5.020875e-05), np.complex128(7.521713381085814e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76d65f31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 5.477861577674048e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012652889), np.complex128(7.829892007900187e-05+0j)) <f>: (np.float32(-7.739968e-05), np.complex128(0.00011691616795212846+0j))\n",
      "Epoch 800: <Test loss>: 1.7835769767771126e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001247098), np.complex128(4.8570187843036296e-05+0j)) <f>: (np.float32(-5.9209928e-05), np.complex128(8.126070409552061e-05+0j))\n",
      "Epoch 1200: <Test loss>: 1.172783640868147e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001234179), np.complex128(3.6023038642482217e-05+0j)) <f>: (np.float32(-4.6291592e-05), np.complex128(6.876284422415231e-05+0j))\n",
      "Epoch 1600: <Test loss>: 5.30440001966781e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012532875), np.complex128(7.295931543112672e-05+0j)) <f>: (np.float32(-6.539984e-05), np.complex128(0.000101072857797054+0j))\n",
      "Epoch 2000: <Test loss>: 1.7790388255889411e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012217333), np.complex128(4.7749962343970984e-05+0j)) <f>: (np.float32(-3.3845903e-05), np.complex128(7.490629348890861e-05+0j))\n",
      "Epoch 2400: <Test loss>: 1.9863482521031983e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012225872), np.complex128(5.0064959199922656e-05+0j)) <f>: (np.float32(-3.469931e-05), np.complex128(8.492267964431572e-05+0j))\n",
      "Epoch 2800: <Test loss>: 2.124325419572415e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012224872), np.complex128(5.1599061324512077e-05+0j)) <f>: (np.float32(-3.4599932e-05), np.complex128(8.409475528919967e-05+0j))\n",
      "Epoch 3200: <Test loss>: 2.361333145017852e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012113854), np.complex128(5.4032074269945886e-05+0j)) <f>: (np.float32(-2.3496155e-05), np.complex128(8.77568576986506e-05+0j))\n",
      "Epoch 3600: <Test loss>: 2.392775286352844e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011995613), np.complex128(5.3250228083664904e-05+0j)) <f>: (np.float32(-1.16733745e-05), np.complex128(8.43908163447449e-05+0j))\n",
      "Epoch 4000: <Test loss>: 2.544974449847359e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012067842), np.complex128(5.495700341842208e-05+0j)) <f>: (np.float32(-1.8896748e-05), np.complex128(8.524778386125522e-05+0j))\n",
      "Epoch 4400: <Test loss>: 2.7183443762623938e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012031082), np.complex128(5.5939757239584175e-05+0j)) <f>: (np.float32(-1.5220985e-05), np.complex128(8.565377760386887e-05+0j))\n",
      "Epoch 4800: <Test loss>: 2.7398523343435954e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011932347), np.complex128(5.574563664979002e-05+0j)) <f>: (np.float32(-5.3468953e-06), np.complex128(8.566868373092899e-05+0j))\n",
      "Epoch 5200: <Test loss>: 2.992477675434202e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011908332), np.complex128(5.650240407017144e-05+0j)) <f>: (np.float32(-2.9441082e-06), np.complex128(8.563826095990255e-05+0j))\n",
      "Epoch 5600: <Test loss>: 3.5262994515505852e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011877227), np.complex128(5.96602908742317e-05+0j)) <f>: (np.float32(1.658625e-07), np.complex128(8.900117044136744e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "305da60d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 9.261979357688688e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012220592), np.complex128(8.877955280442195e-05+0j)) <f>: (np.float32(-3.4172383e-05), np.complex128(9.280778299539169e-05+0j))\n",
      "Epoch 1600: <Test loss>: 1.4695173376821913e-05 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0009915578), np.complex128(0.00010695480760618799+0j)) <f>: (np.float32(0.0001963303), np.complex128(8.663077115837713e-05+0j))\n",
      "Epoch 2400: <Test loss>: 6.63722539684386e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012030159), np.complex128(7.502183183120655e-05+0j)) <f>: (np.float32(-1.5128101e-05), np.complex128(7.410304353135166e-05+0j))\n",
      "Epoch 3200: <Test loss>: 3.046428901143372e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012374963), np.complex128(5.045728370687053e-05+0j)) <f>: (np.float32(-4.9607985e-05), np.complex128(7.25873520594397e-05+0j))\n",
      "Epoch 4000: <Test loss>: 3.875658421748085e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012522421), np.complex128(6.994732631006293e-05+0j)) <f>: (np.float32(-6.43544e-05), np.complex128(9.09082032501446e-05+0j))\n",
      "Epoch 4800: <Test loss>: 4.074535809195368e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011833193), np.complex128(7.399743996416348e-05+0j)) <f>: (np.float32(4.5688876e-06), np.complex128(7.852387573695391e-05+0j))\n",
      "Epoch 5600: <Test loss>: 5.362927822716301e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011506269), np.complex128(8.15410423585338e-05+0j)) <f>: (np.float32(3.7261856e-05), np.complex128(7.811152310396674e-05+0j))\n",
      "Epoch 6400: <Test loss>: 7.048619409033563e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001100606), np.complex128(9.000755602407353e-05+0j)) <f>: (np.float32(8.7282046e-05), np.complex128(7.399730517471666e-05+0j))\n",
      "Epoch 7200: <Test loss>: 6.348162060021423e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001152247), np.complex128(8.980507055978551e-05+0j)) <f>: (np.float32(3.564093e-05), np.complex128(7.635976773792654e-05+0j))\n",
      "Epoch 8000: <Test loss>: 8.308221367769875e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011189539), np.complex128(9.887805744810656e-05+0j)) <f>: (np.float32(6.893452e-05), np.complex128(7.864536067249392e-05+0j))\n",
      "Epoch 8800: <Test loss>: 7.619697498739697e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011295995), np.complex128(9.964940987951303e-05+0j)) <f>: (np.float32(5.828876e-05), np.complex128(8.30950378197157e-05+0j))\n",
      "Epoch 9600: <Test loss>: 7.9561696111341e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011325928), np.complex128(9.896594016743337e-05+0j)) <f>: (np.float32(5.529557e-05), np.complex128(7.823185836481329e-05+0j))\n",
      "Epoch 10400: <Test loss>: 8.973481271823402e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011262898), np.complex128(9.949952401464888e-05+0j)) <f>: (np.float32(6.1597464e-05), np.complex128(7.981412789466348e-05+0j))\n",
      "Epoch 11200: <Test loss>: 9.132520062848926e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011364217), np.complex128(0.00010879091737956471+0j)) <f>: (np.float32(5.146657e-05), np.complex128(8.795464138988188e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb0f6aa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 2.1847345124115236e-05 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011276483), np.complex128(0.00013344691221477728+0j)) <f>: (np.float32(6.0240276e-05), np.complex128(0.00012199213237081415+0j))\n",
      "Epoch 3200: <Test loss>: 9.977076842915267e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0010025416), np.complex128(7.56549933644583e-05+0j)) <f>: (np.float32(0.00018534606), np.complex128(8.04212829934699e-05+0j))\n",
      "Epoch 4800: <Test loss>: 1.2470858564483933e-05 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011014277), np.complex128(0.00011633308466276595+0j)) <f>: (np.float32(8.6460335e-05), np.complex128(7.773580941414172e-05+0j))\n",
      "Epoch 6400: <Test loss>: 5.301631517795613e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011193759), np.complex128(5.8267452054305416e-05+0j)) <f>: (np.float32(6.851186e-05), np.complex128(4.147804684320849e-05+0j))\n",
      "Epoch 8000: <Test loss>: 4.067333065904677e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012000414), np.complex128(5.566820011259177e-05+0j)) <f>: (np.float32(-1.2153488e-05), np.complex128(8.587977986223151e-05+0j))\n",
      "Epoch 9600: <Test loss>: 3.9937290239322465e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011647769), np.complex128(4.6777983749763524e-05+0j)) <f>: (np.float32(2.3110673e-05), np.complex128(5.348821470960807e-05+0j))\n",
      "Epoch 11200: <Test loss>: 8.146937943820376e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011835495), np.complex128(8.275227618524275e-05+0j)) <f>: (np.float32(4.337962e-06), np.complex128(6.055819473862043e-05+0j))\n",
      "Epoch 12800: <Test loss>: 4.686306056100875e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011730191), np.complex128(5.8133047153848135e-05+0j)) <f>: (np.float32(1.48686695e-05), np.complex128(5.312398587353124e-05+0j))\n",
      "Epoch 14400: <Test loss>: 9.23449988476932e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011811026), np.complex128(8.975458794755583e-05+0j)) <f>: (np.float32(6.785606e-06), np.complex128(6.0772351383244875e-05+0j))\n",
      "Epoch 16000: <Test loss>: 8.232550499087665e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011754014), np.complex128(8.750063881782725e-05+0j)) <f>: (np.float32(1.2486356e-05), np.complex128(5.1934425396991905e-05+0j))\n",
      "Epoch 17600: <Test loss>: 9.230996511178091e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.001174338), np.complex128(9.083876289865919e-05+0j)) <f>: (np.float32(1.3549898e-05), np.complex128(5.583739258351517e-05+0j))\n",
      "Epoch 19200: <Test loss>: 6.764456884411629e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011854012), np.complex128(7.596793481602002e-05+0j)) <f>: (np.float32(2.4871001e-06), np.complex128(5.271555006329285e-05+0j))\n",
      "Epoch 20800: <Test loss>: 1.011384665616788e-05 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0012039206), np.complex128(9.958699443684425e-05+0j)) <f>: (np.float32(-1.603328e-05), np.complex128(6.912687880484991e-05+0j))\n",
      "Epoch 22400: <Test loss>: 9.2822483566124e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011921354), np.complex128(9.147429514041677e-05+0j)) <f>: (np.float32(-4.247743e-06), np.complex128(6.139364748078093e-05+0j))\n",
      "Epoch 24000: <Test loss>: 8.194655492843594e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011952951), np.complex128(8.613609388854675e-05+0j)) <f>: (np.float32(-7.4085597e-06), np.complex128(5.890123996204532e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "'''\n",
    "def save():\n",
    "    with open(\"1+1scalar_scaling/cv/cv_32x32_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e3_1h_sweep10.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "'''\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 7200:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        # save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b02fa",
   "metadata": {},
   "source": [
    "## 2 hours run recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7d3514f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0012261154), np.complex128(8.457335901062821e-06+0j))\n",
      "bin size 1: (np.float32(0.0012261154), np.complex128(8.457215177327146e-06+0j))\n",
      "jack bin size 2: (np.float32(0.0012261154), np.complex128(1.188898502447743e-05+0j))\n",
      "bin size 2: (np.float32(0.0012261154), np.complex128(1.188875654250737e-05+0j))\n",
      "jack bin size 4: (np.float32(0.0012261154), np.complex128(1.6674702039371887e-05+0j))\n",
      "bin size 4: (np.float32(0.0012261154), np.complex128(1.6674478345081373e-05+0j))\n",
      "jack bin size 5: (np.float32(0.0012261154), np.complex128(1.8570313994637877e-05+0j))\n",
      "bin size 5: (np.float32(0.0012261154), np.complex128(1.8570103940859812e-05+0j))\n",
      "jack bin size 10: (np.float32(0.0012261154), np.complex128(2.5810362289028454e-05+0j))\n",
      "bin size 10: (np.float32(0.0012261154), np.complex128(2.5810409076912232e-05+0j))\n",
      "jack bin size 20: (np.float32(0.0012261154), np.complex128(3.539596039395593e-05+0j))\n",
      "bin size 20: (np.float32(0.0012261154), np.complex128(3.5395973855591765e-05+0j))\n",
      "jack bin size 50: (np.float32(0.0012261154), np.complex128(5.169873765644382e-05+0j))\n",
      "bin size 50: (np.float32(0.0012261154), np.complex128(5.1698784282864135e-05+0j))\n",
      "jack bin size 100: (np.float32(0.0012261154), np.complex128(6.635033031328507e-05+0j))\n",
      "bin size 100: (np.float32(0.0012261154), np.complex128(6.635045017612634e-05+0j))\n",
      "jack bin size 200: (np.float32(0.0012261154), np.complex128(7.802976420747159e-05+0j))\n",
      "bin size 200: (np.float32(0.0012261154), np.complex128(7.802970485580059e-05+0j))\n",
      "jack bin size 500: (np.float32(0.0012261154), np.complex128(9.073681867266015e-05+0j))\n",
      "bin size 500: (np.float32(0.0012261154), np.complex128(9.073683321117957e-05+0j))\n",
      "jack bin size 1000: (np.float32(0.0012261154), np.complex128(9.114285357740051e-05+0j))\n",
      "bin size 1000: (np.float32(0.0012261154), np.complex128(9.114295193203923e-05+0j))\n",
      "jack bin size 2000: (np.float32(0.0012261154), np.complex128(8.331699609698262e-05+0j))\n",
      "bin size 2000: (np.float32(0.0012261154), np.complex128(8.331699895539453e-05+0j))\n",
      "jack bin size 5000: (np.float32(0.0012261154), np.complex128(8.367422014391141e-05+0j))\n",
      "bin size 5000: (np.float32(0.0012261154), np.complex128(8.367411039275192e-05+0j))\n",
      "jack bin size 10000: (np.float32(0.0012261154), np.complex128(8.254278873209842e-05+0j))\n",
      "bin size 10000: (np.float32(0.0012261154), np.complex128(8.254277054220438e-05+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar4d/config/c_8x8x8x8_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(8, 8,8,8), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9691375",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99007e83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 8.053002602537163e-06 <O>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <O-f>: (np.float32(0.0011878875), np.complex128(6.720815102936334e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e28369",
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc942911",
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d7cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6309bae6",
   "metadata": {},
   "source": [
    "## m^2=0.01, lambda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c7c751f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.012423942), np.complex128(5.52549667188217e-05+0j))\n",
      "bin size 1: (np.float32(0.012423942), np.complex128(5.5258515064565094e-05+0j))\n",
      "jack bin size 2: (np.float32(0.012423942), np.complex128(7.80913704445488e-05+0j))\n",
      "bin size 2: (np.float32(0.012423942), np.complex128(7.809003801127697e-05+0j))\n",
      "jack bin size 4: (np.float32(0.012423942), np.complex128(0.0001103191147122132+0j))\n",
      "bin size 4: (np.float32(0.012423942), np.complex128(0.00011031821682325576+0j))\n",
      "jack bin size 5: (np.float32(0.012423942), np.complex128(0.00012327366848688658+0j))\n",
      "bin size 5: (np.float32(0.012423942), np.complex128(0.00012327759576363353+0j))\n",
      "jack bin size 10: (np.float32(0.012423942), np.complex128(0.00017394727267881516+0j))\n",
      "bin size 10: (np.float32(0.012423942), np.complex128(0.0001739468229305239+0j))\n",
      "jack bin size 20: (np.float32(0.012423942), np.complex128(0.0002449656970134387+0j))\n",
      "bin size 20: (np.float32(0.012423942), np.complex128(0.000244965412331678+0j))\n",
      "jack bin size 50: (np.float32(0.012423942), np.complex128(0.0003823081533204692+0j))\n",
      "bin size 50: (np.float32(0.012423942), np.complex128(0.0003823083753127798+0j))\n",
      "jack bin size 100: (np.float32(0.012423942), np.complex128(0.0005297263211527386+0j))\n",
      "bin size 100: (np.float32(0.012423942), np.complex128(0.0005297270999806493+0j))\n",
      "jack bin size 200: (np.float32(0.012423942), np.complex128(0.0007205116471448936+0j))\n",
      "bin size 200: (np.float32(0.012423942), np.complex128(0.0007205118767750939+0j))\n",
      "jack bin size 500: (np.float32(0.012423942), np.complex128(0.001005627639501633+0j))\n",
      "bin size 500: (np.float32(0.012423942), np.complex128(0.0010056296706391777+0j))\n",
      "jack bin size 1000: (np.float32(0.012423942), np.complex128(0.0011998328790997783+0j))\n",
      "bin size 1000: (np.float32(0.012423942), np.complex128(0.0011998331547852712+0j))\n",
      "jack bin size 2000: (np.float32(0.012423942), np.complex128(0.0013805224589304999+0j))\n",
      "bin size 2000: (np.float32(0.012423942), np.complex128(0.0013805234006472997+0j))\n",
      "jack bin size 5000: (np.float32(0.012423942), np.complex128(0.001370326223088033+0j))\n",
      "bin size 5000: (np.float32(0.012423942), np.complex128(0.001370326904129448+0j))\n",
      "jack bin size 10000: (np.float32(0.012423942), np.complex128(0.0012665046961046755+0j))\n",
      "bin size 10000: (np.float32(0.012423942), np.complex128(0.0012665051811685164+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar4d/config/c_8x8x8x8_0.01_0.1_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(8, 8,8,8), m2=0.01, lamda=0.1)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23652613",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1291dac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 8.145681204041466e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012414069), np.complex128(0.0004310623596607395+0j)) <f>: (np.float32(-0.00047674534), np.complex128(0.0019324786459843602+0j))\n",
      "Epoch 200: <Test loss>: 4.908275150228292e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.0122484295), np.complex128(0.0002617455018642994+0j)) <f>: (np.float32(-0.00031112175), np.complex128(0.0017338620511172286+0j))\n",
      "Epoch 300: <Test loss>: 7.020409975666553e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012370835), np.complex128(0.0004868678547181042+0j)) <f>: (np.float32(-0.00043352653), np.complex128(0.0019679921448383024+0j))\n",
      "Epoch 400: <Test loss>: 8.894233906175941e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012394193), np.complex128(0.0003166272610562318+0j)) <f>: (np.float32(-0.00045688046), np.complex128(0.0016990183765260732+0j))\n",
      "Epoch 500: <Test loss>: 4.1882780351443216e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012709315), np.complex128(0.00042792915518302937+0j)) <f>: (np.float32(-0.0007720039), np.complex128(0.0019440820827305843+0j))\n",
      "Epoch 600: <Test loss>: 2.999671414727345e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012837149), np.complex128(0.00030993942100214137+0j)) <f>: (np.float32(-0.0008998306), np.complex128(0.0018287317479245848+0j))\n",
      "Epoch 700: <Test loss>: 3.096160799032077e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01274456), np.complex128(0.00037200403052721617+0j)) <f>: (np.float32(-0.0008072489), np.complex128(0.001891491997855908+0j))\n",
      "Epoch 800: <Test loss>: 3.479920997051522e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012521618), np.complex128(0.00027969790213772555+0j)) <f>: (np.float32(-0.00058430637), np.complex128(0.0017358215408071918+0j))\n",
      "Epoch 900: <Test loss>: 3.832453876384534e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012746635), np.complex128(0.0003975404462426858+0j)) <f>: (np.float32(-0.0008093161), np.complex128(0.0019166608982514358+0j))\n",
      "Epoch 1000: <Test loss>: 4.1184979636454955e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012734438), np.complex128(0.00035759548267442457+0j)) <f>: (np.float32(-0.0007971259), np.complex128(0.0018639750335816073+0j))\n",
      "Epoch 1100: <Test loss>: 4.5251319534145296e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012771493), np.complex128(0.00042043594025539685+0j)) <f>: (np.float32(-0.00083418336), np.complex128(0.0019312275461965565+0j))\n",
      "Epoch 1200: <Test loss>: 5.293338108458556e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012776009), np.complex128(0.0004775043648567215+0j)) <f>: (np.float32(-0.00083870377), np.complex128(0.0019915377362818188+0j))\n",
      "Epoch 1300: <Test loss>: 4.1852184949675575e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012644757), np.complex128(0.00036228672629672125+0j)) <f>: (np.float32(-0.0007074426), np.complex128(0.0018712764986279515+0j))\n",
      "Epoch 1400: <Test loss>: 7.343632023548707e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012811895), np.complex128(0.0005226803007075117+0j)) <f>: (np.float32(-0.0008745816), np.complex128(0.0020052950132427388+0j))\n",
      "Epoch 1500: <Test loss>: 6.878462590975687e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012740509), np.complex128(0.00048703610366289987+0j)) <f>: (np.float32(-0.0008031949), np.complex128(0.0019676937685757885+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb80964d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 9.545249486109242e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012356549), np.complex128(0.000360115030444717+0j)) <f>: (np.float32(-0.00041924615), np.complex128(0.00174822267735728+0j))\n",
      "Epoch 400: <Test loss>: 9.10471280803904e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012348781), np.complex128(0.0002985516156556659+0j)) <f>: (np.float32(-0.00041146972), np.complex128(0.0016243124197990945+0j))\n",
      "Epoch 600: <Test loss>: 0.0001961984671652317 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012547771), np.complex128(0.0005837739187728573+0j)) <f>: (np.float32(-0.0006104602), np.complex128(0.0019214610517467633+0j))\n",
      "Epoch 800: <Test loss>: 0.00010500774078536779 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012400699), np.complex128(0.00038093118316286865+0j)) <f>: (np.float32(-0.0004633813), np.complex128(0.0015211192685020208+0j))\n",
      "Epoch 1000: <Test loss>: 0.00012495026749093086 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012448247), np.complex128(0.000429295095579524+0j)) <f>: (np.float32(-0.0005109367), np.complex128(0.0016423767428861275+0j))\n",
      "Epoch 1200: <Test loss>: 8.542016439605504e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.0124009065), np.complex128(0.0003437834336253496+0j)) <f>: (np.float32(-0.00046360708), np.complex128(0.0015079224619185614+0j))\n",
      "Epoch 1400: <Test loss>: 0.00010345402552047744 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012566767), np.complex128(0.0003833913918613962+0j)) <f>: (np.float32(-0.00062944955), np.complex128(0.0016339713364127433+0j))\n",
      "Epoch 1600: <Test loss>: 0.00012102352775400504 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012714288), np.complex128(0.0004410996698955677+0j)) <f>: (np.float32(-0.0007769804), np.complex128(0.0017508847213592344+0j))\n",
      "Epoch 1800: <Test loss>: 0.00012896256521344185 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012655334), np.complex128(0.0004204296606529332+0j)) <f>: (np.float32(-0.00071802945), np.complex128(0.0016529929500086757+0j))\n",
      "Epoch 2000: <Test loss>: 0.00010445338557474315 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012581894), np.complex128(0.0003682580256515151+0j)) <f>: (np.float32(-0.0006445899), np.complex128(0.001554076778862283+0j))\n",
      "Epoch 2200: <Test loss>: 0.00010799014853546396 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012538648), np.complex128(0.0003799955858261162+0j)) <f>: (np.float32(-0.0006013368), np.complex128(0.0015761975982801631+0j))\n",
      "Epoch 2400: <Test loss>: 0.00011811102012870833 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01253027), np.complex128(0.0004073656454609648+0j)) <f>: (np.float32(-0.00059295207), np.complex128(0.0016529811519676834+0j))\n",
      "Epoch 2600: <Test loss>: 0.00010550092702032998 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012536785), np.complex128(0.00036671501949464455+0j)) <f>: (np.float32(-0.00059947075), np.complex128(0.0015745485366150179+0j))\n",
      "Epoch 2800: <Test loss>: 0.0001186617009807378 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012500023), np.complex128(0.00038893152356218495+0j)) <f>: (np.float32(-0.0005627022), np.complex128(0.001625511252996696+0j))\n",
      "Epoch 3000: <Test loss>: 0.00012174108996987343 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012529303), np.complex128(0.0003981118900668758+0j)) <f>: (np.float32(-0.0005919943), np.complex128(0.001654205484157107+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d94fe60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e1a3099",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.000484796823002398 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012780518), np.complex128(0.000793213186479902+0j)) <f>: (np.float32(-0.00084320724), np.complex128(0.0015193372568695648+0j))\n",
      "Epoch 800: <Test loss>: 0.0003010065702255815 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012653469), np.complex128(0.0006314568431890126+0j)) <f>: (np.float32(-0.0007161572), np.complex128(0.0015867463155338094+0j))\n",
      "Epoch 1200: <Test loss>: 0.00029810378327965736 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012756396), np.complex128(0.0006440959703294395+0j)) <f>: (np.float32(-0.0008190811), np.complex128(0.0016203231596158378+0j))\n",
      "Epoch 1600: <Test loss>: 0.00025231941253878176 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012743345), np.complex128(0.0005799984822250034+0j)) <f>: (np.float32(-0.0008060249), np.complex128(0.0014855620027218169+0j))\n",
      "Epoch 2000: <Test loss>: 0.00031392902019433677 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012607011), np.complex128(0.0006547726265544912+0j)) <f>: (np.float32(-0.0006697033), np.complex128(0.0018881109076567045+0j))\n",
      "Epoch 2400: <Test loss>: 0.00021616267622448504 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01272198), np.complex128(0.0004883200603605639+0j)) <f>: (np.float32(-0.0007846659), np.complex128(0.0014224585947164713+0j))\n",
      "Epoch 2800: <Test loss>: 0.00023961714759934694 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012802341), np.complex128(0.0005443370004160346+0j)) <f>: (np.float32(-0.000865046), np.complex128(0.001429073489893459+0j))\n",
      "Epoch 3200: <Test loss>: 0.00022841538884676993 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01274896), np.complex128(0.0005597793674683557+0j)) <f>: (np.float32(-0.0008116549), np.complex128(0.0014091738736985212+0j))\n",
      "Epoch 3600: <Test loss>: 0.0002483610878698528 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012781589), np.complex128(0.000622581481416103+0j)) <f>: (np.float32(-0.00084428786), np.complex128(0.0012720205054174982+0j))\n",
      "Epoch 4000: <Test loss>: 0.0001912949956022203 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012746792), np.complex128(0.0004985445217779065+0j)) <f>: (np.float32(-0.0008094928), np.complex128(0.0014251351008331904+0j))\n",
      "Epoch 4400: <Test loss>: 0.0001796121650841087 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012789075), np.complex128(0.0005107274579999657+0j)) <f>: (np.float32(-0.0008517655), np.complex128(0.0013927158602356185+0j))\n",
      "Epoch 4800: <Test loss>: 0.00020789749396499246 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012813924), np.complex128(0.0005459880284213451+0j)) <f>: (np.float32(-0.0008766151), np.complex128(0.0013388718846387103+0j))\n",
      "Epoch 5200: <Test loss>: 0.00020639377180486917 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012855223), np.complex128(0.0005179237555626261+0j)) <f>: (np.float32(-0.00091791997), np.complex128(0.0014338244214543241+0j))\n",
      "Epoch 5600: <Test loss>: 0.00017387393745593727 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012774747), np.complex128(0.0004715294499732365+0j)) <f>: (np.float32(-0.0008374455), np.complex128(0.0013993609482486939+0j))\n",
      "Epoch 6000: <Test loss>: 0.0001712832454359159 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012819736), np.complex128(0.0004616856973840422+0j)) <f>: (np.float32(-0.00088242086), np.complex128(0.0014131821629801526+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc83a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cf256c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.00026227813214063644 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012248121), np.complex128(0.0005221316600862073+0j)) <f>: (np.float32(-0.00031081357), np.complex128(0.001871278401537789+0j))\n",
      "Epoch 1600: <Test loss>: 0.0012981041800230742 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012022921), np.complex128(0.001183970837467608+0j)) <f>: (np.float32(-8.560755e-05), np.complex128(0.0014623003254260502+0j))\n",
      "Epoch 2400: <Test loss>: 0.000334713637130335 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012595547), np.complex128(0.0005310530405620195+0j)) <f>: (np.float32(-0.00065823423), np.complex128(0.0012177718443284896+0j))\n",
      "Epoch 3200: <Test loss>: 0.00030550057999789715 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012439627), np.complex128(0.0005290718259847452+0j)) <f>: (np.float32(-0.00050231465), np.complex128(0.0012697753255306039+0j))\n",
      "Epoch 4000: <Test loss>: 0.0003560443583410233 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012335044), np.complex128(0.00060224781148724+0j)) <f>: (np.float32(-0.00039773362), np.complex128(0.001212116903734174+0j))\n",
      "Epoch 4800: <Test loss>: 0.00027987698558717966 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012258085), np.complex128(0.0004912933886967632+0j)) <f>: (np.float32(-0.00032077212), np.complex128(0.0013415752852144656+0j))\n",
      "Epoch 5600: <Test loss>: 0.00038259048596955836 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012228945), np.complex128(0.0007037390002256873+0j)) <f>: (np.float32(-0.00029164468), np.complex128(0.0011546839133231553+0j))\n",
      "Epoch 6400: <Test loss>: 0.0004466265963856131 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012249752), np.complex128(0.0008024217477653513+0j)) <f>: (np.float32(-0.00031244048), np.complex128(0.001085261195468908+0j))\n",
      "Epoch 7200: <Test loss>: 0.0007321599405258894 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012231133), np.complex128(0.0011123994806855535+0j)) <f>: (np.float32(-0.0002938136), np.complex128(0.000910476135963535+0j))\n",
      "Epoch 8000: <Test loss>: 0.0007805995410308242 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012275769), np.complex128(0.0011536224702158185+0j)) <f>: (np.float32(-0.0003384595), np.complex128(0.0008812922861019449+0j))\n",
      "Epoch 8800: <Test loss>: 0.000705175451003015 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012236782), np.complex128(0.0011140503183998804+0j)) <f>: (np.float32(-0.00029947233), np.complex128(0.0008811254009091995+0j))\n",
      "Epoch 9600: <Test loss>: 0.0006565816001966596 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012152562), np.complex128(0.0010899552934559608+0j)) <f>: (np.float32(-0.00021525394), np.complex128(0.000896438433522901+0j))\n",
      "Epoch 10400: <Test loss>: 0.0007814596174284816 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012145968), np.complex128(0.0012126286596197955+0j)) <f>: (np.float32(-0.00020866172), np.complex128(0.0008200709193561778+0j))\n",
      "Epoch 11200: <Test loss>: 0.0007536560879088938 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012117634), np.complex128(0.0011966712383048013+0j)) <f>: (np.float32(-0.00018032643), np.complex128(0.000825317495499374+0j))\n",
      "Epoch 12000: <Test loss>: 0.0008177958661690354 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012087496), np.complex128(0.001281644154768826+0j)) <f>: (np.float32(-0.0001501825), np.complex128(0.0007855328520849189+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2b9427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c47c10a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.00027298033819533885 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011450401), np.complex128(0.0008378528506233313+0j)) <f>: (np.float32(0.0004869105), np.complex128(0.0009494107495535954+0j))\n",
      "Epoch 3200: <Test loss>: 0.00029624858871102333 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011532616), np.complex128(0.0009136340148388043+0j)) <f>: (np.float32(0.0004046984), np.complex128(0.0008323108160127051+0j))\n",
      "Epoch 4800: <Test loss>: 0.001740394625812769 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011781999), np.complex128(0.0024120356479675887+0j)) <f>: (np.float32(0.00015530975), np.complex128(0.0011255017760804188+0j))\n",
      "Epoch 6400: <Test loss>: 0.000548616168089211 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012128501), np.complex128(0.0009284963749726667+0j)) <f>: (np.float32(-0.00019119903), np.complex128(0.001372827154057082+0j))\n",
      "Epoch 8000: <Test loss>: 0.0005944382282905281 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011579386), np.complex128(0.0015165856492445938+0j)) <f>: (np.float32(0.0003579275), np.complex128(0.0006420948069140359+0j))\n",
      "Epoch 9600: <Test loss>: 0.0005924252327531576 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01187384), np.complex128(0.0014126346323895867+0j)) <f>: (np.float32(6.3468455e-05), np.complex128(0.0006411135397411844+0j))\n",
      "Epoch 11200: <Test loss>: 0.0003261375823058188 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011862018), np.complex128(0.0010915458723587679+0j)) <f>: (np.float32(7.52942e-05), np.complex128(0.0007231802054407803+0j))\n",
      "Epoch 12800: <Test loss>: 0.0006341580301523209 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011189004), np.complex128(0.0016631211241882428+0j)) <f>: (np.float32(0.00074830756), np.complex128(0.0006918327470939951+0j))\n",
      "Epoch 14400: <Test loss>: 0.00036218107561580837 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011604738), np.complex128(0.0012588464068914325+0j)) <f>: (np.float32(0.00033257695), np.complex128(0.0006104572182473656+0j))\n",
      "Epoch 16000: <Test loss>: 0.0005156314582563937 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011733666), np.complex128(0.0014231575969300993+0j)) <f>: (np.float32(0.00020363645), np.complex128(0.0005240668876757428+0j))\n",
      "Epoch 17600: <Test loss>: 0.00044086328125558794 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011644861), np.complex128(0.0013839502963603305+0j)) <f>: (np.float32(0.00029245156), np.complex128(0.0005396451224904588+0j))\n",
      "Epoch 19200: <Test loss>: 0.0004257062391843647 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.0116882995), np.complex128(0.0013477630405632707+0j)) <f>: (np.float32(0.00024901217), np.complex128(0.0005521910070488501+0j))\n",
      "Epoch 20800: <Test loss>: 0.00039095935062505305 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011753988), np.complex128(0.001305479369089604+0j)) <f>: (np.float32(0.00018331893), np.complex128(0.0005574622575896053+0j))\n",
      "Epoch 22400: <Test loss>: 0.00048160162987187505 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011694171), np.complex128(0.0014676226373807773+0j)) <f>: (np.float32(0.00024314284), np.complex128(0.00046436740478765043+0j))\n",
      "Epoch 24000: <Test loss>: 0.0005303231882862747 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011749349), np.complex128(0.0014909072764552496+0j)) <f>: (np.float32(0.00018795597), np.complex128(0.0004589317111217744+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb48383",
   "metadata": {},
   "source": [
    "### l4, w64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f458c9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9df2fd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.00015854480443522334 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012944298), np.complex128(0.00037256424718336517+0j)) <f>: (np.float32(-0.0010069992), np.complex128(0.0016939183244403617+0j))\n",
      "Epoch 200: <Test loss>: 6.779019895475358e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012564314), np.complex128(0.0003612431071115288+0j)) <f>: (np.float32(-0.00062701124), np.complex128(0.0018241632419868045+0j))\n",
      "Epoch 300: <Test loss>: 3.63264225597959e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012604425), np.complex128(0.0003217950250169931+0j)) <f>: (np.float32(-0.00066711905), np.complex128(0.0018010942660272448+0j))\n",
      "Epoch 400: <Test loss>: 3.103746712440625e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012590269), np.complex128(0.00023517743943803133+0j)) <f>: (np.float32(-0.0006529688), np.complex128(0.001640219477433724+0j))\n",
      "Epoch 500: <Test loss>: 2.7728827262762934e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012609741), np.complex128(0.00021621793999071002+0j)) <f>: (np.float32(-0.0006724381), np.complex128(0.0016596235759069864+0j))\n",
      "Epoch 600: <Test loss>: 3.301068500149995e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012630138), np.complex128(0.00026933211794978736+0j)) <f>: (np.float32(-0.00069282367), np.complex128(0.0017144471694882204+0j))\n",
      "Epoch 700: <Test loss>: 3.155815647915006e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01253357), np.complex128(0.00022596813284622908+0j)) <f>: (np.float32(-0.00059625675), np.complex128(0.0016654335402227275+0j))\n",
      "Epoch 800: <Test loss>: 2.8700051188934594e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012543692), np.complex128(0.00020175844172702037+0j)) <f>: (np.float32(-0.0006063878), np.complex128(0.001607071295507748+0j))\n",
      "Epoch 900: <Test loss>: 3.332403866806999e-05 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01247248), np.complex128(0.00018705897067524473+0j)) <f>: (np.float32(-0.00053516764), np.complex128(0.0015387733242280966+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd5d8c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fc8184fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.0002680255565792322 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012728359), np.complex128(0.0009364891671629611+0j)) <f>: (np.float32(-0.0007910537), np.complex128(0.0022718310271925223+0j))\n",
      "Epoch 400: <Test loss>: 0.00016229529865086079 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012687367), np.complex128(0.0005523475530981453+0j)) <f>: (np.float32(-0.0007500501), np.complex128(0.0018303049469175431+0j))\n",
      "Epoch 600: <Test loss>: 0.00014475877105724066 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01257355), np.complex128(0.0005915079152252567+0j)) <f>: (np.float32(-0.0006362465), np.complex128(0.0018371011259717132+0j))\n",
      "Epoch 800: <Test loss>: 0.00011375256144674495 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012647253), np.complex128(0.0004694713578385288+0j)) <f>: (np.float32(-0.0007099402), np.complex128(0.0015492895651538503+0j))\n",
      "Epoch 1000: <Test loss>: 0.00012047710333717987 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012327119), np.complex128(0.00048044800779945647+0j)) <f>: (np.float32(-0.00038981086), np.complex128(0.001381196024661587+0j))\n",
      "Epoch 1200: <Test loss>: 0.00010510512220207602 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012383406), np.complex128(0.0004600188435119168+0j)) <f>: (np.float32(-0.0004460986), np.complex128(0.0013404939249841634+0j))\n",
      "Epoch 1400: <Test loss>: 0.0001261749566765502 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01232952), np.complex128(0.00045564240460706435+0j)) <f>: (np.float32(-0.00039221032), np.complex128(0.0014179767346066234+0j))\n",
      "Epoch 1600: <Test loss>: 0.0001355824788333848 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.0123078795), np.complex128(0.0006788436748344588+0j)) <f>: (np.float32(-0.00037057087), np.complex128(0.0010436308563727698+0j))\n",
      "Epoch 1800: <Test loss>: 0.00011788665142375976 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012265751), np.complex128(0.0005627330641180652+0j)) <f>: (np.float32(-0.00032844464), np.complex128(0.0011847474784026043+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04117c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a20c8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.000427421648055315 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012392374), np.complex128(0.0007385085881938373+0j)) <f>: (np.float32(-0.00045506668), np.complex128(0.0020966184472769995+0j))\n",
      "Epoch 800: <Test loss>: 0.00023249891819432378 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012988697), np.complex128(0.000622995491166407+0j)) <f>: (np.float32(-0.0010513731), np.complex128(0.0018529599765571268+0j))\n",
      "Epoch 1200: <Test loss>: 0.0002312288706889376 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012920385), np.complex128(0.0005424925099105821+0j)) <f>: (np.float32(-0.000983075), np.complex128(0.0014942447265888473+0j))\n",
      "Epoch 1600: <Test loss>: 0.00022469258692581207 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012847367), np.complex128(0.0005882325633826881+0j)) <f>: (np.float32(-0.00091006566), np.complex128(0.0013120471989632784+0j))\n",
      "Epoch 2000: <Test loss>: 0.00024720479268580675 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012860521), np.complex128(0.0006908467226465499+0j)) <f>: (np.float32(-0.0009232018), np.complex128(0.0011860196370592762+0j))\n",
      "Epoch 2400: <Test loss>: 0.00022701187117490917 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012685928), np.complex128(0.0006811491769632006+0j)) <f>: (np.float32(-0.000748621), np.complex128(0.0010910542873174233+0j))\n",
      "Epoch 2800: <Test loss>: 0.0002426944556646049 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012753403), np.complex128(0.000718615441892347+0j)) <f>: (np.float32(-0.0008160836), np.complex128(0.001065875364930085+0j))\n",
      "Epoch 3200: <Test loss>: 0.00032161950366571546 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012958074), np.complex128(0.0007080211816632573+0j)) <f>: (np.float32(-0.0010207725), np.complex128(0.0014405220297789136+0j))\n",
      "Epoch 3600: <Test loss>: 0.0002340926293982193 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012558353), np.complex128(0.0007982635091885278+0j)) <f>: (np.float32(-0.0006210402), np.complex128(0.0009416681900069274+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cc6d19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed6d07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0003643418021965772 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.0119368555), np.complex128(0.000688784983267987+0j)) <f>: (np.float32(4.5356154e-07), np.complex128(0.0012395623185985861+0j))\n",
      "Epoch 1600: <Test loss>: 0.0002605509362183511 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011737254), np.complex128(0.0006883048156856673+0j)) <f>: (np.float32(0.0002000466), np.complex128(0.0010391929534801674+0j))\n",
      "Epoch 2400: <Test loss>: 0.00027872141799889505 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011739339), np.complex128(0.0008346150495348886+0j)) <f>: (np.float32(0.00019798048), np.complex128(0.000897996028655192+0j))\n",
      "Epoch 3200: <Test loss>: 0.00028192365425638855 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011852469), np.complex128(0.0009052639390580689+0j)) <f>: (np.float32(8.48426e-05), np.complex128(0.0008003282932228307+0j))\n",
      "Epoch 4000: <Test loss>: 0.0003316782822366804 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011856008), np.complex128(0.0009192425878634138+0j)) <f>: (np.float32(8.1298575e-05), np.complex128(0.000847943030036477+0j))\n",
      "Epoch 4800: <Test loss>: 0.0003895843110512942 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011953411), np.complex128(0.0008867329590485622+0j)) <f>: (np.float32(-1.610455e-05), np.complex128(0.0009484777528602875+0j))\n",
      "Epoch 5600: <Test loss>: 0.00030814833007752895 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.0118838325), np.complex128(0.0010351861230960783+0j)) <f>: (np.float32(5.3471304e-05), np.complex128(0.0006752988709589054+0j))\n",
      "Epoch 6400: <Test loss>: 0.0003490377275738865 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01200951), np.complex128(0.000957698873350664+0j)) <f>: (np.float32(-7.22066e-05), np.complex128(0.0007944756402964026+0j))\n",
      "Epoch 7200: <Test loss>: 0.00031383452005684376 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011865958), np.complex128(0.001057795292608579+0j)) <f>: (np.float32(7.135086e-05), np.complex128(0.0006494989022309473+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a16b241",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7abb143b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00041522321407683194 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.00046264618868008256 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012647253), np.complex128(0.0010122745812104045+0j)) <f>: (np.float32(-0.0007099529), np.complex128(0.0008445179826200279+0j))\n",
      "Epoch 3200: <Test loss>: 0.00034455760032869875 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011794178), np.complex128(0.001117345016492463+0j)) <f>: (np.float32(0.0001431243), np.complex128(0.0006686674204663319+0j))\n",
      "Epoch 4800: <Test loss>: 0.00032699116854928434 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012019442), np.complex128(0.0011986709428226628+0j)) <f>: (np.float32(-8.213878e-05), np.complex128(0.0005365750946193524+0j))\n",
      "Epoch 6400: <Test loss>: 0.0003503608168102801 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011959912), np.complex128(0.0012537925320844434+0j)) <f>: (np.float32(-2.2607193e-05), np.complex128(0.0005353838730611007+0j))\n",
      "Epoch 8000: <Test loss>: 0.0003287185390945524 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011874605), np.complex128(0.0012876293136502694+0j)) <f>: (np.float32(6.269382e-05), np.complex128(0.000449243235975317+0j))\n",
      "Epoch 9600: <Test loss>: 0.00031569594284519553 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.012030465), np.complex128(0.0012336947595450978+0j)) <f>: (np.float32(-9.314958e-05), np.complex128(0.0004907964120775459+0j))\n",
      "Epoch 11200: <Test loss>: 0.00030732626328244805 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011960091), np.complex128(0.0012320218480766565+0j)) <f>: (np.float32(-2.2780157e-05), np.complex128(0.0004976769534678433+0j))\n",
      "Epoch 12800: <Test loss>: 0.00032157136593014 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.01197297), np.complex128(0.0012579420172760146+0j)) <f>: (np.float32(-3.565577e-05), np.complex128(0.00046545437860197303+0j))\n",
      "Epoch 14400: <Test loss>: 0.0003257636562921107 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011939313), np.complex128(0.0012813325849981053+0j)) <f>: (np.float32(-1.997647e-06), np.complex128(0.00044609623540128526+0j))\n",
      "Epoch 16000: <Test loss>: 0.00030368674197234213 <O>: (np.float32(0.011937312), np.complex128(0.0015528235224433302+0j)) <O-f>: (np.float32(0.011985166), np.complex128(0.0012415712839443233+0j)) <f>: (np.float32(-4.7851532e-05), np.complex128(0.00046454814950704543+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_8x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_2h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 2*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bc1c57",
   "metadata": {},
   "source": [
    "# 16x8x8x8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81660458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.00025539257), np.complex128(4.718580965620353e-06+0j))\n",
      "bin size 1: (np.float32(0.00025539257), np.complex128(4.718579426098136e-06+0j))\n",
      "jack bin size 2: (np.float32(0.00025539257), np.complex128(6.6153928426799035e-06+0j))\n",
      "bin size 2: (np.float32(0.00025539257), np.complex128(6.61536614929218e-06+0j))\n",
      "jack bin size 4: (np.float32(0.00025539257), np.complex128(9.241323125324693e-06+0j))\n",
      "bin size 4: (np.float32(0.00025539257), np.complex128(9.24131772201583e-06+0j))\n",
      "jack bin size 5: (np.float32(0.00025539257), np.complex128(1.0277888827763741e-05+0j))\n",
      "bin size 5: (np.float32(0.00025539257), np.complex128(1.0277895199333107e-05+0j))\n",
      "jack bin size 10: (np.float32(0.00025539257), np.complex128(1.4172190280678994e-05+0j))\n",
      "bin size 10: (np.float32(0.00025539257), np.complex128(1.4172204195933168e-05+0j))\n",
      "jack bin size 20: (np.float32(0.00025539257), np.complex128(1.9192817965108578e-05+0j))\n",
      "bin size 20: (np.float32(0.00025539257), np.complex128(1.9192833243309523e-05+0j))\n",
      "jack bin size 50: (np.float32(0.00025539257), np.complex128(2.7326746031810427e-05+0j))\n",
      "bin size 50: (np.float32(0.00025539257), np.complex128(2.7326738620963392e-05+0j))\n",
      "jack bin size 100: (np.float32(0.00025539257), np.complex128(3.297766326412852e-05+0j))\n",
      "bin size 100: (np.float32(0.00025539257), np.complex128(3.2977668335746764e-05+0j))\n",
      "jack bin size 200: (np.float32(0.00025539257), np.complex128(3.749770140533976e-05+0j))\n",
      "bin size 200: (np.float32(0.00025539257), np.complex128(3.7497706214748875e-05+0j))\n",
      "jack bin size 500: (np.float32(0.00025539257), np.complex128(3.81287152599673e-05+0j))\n",
      "bin size 500: (np.float32(0.00025539257), np.complex128(3.812870792623521e-05+0j))\n",
      "jack bin size 1000: (np.float32(0.00025539257), np.complex128(3.8771114763920086e-05+0j))\n",
      "bin size 1000: (np.float32(0.00025539257), np.complex128(3.877111485532774e-05+0j))\n",
      "jack bin size 2000: (np.float32(0.00025539257), np.complex128(4.0178273138735676e-05+0j))\n",
      "bin size 2000: (np.float32(0.00025539257), np.complex128(4.017827866066779e-05+0j))\n",
      "jack bin size 5000: (np.float32(0.00025539257), np.complex128(3.506694934541757e-05+0j))\n",
      "bin size 5000: (np.float32(0.00025539257), np.complex128(3.506693265322601e-05+0j))\n",
      "jack bin size 10000: (np.float32(0.00025539257), np.complex128(2.926582146756118e-05+0j))\n",
      "bin size 10000: (np.float32(0.00025539257), np.complex128(2.9265829653013498e-05+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar4d/config/c_16x8x8x8_0.1_0.5_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(16, 8,8,8), m2=0.1, lamda=0.5)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d5ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14e10117",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.9021271075180266e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 1.113855773837713e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00026664557), np.complex128(3.5562130093475796e-05+0j)) <f>: (np.float32(7.6985794e-05), np.complex128(4.784270541217432e-05+0j))\n",
      "Epoch 200: <Test loss>: 1.1301762015136774e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00027311046), np.complex128(1.2396322782858813e-05+0j)) <f>: (np.float32(7.052099e-05), np.complex128(5.619017620979877e-05+0j))\n",
      "Epoch 300: <Test loss>: 1.2797177362244838e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00027264748), np.complex128(1.3891941529282173e-05+0j)) <f>: (np.float32(7.0984395e-05), np.complex128(5.803593532180962e-05+0j))\n",
      "Epoch 400: <Test loss>: 1.5818207543816243e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00027853812), np.complex128(1.5909776218839956e-05+0j)) <f>: (np.float32(6.509347e-05), np.complex128(5.4249041670976086e-05+0j))\n",
      "Epoch 500: <Test loss>: 1.533732358893758e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.0002809763), np.complex128(1.6415737149347183e-05+0j)) <f>: (np.float32(6.2655105e-05), np.complex128(5.5341474457894926e-05+0j))\n",
      "Epoch 600: <Test loss>: 1.5944335984841018e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00027901804), np.complex128(1.7223417318869468e-05+0j)) <f>: (np.float32(6.461329e-05), np.complex128(5.581740410143082e-05+0j))\n",
      "Epoch 700: <Test loss>: 1.7360872561766882e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00027710895), np.complex128(1.8023426383109484e-05+0j)) <f>: (np.float32(6.652264e-05), np.complex128(5.922869902990869e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d85f0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.9021271075180266e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 9.162791911876411e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.0002745814), np.complex128(3.146303236864255e-05+0j)) <f>: (np.float32(6.905005e-05), np.complex128(5.827745025974309e-05+0j))\n",
      "Epoch 400: <Test loss>: 3.4819544225683785e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00025316438), np.complex128(1.652574813322662e-05+0j)) <f>: (np.float32(9.046704e-05), np.complex128(6.403427185387741e-05+0j))\n",
      "Epoch 600: <Test loss>: 2.959196194751712e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00025882205), np.complex128(1.7144291949190474e-05+0j)) <f>: (np.float32(8.480946e-05), np.complex128(5.851394627298024e-05+0j))\n",
      "Epoch 800: <Test loss>: 3.068462319788523e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00026022963), np.complex128(1.758319612100349e-05+0j)) <f>: (np.float32(8.340169e-05), np.complex128(6.014271419758793e-05+0j))\n",
      "Epoch 1000: <Test loss>: 3.2793090554150695e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.0002631693), np.complex128(1.9770081554345734e-05+0j)) <f>: (np.float32(8.046209e-05), np.complex128(5.918484488694616e-05+0j))\n",
      "Epoch 1200: <Test loss>: 3.3723927117534913e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00026236428), np.complex128(1.9947434733395e-05+0j)) <f>: (np.float32(8.126732e-05), np.complex128(5.873410501238368e-05+0j))\n",
      "Epoch 1400: <Test loss>: 3.565428983165475e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.0002598161), np.complex128(2.1371290983671924e-05+0j)) <f>: (np.float32(8.38155e-05), np.complex128(5.7737590774459436e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab6f123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b801f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.9021271075180266e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 4.894249741482781e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00036970762), np.complex128(6.0111363758015735e-05+0j)) <f>: (np.float32(-2.6076223e-05), np.complex128(6.962272953574675e-05+0j))\n",
      "Epoch 800: <Test loss>: 2.331479208805831e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00024251493), np.complex128(3.698520932223576e-05+0j)) <f>: (np.float32(0.000101116704), np.complex128(7.501682083530123e-05+0j))\n",
      "Epoch 1200: <Test loss>: 6.875184226373676e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00024490184), np.complex128(2.1542955255186277e-05+0j)) <f>: (np.float32(9.872962e-05), np.complex128(6.254537969550971e-05+0j))\n",
      "Epoch 1600: <Test loss>: 6.953862339287298e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00024850803), np.complex128(2.3466214334723295e-05+0j)) <f>: (np.float32(9.512345e-05), np.complex128(6.205509101149196e-05+0j))\n",
      "Epoch 2000: <Test loss>: 7.028482968962635e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00025184438), np.complex128(2.5580789194241962e-05+0j)) <f>: (np.float32(9.178725e-05), np.complex128(6.30016697237648e-05+0j))\n",
      "Epoch 2400: <Test loss>: 6.94936090894771e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00025312285), np.complex128(2.5700835054217392e-05+0j)) <f>: (np.float32(9.05089e-05), np.complex128(6.218333920574595e-05+0j))\n",
      "Epoch 2800: <Test loss>: 6.903502480781754e-07 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00025462804), np.complex128(2.4747723018169273e-05+0j)) <f>: (np.float32(8.90035e-05), np.complex128(6.023624618049472e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6441fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c34d8f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.9021271075180266e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.0005554841482080519 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(-6.883569e-05), np.complex128(0.0008414001282816733+0j)) <f>: (np.float32(0.00041246694), np.complex128(0.0008496690961197114+0j))\n",
      "Epoch 1600: <Test loss>: 2.2908839127921965e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00030829353), np.complex128(3.8823971276198444e-05+0j)) <f>: (np.float32(3.5337944e-05), np.complex128(4.652173711817171e-05+0j))\n",
      "Epoch 2400: <Test loss>: 1.1219034377063508e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00026185787), np.complex128(2.597157948013338e-05+0j)) <f>: (np.float32(8.1773724e-05), np.complex128(6.244543728508797e-05+0j))\n",
      "Epoch 3200: <Test loss>: 1.4479618357654545e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.000304548), np.complex128(3.070394974129222e-05+0j)) <f>: (np.float32(3.908351e-05), np.complex128(5.167232731764989e-05+0j))\n",
      "Epoch 4000: <Test loss>: 1.1142010407638736e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00028263647), np.complex128(2.548279924860137e-05+0j)) <f>: (np.float32(6.0995077e-05), np.complex128(4.8989149123313146e-05+0j))\n",
      "Epoch 4800: <Test loss>: 1.180002982437145e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.0002762654), np.complex128(2.567914782866356e-05+0j)) <f>: (np.float32(6.736601e-05), np.complex128(4.680118339219859e-05+0j))\n",
      "Epoch 5600: <Test loss>: 1.2462653558031889e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.0002681615), np.complex128(2.9218427319095016e-05+0j)) <f>: (np.float32(7.5470074e-05), np.complex128(4.6635868100069026e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0675d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e72ec34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 2.9021271075180266e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 7.440384251822252e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00023667014), np.complex128(7.083141822054806e-05+0j)) <f>: (np.float32(0.00010696129), np.complex128(8.081232303627748e-05+0j))\n",
      "Epoch 3200: <Test loss>: 2.9526001526392065e-05 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00046663388), np.complex128(0.00019151222370720274+0j)) <f>: (np.float32(-0.00012300238), np.complex128(0.00020005904809667437+0j))\n",
      "Epoch 4800: <Test loss>: 3.33036268784781e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.000330618), np.complex128(5.7573528231306196e-05+0j)) <f>: (np.float32(1.3013522e-05), np.complex128(4.321798038188345e-05+0j))\n",
      "Epoch 6400: <Test loss>: 2.627054300319287e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00031996364), np.complex128(5.138199877799001e-05+0j)) <f>: (np.float32(2.3667733e-05), np.complex128(4.681084858841471e-05+0j))\n",
      "Epoch 8000: <Test loss>: 1.2679042811214458e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00026726656), np.complex128(3.519354835114577e-05+0j)) <f>: (np.float32(7.636497e-05), np.complex128(4.642356282814061e-05+0j))\n",
      "Epoch 9600: <Test loss>: 1.818482814996969e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00029265045), np.complex128(4.1308287463729346e-05+0j)) <f>: (np.float32(5.098097e-05), np.complex128(3.531755067782492e-05+0j))\n",
      "Epoch 11200: <Test loss>: 2.573332494648639e-06 <O>: (np.float32(0.00034363184), np.complex128(6.151087465555962e-05+0j)) <O-f>: (np.float32(0.00031121096), np.complex128(4.8273916674529845e-05+0j)) <f>: (np.float32(3.24207e-05), np.complex128(2.985826885875535e-05+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.1_0.5_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcdbef4",
   "metadata": {},
   "source": [
    "## m^2=0.01, lamda=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c64ff57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jack bin size 1: (np.float32(0.0047519677), np.complex128(2.4193233565428046e-05+0j))\n",
      "bin size 1: (np.float32(0.0047519677), np.complex128(2.41939970034545e-05+0j))\n",
      "jack bin size 2: (np.float32(0.0047519677), np.complex128(3.41776186845991e-05+0j))\n",
      "bin size 2: (np.float32(0.0047519677), np.complex128(3.417888285517308e-05+0j))\n",
      "jack bin size 4: (np.float32(0.0047519677), np.complex128(4.826344318113903e-05+0j))\n",
      "bin size 4: (np.float32(0.0047519677), np.complex128(4.826205443166974e-05+0j))\n",
      "jack bin size 5: (np.float32(0.0047519677), np.complex128(5.3917650825755836e-05+0j))\n",
      "bin size 5: (np.float32(0.0047519677), np.complex128(5.391733841566924e-05+0j))\n",
      "jack bin size 10: (np.float32(0.0047519677), np.complex128(7.599918748119268e-05+0j))\n",
      "bin size 10: (np.float32(0.0047519677), np.complex128(7.599784546091029e-05+0j))\n",
      "jack bin size 20: (np.float32(0.0047519677), np.complex128(0.00010681138619188218+0j))\n",
      "bin size 20: (np.float32(0.0047519677), np.complex128(0.00010681086160132414+0j))\n",
      "jack bin size 50: (np.float32(0.0047519677), np.complex128(0.0001658448662317524+0j))\n",
      "bin size 50: (np.float32(0.0047519677), np.complex128(0.0001658446756068149+0j))\n",
      "jack bin size 100: (np.float32(0.0047519677), np.complex128(0.00022827500432462007+0j))\n",
      "bin size 100: (np.float32(0.0047519677), np.complex128(0.00022827523062670028+0j))\n",
      "jack bin size 200: (np.float32(0.0047519677), np.complex128(0.00030529567494391876+0j))\n",
      "bin size 200: (np.float32(0.0047519677), np.complex128(0.00030529567689821833+0j))\n",
      "jack bin size 500: (np.float32(0.0047519677), np.complex128(0.00043085998875358125+0j))\n",
      "bin size 500: (np.float32(0.0047519677), np.complex128(0.0004308601878442598+0j))\n",
      "jack bin size 1000: (np.float32(0.0047519677), np.complex128(0.0004917416806436153+0j))\n",
      "bin size 1000: (np.float32(0.0047519677), np.complex128(0.0004917416265302826+0j))\n",
      "jack bin size 2000: (np.float32(0.0047519677), np.complex128(0.0005328994811861776+0j))\n",
      "bin size 2000: (np.float32(0.0047519677), np.complex128(0.0005328991516892399+0j))\n",
      "jack bin size 5000: (np.float32(0.0047519677), np.complex128(0.0005731003706338734+0j))\n",
      "bin size 5000: (np.float32(0.0047519677), np.complex128(0.0005731004106951331+0j))\n",
      "jack bin size 10000: (np.float32(0.0047519677), np.complex128(0.0005554811214096844+0j))\n",
      "bin size 10000: (np.float32(0.0047519677), np.complex128(0.0005554809855918089+0j))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with open(\"scalar4d/config/c_16x8x8x8_0.01_0.1_sweep1.bin\", \"rb\") as aa:\n",
    "    m = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    n = np.fromfile(aa, dtype=np.int32, count=1)[0]\n",
    "    data = np.fromfile(aa, dtype=np.float64, count=m * n).reshape((m, n), order='F').T\n",
    "\n",
    "conf = jax.device_put(data)\n",
    "\n",
    "# Correctly reshape Eigen column-major layout\n",
    "# conf = data.reshape((m, n), order='F').T\n",
    "\n",
    "model =  eval(\"scalar.Model(geom=(16, 8,8,8), m2=0.01, lamda=0.1)\")\n",
    "V=model.dof\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf)\n",
    "\n",
    "error_jac, error_bi = [], []\n",
    "\n",
    "for i in 1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000:\n",
    "    jac, bi = jackknife(obs[:100000], Bs=i), bin(obs[:100000], Bs=i)\n",
    "    print(f\"jack bin size {i}: {jac}\")\n",
    "    print(f\"bin size {i}: {bi}\")\n",
    "    error_jac.append(jac[1].real)\n",
    "    error_bi.append(bi[1].real)\n",
    "\n",
    "x_axis = np.array([1, 2, 4, 5, 10, 20, 50, 100,200, 500, 1000, 2000, 5000, 10000])\n",
    "\n",
    "plt.plot(x_axis, np.sqrt(x_axis), color='black', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_jac)/error_jac[0], color='red', marker='.', linestyle='-')\n",
    "plt.plot(x_axis, np.array(error_bi)/error_bi[0], color='blue', marker='.', linestyle='-')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.xlabel('Bin size')\n",
    "plt.ylabel('Error reduction')\n",
    "plt.tight_layout()\n",
    "#plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c8e7efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac02f860",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.00011507108865771443 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0056659323), np.complex128(0.0006814493292748964+0j)) <f>: (np.float32(0.00048255312), np.complex128(0.0005808241548034785+0j))\n",
      "Epoch 200: <Test loss>: 8.017936488613486e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005573864), np.complex128(0.000536499612529133+0j)) <f>: (np.float32(0.0005746172), np.complex128(0.0016187911903360246+0j))\n",
      "Epoch 300: <Test loss>: 3.19901664624922e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0055808565), np.complex128(0.0002464783928080086+0j)) <f>: (np.float32(0.0005676271), np.complex128(0.001286442405354316+0j))\n",
      "Epoch 400: <Test loss>: 2.3140188204706647e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005614212), np.complex128(0.0001981980657059638+0j)) <f>: (np.float32(0.000534273), np.complex128(0.001165587079249193+0j))\n",
      "Epoch 500: <Test loss>: 2.065650187432766e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0056431247), np.complex128(0.00019770713082548042+0j)) <f>: (np.float32(0.00050535944), np.complex128(0.0010736828771325383+0j))\n",
      "Epoch 600: <Test loss>: 2.0736262740683742e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005639189), np.complex128(0.00020894581147102673+0j)) <f>: (np.float32(0.00050929387), np.complex128(0.0010592810846096698+0j))\n",
      "Epoch 700: <Test loss>: 2.101677455357276e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0056633824), np.complex128(0.0002239554409687268+0j)) <f>: (np.float32(0.00048510285), np.complex128(0.0010411399473955313+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2889804",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81997b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.001233301474712789 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005054202), np.complex128(0.002534322469713107+0j)) <f>: (np.float32(0.0010942793), np.complex128(0.003609032410715252+0j))\n",
      "Epoch 400: <Test loss>: 4.420544428285211e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0054309326), np.complex128(0.0002861469904375423+0j)) <f>: (np.float32(0.00071755075), np.complex128(0.001382674966187263+0j))\n",
      "Epoch 600: <Test loss>: 3.6237077438272536e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005475105), np.complex128(0.0002401321567849067+0j)) <f>: (np.float32(0.00067338144), np.complex128(0.0013090869055594656+0j))\n",
      "Epoch 800: <Test loss>: 2.8161823138361797e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0056427377), np.complex128(0.0001665831375239413+0j)) <f>: (np.float32(0.00050574454), np.complex128(0.0011240209316449054+0j))\n",
      "Epoch 1000: <Test loss>: 2.4703094823053107e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005810204), np.complex128(0.0002628959060065374+0j)) <f>: (np.float32(0.00033827714), np.complex128(0.0009573277425020309+0j))\n",
      "Epoch 1200: <Test loss>: 3.182234650012106e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005913947), np.complex128(0.0004567085415586668+0j)) <f>: (np.float32(0.00023453409), np.complex128(0.0007323525480089898+0j))\n",
      "Epoch 1400: <Test loss>: 3.0735704058315605e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005837844), np.complex128(0.0003085648857962142+0j)) <f>: (np.float32(0.00031063787), np.complex128(0.000926893554016572+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b86c6151",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7932909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.00020119413966313004 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006032812), np.complex128(0.0010421856597815449+0j)) <f>: (np.float32(0.00011566784), np.complex128(0.0004170622714045593+0j))\n",
      "Epoch 800: <Test loss>: 4.0422142774332315e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0056977225), np.complex128(0.00029346244187119185+0j)) <f>: (np.float32(0.0004507615), np.complex128(0.0009950472213002156+0j))\n",
      "Epoch 1200: <Test loss>: 0.00011860570521093905 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0059059584), np.complex128(0.0007249872720527628+0j)) <f>: (np.float32(0.00024252509), np.complex128(0.0005775874320306104+0j))\n",
      "Epoch 1600: <Test loss>: 0.00014149064372759312 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006238068), np.complex128(0.0009384811966111449+0j)) <f>: (np.float32(-8.9582565e-05), np.complex128(0.00039067848993846755+0j))\n",
      "Epoch 2000: <Test loss>: 0.00014314992586150765 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006235181), np.complex128(0.0009702253482287131+0j)) <f>: (np.float32(-8.6697015e-05), np.complex128(0.00037556903696221055+0j))\n",
      "Epoch 2400: <Test loss>: 0.0001301825395785272 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006178267), np.complex128(0.0009220773526482819+0j)) <f>: (np.float32(-2.9782395e-05), np.complex128(0.00039750156345167364+0j))\n",
      "Epoch 2800: <Test loss>: 0.00013124792894814163 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006205957), np.complex128(0.0009276883360253522+0j)) <f>: (np.float32(-5.747158e-05), np.complex128(0.000390872396450905+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70bfc583",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dff0ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 7.342783646890894e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0052997065), np.complex128(0.00034022549967191776+0j)) <f>: (np.float32(0.0008487739), np.complex128(0.001336756483202731+0j))\n",
      "Epoch 1600: <Test loss>: 9.818605030886829e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0058496925), np.complex128(0.0008027853304049625+0j)) <f>: (np.float32(0.000298792), np.complex128(0.0004040914353642987+0j))\n",
      "Epoch 2400: <Test loss>: 9.904333273880184e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005809924), np.complex128(0.0007998352492839441+0j)) <f>: (np.float32(0.00033855895), np.complex128(0.0004071509338009711+0j))\n",
      "Epoch 3200: <Test loss>: 0.00011692792759276927 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0058715143), np.complex128(0.0009598013349997486+0j)) <f>: (np.float32(0.00027697103), np.complex128(0.00028059169888838175+0j))\n",
      "Epoch 4000: <Test loss>: 0.00012013005471089855 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0059061353), np.complex128(0.001007712228014432+0j)) <f>: (np.float32(0.00024234633), np.complex128(0.0002427826723247625+0j))\n",
      "Epoch 4800: <Test loss>: 0.00011835562327178195 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0059403856), np.complex128(0.0010167587882423852+0j)) <f>: (np.float32(0.0002080985), np.complex128(0.00023423381816479758+0j))\n",
      "Epoch 5600: <Test loss>: 0.00010980754450429231 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005934864), np.complex128(0.0009833394416645708+0j)) <f>: (np.float32(0.00021361705), np.complex128(0.00025543087000208135+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86460365",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [32]*2)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0313dfc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0023390566930174828 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0074144914), np.complex128(0.0034502183466150475+0j)) <f>: (np.float32(-0.0012660049), np.complex128(0.002379677046763504+0j))\n",
      "Epoch 3200: <Test loss>: 0.0002175052504753694 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005889082), np.complex128(0.0014890285970030506+0j)) <f>: (np.float32(0.00025940477), np.complex128(0.0004387097102852065+0j))\n",
      "Epoch 4800: <Test loss>: 0.00011925009312108159 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005836177), np.complex128(0.0011380841965076894+0j)) <f>: (np.float32(0.0003123096), np.complex128(0.00021158559142788282+0j))\n",
      "Epoch 6400: <Test loss>: 0.0001146671493188478 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005930354), np.complex128(0.001130877496371251+0j)) <f>: (np.float32(0.00021812589), np.complex128(0.00023308989977358888+0j))\n",
      "Epoch 8000: <Test loss>: 0.00011033033661078662 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005984246), np.complex128(0.0011438581323971744+0j)) <f>: (np.float32(0.00016424447), np.complex128(0.00020571018287131513+0j))\n",
      "Epoch 9600: <Test loss>: 0.00010604443377815187 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.00606942), np.complex128(0.0011138153724652815+0j)) <f>: (np.float32(7.906214e-05), np.complex128(0.00021285847953332555+0j))\n",
      "Epoch 11200: <Test loss>: 9.685686381999403e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0060838647), np.complex128(0.0010546494020652857+0j)) <f>: (np.float32(6.461807e-05), np.complex128(0.00023519918018292436+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l2_w32_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_4h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 4*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35769b7e",
   "metadata": {},
   "source": [
    "### l4, w64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60c2a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b942c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 100: <Test loss>: 0.0002370414003962651 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005340376), np.complex128(0.0005440681192560011+0j)) <f>: (np.float32(0.0008081084), np.complex128(0.0011995716534790618+0j))\n",
      "Epoch 200: <Test loss>: 0.00011364262172719464 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006075488), np.complex128(0.0005686079810895916+0j)) <f>: (np.float32(7.299608e-05), np.complex128(0.0006995340769275181+0j))\n",
      "Epoch 300: <Test loss>: 9.65757280937396e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006347125), np.complex128(0.0008918491224375643+0j)) <f>: (np.float32(-0.00019864438), np.complex128(0.00032147321124831416+0j))\n",
      "Epoch 400: <Test loss>: 8.722764323465526e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006247349), np.complex128(0.0008397007029361634+0j)) <f>: (np.float32(-9.886399e-05), np.complex128(0.0003697922149928054+0j))\n",
      "Epoch 500: <Test loss>: 0.00011985684250248596 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006226405), np.complex128(0.0010952491886237817+0j)) <f>: (np.float32(-7.792258e-05), np.complex128(0.00016201117463328956+0j))\n",
      "Epoch 600: <Test loss>: 9.588164539309219e-05 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061585996), np.complex128(0.000961569455390391+0j)) <f>: (np.float32(-1.0117712e-05), np.complex128(0.0002422170482331587+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=1\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_5h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 5*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f37ad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47608a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 200: <Test loss>: 0.0001555692870169878 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0059223846), np.complex128(0.000639427878776832+0j)) <f>: (np.float32(0.00022609964), np.complex128(0.0007648461289508787+0j))\n",
      "Epoch 400: <Test loss>: 0.0001792251568986103 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.00584524), np.complex128(0.0006487966016402676+0j)) <f>: (np.float32(0.00030324268), np.complex128(0.0007068418215763259+0j))\n",
      "Epoch 600: <Test loss>: 0.00011884656851179898 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0063731377), np.complex128(0.0008180169819079485+0j)) <f>: (np.float32(-0.00022465004), np.complex128(0.0005039519255175323+0j))\n",
      "Epoch 800: <Test loss>: 0.00012006828910671175 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006196138), np.complex128(0.000822876379329549+0j)) <f>: (np.float32(-4.7656216e-05), np.complex128(0.00044281682401773004+0j))\n",
      "Epoch 1000: <Test loss>: 0.00011746952804969624 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006275141), np.complex128(0.0008974473563187237+0j)) <f>: (np.float32(-0.00012665465), np.complex128(0.000367010160810435+0j))\n",
      "Epoch 1200: <Test loss>: 0.00010989799193339422 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0063393693), np.complex128(0.0009379939882624266+0j)) <f>: (np.float32(-0.00019088763), np.complex128(0.0003029513334908633+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=2\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_5h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 5*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1dbfb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "655b8113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 400: <Test loss>: 0.00023459522344637662 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005910628), np.complex128(0.000444086540606777+0j)) <f>: (np.float32(0.00023785417), np.complex128(0.0011157261476634056+0j))\n",
      "Epoch 1200: <Test loss>: 0.00012208976841066033 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0060053463), np.complex128(0.0008311335491265947+0j)) <f>: (np.float32(0.00014313818), np.complex128(0.00047867497155840063+0j))\n",
      "Epoch 1600: <Test loss>: 0.00011536582314874977 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061970167), np.complex128(0.0008635520919905598+0j)) <f>: (np.float32(-4.853165e-05), np.complex128(0.00045400961282102955+0j))\n",
      "Epoch 2000: <Test loss>: 0.0001307202473981306 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0063767033), np.complex128(0.0010535830113923717+0j)) <f>: (np.float32(-0.00022822345), np.complex128(0.00025872422020019597+0j))\n",
      "Epoch 2400: <Test loss>: 0.00012245553079992533 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0062574875), np.complex128(0.0009949942569764062+0j)) <f>: (np.float32(-0.0001090106), np.complex128(0.00031204660821065563+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=4\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_5h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 5*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd8447e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7d5ceb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 800: <Test loss>: 0.00010827824735315517 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005905966), np.complex128(0.0007478097577580432+0j)) <f>: (np.float32(0.00024251641), np.complex128(0.0005044798878519362+0j))\n",
      "Epoch 1600: <Test loss>: 0.00011269434617133811 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.005760219), np.complex128(0.0007158914264600192+0j)) <f>: (np.float32(0.00038826274), np.complex128(0.0005184203833150399+0j))\n",
      "Epoch 2400: <Test loss>: 0.0001344707270618528 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006231046), np.complex128(0.0011054534791968473+0j)) <f>: (np.float32(-8.256092e-05), np.complex128(0.00018604682879028032+0j))\n",
      "Epoch 3200: <Test loss>: 0.00011176601401530206 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006097182), np.complex128(0.0009963182382109847+0j)) <f>: (np.float32(5.1303577e-05), np.complex128(0.00024590284187008606+0j))\n",
      "Epoch 4000: <Test loss>: 0.0001176790174213238 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0062344237), np.complex128(0.0010362659609985103+0j)) <f>: (np.float32(-8.593881e-05), np.complex128(0.00021924773717633608+0j))\n",
      "Epoch 4800: <Test loss>: 0.00015283215907402337 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061471853), np.complex128(0.0007704132819713595+0j)) <f>: (np.float32(1.2954138e-06), np.complex128(0.0005444131168095331+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=8\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_5h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 5*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72dab0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(1)\n",
    "g1 = CV_MLP(V, [64]*4)\n",
    "g_params = g1.init(key, jnp.zeros(V))\n",
    "\n",
    "index = jnp.array([\n",
    "    tuple(-x for x in coords)\n",
    "    for coords in product(*(range(dim) for dim in model.shape))\n",
    "])\n",
    "\n",
    "\n",
    "def g(x, p):\n",
    "    def g_(x, p, ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        return g1.apply(p, rolled_x)[0]\n",
    "    return jnp.ravel(jax.vmap(lambda ind: g_(x, p, ind))(index).T)\n",
    "\n",
    "\n",
    "dS = jax.grad(lambda y: model.action(y).real)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def f(x, p):\n",
    "    # diagonal sum (Stein's identity)\n",
    "    def diag_(ind):\n",
    "        rolled_x = jnp.roll(x.reshape(model.shape), shift=ind, axis=tuple(\n",
    "            range(len(model.shape)))).reshape(-1)\n",
    "\n",
    "        ei = jnp.zeros_like(x).at[0].set(1.0)\n",
    "        _, jvp_val = jax.jvp(lambda y: g1.apply(p, y)[0], (rolled_x,), (ei,))\n",
    "\n",
    "        return jvp_val[0]\n",
    "    j = jax.vmap(diag_)(index).sum()\n",
    "    return j - g(x, p)@dS(x)\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def Loss(x, p):\n",
    "    _, y = g1.apply(p, x)\n",
    "    al = 0\n",
    "\n",
    "    # shift is not regularized\n",
    "    return jnp.abs(model.observe(x, model.shape[0]//2) - f(x, p) - y[0])**2 + al * l2_regularization(p)\n",
    "\n",
    "\n",
    "def Loss_batch(batch, params):\n",
    "    # Compute the per-sample losses\n",
    "    per_sample_losses = jax.vmap(Loss, in_axes=(0, None))(batch, params)\n",
    "    # Return the average loss over the batch\n",
    "    return jnp.mean(per_sample_losses)\n",
    "\n",
    "\n",
    "Loss_batch_grad = jax.jit(jax.grad(Loss_batch, argnums=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fbd5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: <Test loss>: 0.00014373238082043827 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <f>: (np.float32(0.0), np.complex128(0j))\n",
      "Epoch 1600: <Test loss>: 0.0001844009238993749 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006285848), np.complex128(0.001234804536562306+0j)) <f>: (np.float32(-0.00013736695), np.complex128(0.0002458095834304684+0j))\n",
      "Epoch 3200: <Test loss>: 0.00010333464160794392 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0060601113), np.complex128(0.0009632976146744467+0j)) <f>: (np.float32(8.8372566e-05), np.complex128(0.00032009306245836647+0j))\n",
      "Epoch 4800: <Test loss>: 0.00010633793863235041 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006075277), np.complex128(0.001082033226081292+0j)) <f>: (np.float32(7.320944e-05), np.complex128(0.0001735590939850055+0j))\n",
      "Epoch 6400: <Test loss>: 0.00010698861296987161 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006127787), np.complex128(0.0010900320441527384+0j)) <f>: (np.float32(2.0697198e-05), np.complex128(0.00015225105493145175+0j))\n",
      "Epoch 8000: <Test loss>: 0.00010287543409503996 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.006146951), np.complex128(0.001044367916783147+0j)) <f>: (np.float32(1.5324797e-06), np.complex128(0.0001981476068801071+0j))\n",
      "Epoch 9600: <Test loss>: 0.00010507794650038704 <O>: (np.float32(0.0061484845), np.complex128(0.0011717687452865184+0j)) <O-f>: (np.float32(0.0061007147), np.complex128(0.0010973713136745201+0j)) <f>: (np.float32(4.776867e-05), np.complex128(0.0001416086352258924+0j))\n"
     ]
    }
   ],
   "source": [
    "binsize=16\n",
    "\n",
    "sched = optax.constant_schedule(1e-4)\n",
    "\n",
    "opt = optax.chain(\n",
    "    optax.clip_by_global_norm(1.0),  # Clip by the gradient by the global norm.\n",
    "    # optax.adamw(1e-3)\n",
    "    optax.scale_by_adam(),  # Use the updates from adam.\n",
    "    # Use the learning rate from the scheduler.\n",
    "    optax.scale_by_schedule(sched),\n",
    "    # Scale updates by -1 since optax.apply_updates is additive and we want to descend on the loss.\n",
    "    optax.scale(-1.0)\n",
    ")\n",
    "\n",
    "opt_state = opt.init(g_params)\n",
    "\n",
    "@jax.jit\n",
    "def train_batch_shard(x, p, opt_state):\n",
    "    grad = Loss_batch_grad(x, p)\n",
    "    updates, opt_state = opt.update(grad, opt_state)\n",
    "    p = optax.apply_updates(p, updates)\n",
    "    return p, opt_state\n",
    "\n",
    "\n",
    "n_train = 10000 // binsize\n",
    "n_test = 1000\n",
    "nt = 32\n",
    "ns = 50\n",
    "\n",
    "conf_train = conf[:n_train*binsize:binsize]\n",
    "conf_test = conf[-n_test*100::100]\n",
    "\n",
    "obs = jax.vmap(lambda x: model.observe(x, model.shape[0]//2))(conf_test)\n",
    "obs_av = jackknife(np.array(obs))\n",
    "\n",
    "track_red, track_ltest = [], []\n",
    "\n",
    "f_vmap = jax.vmap(f, in_axes=(0, None))\n",
    "Loss_vmap = jax.vmap(Loss, in_axes=(0, None))\n",
    "\n",
    "\n",
    "def save():\n",
    "    with open(\"correlated/scalar4d/cv_16x8x8x8_0.01_0.1_l4_w64_lr1e-4_nt32_ridge0_test1e3_train1e4_sweep1_bs\"+str(binsize)+\"_5h.pkl\", \"wb\") as aa:\n",
    "        pickle.dump((g1, g_params, opt_state, track_red, track_ltest), aa)\n",
    "\n",
    "\n",
    "from jax.sharding import PartitionSpec as P, NamedSharding, Mesh\n",
    "\n",
    "mesh = jax.make_mesh((num_devices,), ('batch',))\n",
    "\n",
    "sharding = NamedSharding(mesh, P('batch'))\n",
    "replicated_sharding = NamedSharding(mesh, P())\n",
    "\n",
    "g_params = jax.device_put(g_params, replicated_sharding)\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "epochs = 0\n",
    "# Training\n",
    "while time.time() - start_time < 5*3600:\n",
    "    key, _ = jax.random.split(key)\n",
    "    conf_train = jax.random.permutation(key, conf_train)\n",
    "\n",
    "    if epochs % (100*binsize) == 0:\n",
    "        # Reduce memory usage\n",
    "        fs, ls = [], []\n",
    "        for i in range(n_test//ns):\n",
    "            fs.append(jax.vmap(lambda x: f(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "            ls.append(jax.vmap(lambda x: Loss(x, g_params))(conf_test[ns*i: ns*(i+1)]))\n",
    "        fs = jnp.ravel(jnp.array(fs))\n",
    "        ls = jnp.mean(jnp.array(ls))\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epochs}: <Test loss>: {ls} <O>: {obs_av} <O-f>: {jackknife(np.array(obs-fs))} <f>: {jackknife(np.array(fs))}\", flush=True)\n",
    "\n",
    "        track_red.append([epochs, jackknife(obs)[1]/jackknife(obs-fs)[1]])\n",
    "        track_ltest.append([epochs, ls])\n",
    "\n",
    "        save()\n",
    "\n",
    "    for s in range(n_train//nt):  # one epoch\n",
    "        g_params, opt_state = train_batch_shard(\n",
    "            conf_train[nt*s: nt*(s+1)], g_params, opt_state)\n",
    "\n",
    "    epochs+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jaxgpu_3.13.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
